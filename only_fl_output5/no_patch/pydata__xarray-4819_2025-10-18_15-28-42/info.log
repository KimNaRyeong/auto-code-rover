2025-10-18 15:28:42.797 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 15:28:46.400 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=332, output_tokens=12, cost=0.003680
2025-10-18 15:28:56.332 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=512, output_tokens=356, cost=0.015800
2025-10-18 15:28:58.072 | INFO     | registered non reproducing test 0
2025-10-18 15:29:08.317 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1711, output_tokens=513, cost=0.032500
2025-10-18 15:29:10.023 | INFO     | registered non reproducing test 1
2025-10-18 15:29:20.941 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1868, output_tokens=497, cost=0.033590
2025-10-18 15:29:22.640 | INFO     | registered non reproducing test 2
2025-10-18 15:29:22.641 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 15:29:22.643 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:29:24.591 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=614, output_tokens=49, cost=0.007610
2025-10-18 15:29:24.593 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:29:26.808 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=403, output_tokens=28, cost=0.004870
2025-10-18 15:29:26.809 | DEBUG    | Extracted a valid json.
2025-10-18 15:29:26.815 | DEBUG    | Agent proxy return the following json: {
    "API_calls": ["search_method(\"drop_sel\")", "search_class(\"Dataset\")"],
    "bug_locations": []
}
2025-10-18 15:29:26.896 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 15:29:26.897 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 15:29:43.177 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=5378, output_tokens=680, cost=0.074180
2025-10-18 15:29:43.191 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 15:29:49.573 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=6274, output_tokens=238, cost=0.069880
2025-10-18 15:29:49.580 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 15:29:58.122 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=592, output_tokens=236, cost=0.013000
2025-10-18 15:29:58.123 | DEBUG    | Extracted a valid json.
2025-10-18 15:29:58.123 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "xarray/core/dataset.py",
            "class": "Dataset",
            "method": "",
            "intended_behavior": "The intended behavior at this location is to add a new method named `drop_isel` which would allow users to drop indices from a dimension without the need for coordinate labels. The `drop_isel` method should mimic the behavior of the `drop_sel` method but utilize integer-based indexing instead of label-based. This method should be designed to accept integer indices or boolean masks as inputs and remove the corresponding elements from the dataset. It should support multi-dimensional operations and ensure compatibility with the existing API designs. Exception handling should be robust, providing informative errors or warnings when invalid indices are provided, or when an operation cannot be performed due to dataset constraints. The existing functionality of the Dataset class should be preserved, ensuring that this new method integrates seamlessly without affecting the current capabilities. Integration with the dataset's indexing, selection, and manipulation interface should be considered, providing users with a coherent and flexible API for handling data within xarray."
        }
    ]
}
2025-10-18 15:29:58.146 | DEBUG    | Bug location extracted successfully: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
511 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
512     """A multi-dimensional, in memory, array database.
513 
514     A dataset resembles an in-memory representation of a NetCDF file,
515     and consists of variables, coordinates and attributes which
516     together form a self describing dataset.
517 
518     Dataset implements the mapping interface with keys given by variable
519     names and values given by DataArray objects for each variable name.
520 
521     One dimensional variables with name equal to their dimension are
522     index coordinates used for label based indexing.
523 
524     To load data from a file or file-like object, use the `open_dataset`
525     function.
526 
527     Parameters
528     ----------
529     data_vars : dict-like, optional
530         A mapping from variable names to :py:class:`~xarray.DataArray`
531         objects, :py:class:`~xarray.Variable` objects or to tuples of
532         the form ``(dims, data[, attrs])`` which can be used as
533         arguments to create a new ``Variable``. Each dimension must
534         have the same length in all variables in which it appears.
535 
536         The following notations are accepted:
537 
538         - mapping {var name: DataArray}
539         - mapping {var name: Variable}
540         - mapping {var name: (dimension name, array-like)}
541         - mapping {var name: (tuple of dimension names, array-like)}
542         - mapping {dimension name: array-like}
543           (it will be automatically moved to coords, see below)
544 
545         Each dimension must have the same length in all variables in
546         which it appears.
547     coords : dict-like, optional
548         Another mapping in similar form as the `data_vars` argument,
549         except the each item is saved on the dataset as a "coordinate".
550         These variables have an associated meaning: they describe
551         constant/fixed/independent quantities, unlike the
552         varying/measured/dependent quantities that belong in
553         `variables`. Coordinates values may be given by 1-dimensional
554         arrays or scalars, in which case `dims` do not need to be
555         supplied: 1D arrays will be assumed to give index values along
556         the dimension with the same name.
557 
558         The following notations are accepted:
559 
560         - mapping {coord name: DataArray}
561         - mapping {coord name: Variable}
562         - mapping {coord name: (dimension name, array-like)}
563         - mapping {coord name: (tuple of dimension names, array-like)}
564         - mapping {dimension name: array-like}
565           (the dimension name is implicitly set to be the same as the
566           coord name)
567 
568         The last notation implies that the coord name is the same as
569         the dimension name.
570 
571     attrs : dict-like, optional
572         Global attributes to save on this dataset.
573 
574     Examples
575     --------
576     Create data:
577 
578     >>> np.random.seed(0)
579     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
580     >>> precipitation = 10 * np.random.rand(2, 2, 3)
581     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
582     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
583     >>> time = pd.date_range("2014-09-06", periods=3)
584     >>> reference_time = pd.Timestamp("2014-09-05")
585 
586     Initialize a dataset with multiple dimensions:
587 
588     >>> ds = xr.Dataset(
589     ...     data_vars=dict(
590     ...         temperature=(["x", "y", "time"], temperature),
591     ...         precipitation=(["x", "y", "time"], precipitation),
592     ...     ),
593     ...     coords=dict(
594     ...         lon=(["x", "y"], lon),
595     ...         lat=(["x", "y"], lat),
596     ...         time=time,
597     ...         reference_time=reference_time,
598     ...     ),
599     ...     attrs=dict(description="Weather related data."),
600     ... )
601     >>> ds
602     <xarray.Dataset>
603     Dimensions:         (time: 3, x: 2, y: 2)
604     Coordinates:
605         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
606         lat             (x, y) float64 42.25 42.21 42.63 42.59
607       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
608         reference_time  datetime64[ns] 2014-09-05
609     Dimensions without coordinates: x, y
610     Data variables:
611         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
612         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
613     Attributes:
614         description:  Weather related data.
615 
616     Find out where the coldest temperature was and what values the
617     other variables had:
618 
619     >>> ds.isel(ds.temperature.argmin(...))
620     <xarray.Dataset>
621     Dimensions:         ()
622     Coordinates:
623         lon             float64 -99.32
624         lat             float64 42.21
625         time            datetime64[ns] 2014-09-08
626         reference_time  datetime64[ns] 2014-09-05
627     Data variables:
628         temperature     float64 7.182
629         precipitation   float64 8.326
630     Attributes:
631         description:  Weather related data.
632     """
633 
634     _attrs: Optional[Dict[Hashable, Any]]
635     _cache: Dict[str, Any]
636     _coord_names: Set[Hashable]
637     _dims: Dict[Hashable, int]
638     _encoding: Optional[Dict[Hashable, Any]]
639     _indexes: Optional[Dict[Hashable, pd.Index]]
640     _variables: Dict[Hashable, Variable]
641 
642     __slots__ = (
643         "_attrs",
644         "_cache",
645         "_coord_names",
646         "_dims",
647         "_encoding",
648         "_file_obj",
649         "_indexes",
650         "_variables",
651         "__weakref__",
652     )
653 
654     _groupby_cls = groupby.DatasetGroupBy
655     _rolling_cls = rolling.DatasetRolling
656     _coarsen_cls = rolling.DatasetCoarsen
657     _resample_cls = resample.DatasetResample
658     _weighted_cls = weighted.DatasetWeighted
659 
660     def __init__(
661         self,
662         # could make a VariableArgs to use more generally, and refine these
663         # categories
664         data_vars: Mapping[Hashable, Any] = None,
665         coords: Mapping[Hashable, Any] = None,
666         attrs: Mapping[Hashable, Any] = None,
667     ):
668         # TODO(shoyer): expose indexes as a public argument in __init__
669 
670         if data_vars is None:
671             data_vars = {}
672         if coords is None:
673             coords = {}
674 
675         both_data_and_coords = set(data_vars) & set(coords)
676         if both_data_and_coords:
677             raise ValueError(
678                 "variables %r are found in both data_vars and coords"
679                 % both_data_and_coords
680             )
681 
682         if isinstance(coords, Dataset):
683             coords = coords.variables
684 
685         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
686             data_vars, coords, compat="broadcast_equals"
687         )
688 
689         self._attrs = dict(attrs) if attrs is not None else None
690         self._file_obj = None
691         self._encoding = None
692         self._variables = variables
693         self._coord_names = coord_names
694         self._dims = dims
695         self._indexes = indexes
696 
697     @classmethod
698     def load_store(cls, store, decoder=None) -> "Dataset":
699         """Create a new dataset from the contents of a backends.*DataStore
700         object
701         """
702         variables, attributes = store.load()
703         if decoder:
704             variables, attributes = decoder(variables, attributes)
705         obj = cls(variables, attrs=attributes)
706         obj._file_obj = store
707         return obj
708 
709     @property
710     def variables(self) -> Mapping[Hashable, Variable]:
711         """Low level interface to Dataset contents as dict of Variable objects.
712 
713         This ordered dictionary is frozen to prevent mutation that could
714         violate Dataset invariants. It contains all variable objects
715         constituting the Dataset, including both data variables and
716         coordinates.
717         """
718         return Frozen(self._variables)
719 
720     @property
721     def attrs(self) -> Dict[Hashable, Any]:
722         """Dictionary of global attributes on this dataset"""
723         if self._attrs is None:
724             self._attrs = {}
725         return self._attrs
726 
727     @attrs.setter
728     def attrs(self, value: Mapping[Hashable, Any]) -> None:
729         self._attrs = dict(value)
730 
731     @property
732     def encoding(self) -> Dict:
733         """Dictionary of global encoding attributes on this dataset"""
734         if self._encoding is None:
735             self._encoding = {}
736         return self._encoding
737 
738     @encoding.setter
739     def encoding(self, value: Mapping) -> None:
740         self._encoding = dict(value)
741 
742     @property
743     def dims(self) -> Mapping[Hashable, int]:
744         """Mapping from dimension names to lengths.
745 
746         Cannot be modified directly, but is updated when adding new variables.
747 
748         Note that type of this object differs from `DataArray.dims`.
749         See `Dataset.sizes` and `DataArray.sizes` for consistently named
750         properties.
751         """
752         return Frozen(SortedKeysDict(self._dims))
753 
754     @property
755     def sizes(self) -> Mapping[Hashable, int]:
756         """Mapping from dimension names to lengths.
757 
758         Cannot be modified directly, but is updated when adding new variables.
759 
760         This is an alias for `Dataset.dims` provided for the benefit of
761         consistency with `DataArray.sizes`.
762 
763         See also
764         --------
765         DataArray.sizes
766         """
767         return self.dims
768 
769     def load(self, **kwargs) -> "Dataset":
770         """Manually trigger loading and/or computation of this dataset's data
771         from disk or a remote source into memory and return this dataset.
772         Unlike compute, the original dataset is modified and returned.
773 
774         Normally, it should not be necessary to call this method in user code,
775         because all xarray functions should either work on deferred data or
776         load data automatically. However, this method can be necessary when
777         working with many file objects on disk.
778 
779         Parameters
780         ----------
781         **kwargs : dict
782             Additional keyword arguments passed on to ``dask.compute``.
783 
784         See Also
785         --------
786         dask.compute
787         """
788         # access .data to coerce everything to numpy or dask arrays
789         lazy_data = {
790             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
791         }
792         if lazy_data:
793             import dask.array as da
794 
795             # evaluate all the dask arrays simultaneously
796             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
797 
798             for k, data in zip(lazy_data, evaluated_data):
799                 self.variables[k].data = data
800 
801         # load everything else sequentially
802         for k, v in self.variables.items():
803             if k not in lazy_data:
804                 v.load()
805 
806         return self
807 
808     def __dask_tokenize__(self):
809         from dask.base import normalize_token
810 
811         return normalize_token(
812             (type(self), self._variables, self._coord_names, self._attrs)
813         )
814 
815     def __dask_graph__(self):
816         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
817         graphs = {k: v for k, v in graphs.items() if v is not None}
818         if not graphs:
819             return None
820         else:
821             try:
822                 from dask.highlevelgraph import HighLevelGraph
823 
824                 return HighLevelGraph.merge(*graphs.values())
825             except ImportError:
826                 from dask import sharedict
827 
828                 return sharedict.merge(*graphs.values())
829 
830     def __dask_keys__(self):
831         import dask
832 
833         return [
834             v.__dask_keys__()
835             for v in self.variables.values()
836             if dask.is_dask_collection(v)
837         ]
838 
839     def __dask_layers__(self):
840         import dask
841 
842         return sum(
843             [
844                 v.__dask_layers__()
845                 for v in self.variables.values()
846                 if dask.is_dask_collection(v)
847             ],
848             (),
849         )
850 
851     @property
852     def __dask_optimize__(self):
853         import dask.array as da
854 
855         return da.Array.__dask_optimize__
856 
857     @property
858     def __dask_scheduler__(self):
859         import dask.array as da
860 
861         return da.Array.__dask_scheduler__
862 
863     def __dask_postcompute__(self):
864         import dask
865 
866         info = [
867             (True, k, v.__dask_postcompute__())
868             if dask.is_dask_collection(v)
869             else (False, k, v)
870             for k, v in self._variables.items()
871         ]
872         args = (
873             info,
874             self._coord_names,
875             self._dims,
876             self._attrs,
877             self._indexes,
878             self._encoding,
879             self._file_obj,
880         )
881         return self._dask_postcompute, args
882 
883     def __dask_postpersist__(self):
884         import dask
885 
886         info = [
887             (True, k, v.__dask_postpersist__())
888             if dask.is_dask_collection(v)
889             else (False, k, v)
890             for k, v in self._variables.items()
891         ]
892         args = (
893             info,
894             self._coord_names,
895             self._dims,
896             self._attrs,
897             self._indexes,
898             self._encoding,
899             self._file_obj,
900         )
901         return self._dask_postpersist, args
902 
903     @staticmethod
904     def _dask_postcompute(results, info, *args):
905         variables = {}
906         results2 = list(results[::-1])
907         for is_dask, k, v in info:
908             if is_dask:
909                 func, args2 = v
910                 r = results2.pop()
911                 result = func(r, *args2)
912             else:
913                 result = v
914             variables[k] = result
915 
916         final = Dataset._construct_direct(variables, *args)
917         return final
918 
919     @staticmethod
920     def _dask_postpersist(dsk, info, *args):
921         variables = {}
922         # postpersist is called in both dask.optimize and dask.persist
923         # When persisting, we want to filter out unrelated keys for
924         # each Variable's task graph.
925         is_persist = len(dsk) == len(info)
926         for is_dask, k, v in info:
927             if is_dask:
928                 func, args2 = v
929                 if is_persist:
930                     name = args2[1][0]
931                     dsk2 = {k: v for k, v in dsk.items() if k[0] == name}
932                 else:
933                     dsk2 = dsk
934                 result = func(dsk2, *args2)
935             else:
936                 result = v
937             variables[k] = result
938 
939         return Dataset._construct_direct(variables, *args)
940 
941     def compute(self, **kwargs) -> "Dataset":
942         """Manually trigger loading and/or computation of this dataset's data
943         from disk or a remote source into memory and return a new dataset.
944         Unlike load, the original dataset is left unaltered.
945 
946         Normally, it should not be necessary to call this method in user code,
947         because all xarray functions should either work on deferred data or
948         load data automatically. However, this method can be necessary when
949         working with many file objects on disk.
950 
951         Parameters
952         ----------
953         **kwargs : dict
954             Additional keyword arguments passed on to ``dask.compute``.
955 
956         See Also
957         --------
958         dask.compute
959         """
960         new = self.copy(deep=False)
961         return new.load(**kwargs)
962 
963     def _persist_inplace(self, **kwargs) -> "Dataset":
964         """Persist all Dask arrays in memory"""
965         # access .data to coerce everything to numpy or dask arrays
966         lazy_data = {
967             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
968         }
969         if lazy_data:
970             import dask
971 
972             # evaluate all the dask arrays simultaneously
973             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
974 
975             for k, data in zip(lazy_data, evaluated_data):
976                 self.variables[k].data = data
977 
978         return self
979 
980     def persist(self, **kwargs) -> "Dataset":
981         """Trigger computation, keeping data as dask arrays
982 
983         This operation can be used to trigger computation on underlying dask
984         arrays, similar to ``.compute()`` or ``.load()``.  However this
985         operation keeps the data as dask arrays. This is particularly useful
986         when using the dask.distributed scheduler and you want to load a large
987         amount of data into distributed memory.
988 
989         Parameters
990         ----------
991         **kwargs : dict
992             Additional keyword arguments passed on to ``dask.persist``.
993 
994         See Also
995         --------
996         dask.persist
997         """
998         new = self.copy(deep=False)
999         return new._persist_inplace(**kwargs)
1000 
1001     @classmethod
1002     def _construct_direct(
1003         cls,
1004         variables,
1005         coord_names,
1006         dims=None,
1007         attrs=None,
1008         indexes=None,
1009         encoding=None,
1010         file_obj=None,
1011     ):
1012         """Shortcut around __init__ for internal use when we want to skip
1013         costly validation
1014         """
1015         if dims is None:
1016             dims = calculate_dimensions(variables)
1017         obj = object.__new__(cls)
1018         obj._variables = variables
1019         obj._coord_names = coord_names
1020         obj._dims = dims
1021         obj._indexes = indexes
1022         obj._attrs = attrs
1023         obj._file_obj = file_obj
1024         obj._encoding = encoding
1025         return obj
1026 
1027     def _replace(
1028         self,
1029         variables: Dict[Hashable, Variable] = None,
1030         coord_names: Set[Hashable] = None,
1031         dims: Dict[Any, int] = None,
1032         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1033         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
1034         encoding: Union[dict, None, Default] = _default,
1035         inplace: bool = False,
1036     ) -> "Dataset":
1037         """Fastpath constructor for internal use.
1038 
1039         Returns an object with optionally with replaced attributes.
1040 
1041         Explicitly passed arguments are *not* copied when placed on the new
1042         dataset. It is up to the caller to ensure that they have the right type
1043         and are not used elsewhere.
1044         """
1045         if inplace:
1046             if variables is not None:
1047                 self._variables = variables
1048             if coord_names is not None:
1049                 self._coord_names = coord_names
1050             if dims is not None:
1051                 self._dims = dims
1052             if attrs is not _default:
1053                 self._attrs = attrs
1054             if indexes is not _default:
1055                 self._indexes = indexes
1056             if encoding is not _default:
1057                 self._encoding = encoding
1058             obj = self
1059         else:
1060             if variables is None:
1061                 variables = self._variables.copy()
1062             if coord_names is None:
1063                 coord_names = self._coord_names.copy()
1064             if dims is None:
1065                 dims = self._dims.copy()
1066             if attrs is _default:
1067                 attrs = copy.copy(self._attrs)
1068             if indexes is _default:
1069                 indexes = copy.copy(self._indexes)
1070             if encoding is _default:
1071                 encoding = copy.copy(self._encoding)
1072             obj = self._construct_direct(
1073                 variables, coord_names, dims, attrs, indexes, encoding
1074             )
1075         return obj
1076 
1077     def _replace_with_new_dims(
1078         self,
1079         variables: Dict[Hashable, Variable],
1080         coord_names: set = None,
1081         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1082         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
1083         inplace: bool = False,
1084     ) -> "Dataset":
1085         """Replace variables with recalculated dimensions."""
1086         dims = calculate_dimensions(variables)
1087         return self._replace(
1088             variables, coord_names, dims, attrs, indexes, inplace=inplace
1089         )
1090 
1091     def _replace_vars_and_dims(
1092         self,
1093         variables: Dict[Hashable, Variable],
1094         coord_names: set = None,
1095         dims: Dict[Hashable, int] = None,
1096         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1097         inplace: bool = False,
1098     ) -> "Dataset":
1099         """Deprecated version of _replace_with_new_dims().
1100 
1101         Unlike _replace_with_new_dims(), this method always recalculates
1102         indexes from variables.
1103         """
1104         if dims is None:
1105             dims = calculate_dimensions(variables)
1106         return self._replace(
1107             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1108         )
1109 
1110     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
1111         if not indexes:
1112             return self
1113 
1114         variables = self._variables.copy()
1115         new_indexes = dict(self.indexes)
1116         for name, idx in indexes.items():
1117             variables[name] = IndexVariable(name, idx)
1118             new_indexes[name] = idx
1119         obj = self._replace(variables, indexes=new_indexes)
1120 
1121         # switch from dimension to level names, if necessary
1122         dim_names: Dict[Hashable, str] = {}
1123         for dim, idx in indexes.items():
1124             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
1125                 dim_names[dim] = idx.name
1126         if dim_names:
1127             obj = obj.rename(dim_names)
1128         return obj
1129 
1130     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
1131         """Returns a copy of this dataset.
1132 
1133         If `deep=True`, a deep copy is made of each of the component variables.
1134         Otherwise, a shallow copy of each of the component variable is made, so
1135         that the underlying memory region of the new dataset is the same as in
1136         the original dataset.
1137 
1138         Use `data` to create a new object with the same structure as
1139         original but entirely new data.
1140 
1141         Parameters
1142         ----------
1143         deep : bool, optional
1144             Whether each component variable is loaded into memory and copied onto
1145             the new object. Default is False.
1146         data : dict-like, optional
1147             Data to use in the new object. Each item in `data` must have same
1148             shape as corresponding data variable in original. When `data` is
1149             used, `deep` is ignored for the data variables and only used for
1150             coords.
1151 
1152         Returns
1153         -------
1154         object : Dataset
1155             New object with dimensions, attributes, coordinates, name, encoding,
1156             and optionally data copied from original.
1157 
1158         Examples
1159         --------
1160 
1161         Shallow copy versus deep copy
1162 
1163         >>> da = xr.DataArray(np.random.randn(2, 3))
1164         >>> ds = xr.Dataset(
1165         ...     {"foo": da, "bar": ("x", [-1, 2])},
1166         ...     coords={"x": ["one", "two"]},
1167         ... )
1168         >>> ds.copy()
1169         <xarray.Dataset>
1170         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1171         Coordinates:
1172           * x        (x) <U3 'one' 'two'
1173         Dimensions without coordinates: dim_0, dim_1
1174         Data variables:
1175             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1176             bar      (x) int64 -1 2
1177 
1178         >>> ds_0 = ds.copy(deep=False)
1179         >>> ds_0["foo"][0, 0] = 7
1180         >>> ds_0
1181         <xarray.Dataset>
1182         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1183         Coordinates:
1184           * x        (x) <U3 'one' 'two'
1185         Dimensions without coordinates: dim_0, dim_1
1186         Data variables:
1187             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1188             bar      (x) int64 -1 2
1189 
1190         >>> ds
1191         <xarray.Dataset>
1192         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1193         Coordinates:
1194           * x        (x) <U3 'one' 'two'
1195         Dimensions without coordinates: dim_0, dim_1
1196         Data variables:
1197             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1198             bar      (x) int64 -1 2
1199 
1200         Changing the data using the ``data`` argument maintains the
1201         structure of the original object, but with the new data. Original
1202         object is unaffected.
1203 
1204         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1205         <xarray.Dataset>
1206         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1207         Coordinates:
1208           * x        (x) <U3 'one' 'two'
1209         Dimensions without coordinates: dim_0, dim_1
1210         Data variables:
1211             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1212             bar      (x) <U1 'a' 'b'
1213 
1214         >>> ds
1215         <xarray.Dataset>
1216         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1217         Coordinates:
1218           * x        (x) <U3 'one' 'two'
1219         Dimensions without coordinates: dim_0, dim_1
1220         Data variables:
1221             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1222             bar      (x) int64 -1 2
1223 
1224         See Also
1225         --------
1226         pandas.DataFrame.copy
1227         """
1228         if data is None:
1229             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1230         elif not utils.is_dict_like(data):
1231             raise ValueError("Data must be dict-like")
1232         else:
1233             var_keys = set(self.data_vars.keys())
1234             data_keys = set(data.keys())
1235             keys_not_in_vars = data_keys - var_keys
1236             if keys_not_in_vars:
1237                 raise ValueError(
1238                     "Data must only contain variables in original "
1239                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1240                 )
1241             keys_missing_from_data = var_keys - data_keys
1242             if keys_missing_from_data:
1243                 raise ValueError(
1244                     "Data must contain all variables in original "
1245                     "dataset. Data is missing {}".format(keys_missing_from_data)
1246                 )
1247             variables = {
1248                 k: v.copy(deep=deep, data=data.get(k))
1249                 for k, v in self._variables.items()
1250             }
1251 
1252         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1253 
1254         return self._replace(variables, attrs=attrs)
1255 
1256     @property
1257     def _level_coords(self) -> Dict[str, Hashable]:
1258         """Return a mapping of all MultiIndex levels and their corresponding
1259         coordinate name.
1260         """
1261         level_coords: Dict[str, Hashable] = {}
1262         for name, index in self.indexes.items():
1263             if isinstance(index, pd.MultiIndex):
1264                 level_names = index.names
1265                 (dim,) = self.variables[name].dims
1266                 level_coords.update({lname: dim for lname in level_names})
1267         return level_coords
1268 
1269     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1270         """Create a new Dataset with the listed variables from this dataset and
1271         the all relevant coordinates. Skips all validation.
1272         """
1273         variables: Dict[Hashable, Variable] = {}
1274         coord_names = set()
1275         indexes: Dict[Hashable, pd.Index] = {}
1276 
1277         for name in names:
1278             try:
1279                 variables[name] = self._variables[name]
1280             except KeyError:
1281                 ref_name, var_name, var = _get_virtual_variable(
1282                     self._variables, name, self._level_coords, self.dims
1283                 )
1284                 variables[var_name] = var
1285                 if ref_name in self._coord_names or ref_name in self.dims:
1286                     coord_names.add(var_name)
1287                 if (var_name,) == var.dims:
1288                     indexes[var_name] = var.to_index()
1289 
1290         needed_dims: Set[Hashable] = set()
1291         for v in variables.values():
1292             needed_dims.update(v.dims)
1293 
1294         dims = {k: self.dims[k] for k in needed_dims}
1295 
1296         # preserves ordering of coordinates
1297         for k in self._variables:
1298             if k not in self._coord_names:
1299                 continue
1300 
1301             if set(self.variables[k].dims) <= needed_dims:
1302                 variables[k] = self._variables[k]
1303                 coord_names.add(k)
1304                 if k in self.indexes:
1305                     indexes[k] = self.indexes[k]
1306 
1307         return self._replace(variables, coord_names, dims, indexes=indexes)
1308 
1309     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1310         """Construct a DataArray by indexing this dataset"""
1311         from .dataarray import DataArray
1312 
1313         try:
1314             variable = self._variables[name]
1315         except KeyError:
1316             _, name, variable = _get_virtual_variable(
1317                 self._variables, name, self._level_coords, self.dims
1318             )
1319 
1320         needed_dims = set(variable.dims)
1321 
1322         coords: Dict[Hashable, Variable] = {}
1323         # preserve ordering
1324         for k in self._variables:
1325             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1326                 coords[k] = self.variables[k]
1327 
1328         if self._indexes is None:
1329             indexes = None
1330         else:
1331             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1332 
1333         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1334 
1335     def __copy__(self) -> "Dataset":
1336         return self.copy(deep=False)
1337 
1338     def __deepcopy__(self, memo=None) -> "Dataset":
1339         # memo does nothing but is required for compatibility with
1340         # copy.deepcopy
1341         return self.copy(deep=True)
1342 
1343     @property
1344     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1345         """Places to look-up items for attribute-style access"""
1346         yield from self._item_sources
1347         yield self.attrs
1348 
1349     @property
1350     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1351         """Places to look-up items for key-completion"""
1352         yield self.data_vars
1353         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1354 
1355         # virtual coordinates
1356         yield HybridMappingProxy(keys=self.dims, mapping=self)
1357 
1358         # uses empty dict -- everything here can already be found in self.coords.
1359         yield HybridMappingProxy(keys=self._level_coords, mapping={})
1360 
1361     def __contains__(self, key: object) -> bool:
1362         """The 'in' operator will return true or false depending on whether
1363         'key' is an array in the dataset or not.
1364         """
1365         return key in self._variables
1366 
1367     def __len__(self) -> int:
1368         return len(self.data_vars)
1369 
1370     def __bool__(self) -> bool:
1371         return bool(self.data_vars)
1372 
1373     def __iter__(self) -> Iterator[Hashable]:
1374         return iter(self.data_vars)
1375 
1376     def __array__(self, dtype=None):
1377         raise TypeError(
1378             "cannot directly convert an xarray.Dataset into a "
1379             "numpy array. Instead, create an xarray.DataArray "
1380             "first, either with indexing on the Dataset or by "
1381             "invoking the `to_array()` method."
1382         )
1383 
1384     @property
1385     def nbytes(self) -> int:
1386         return sum(v.nbytes for v in self.variables.values())
1387 
1388     @property
1389     def loc(self) -> _LocIndexer:
1390         """Attribute for location based indexing. Only supports __getitem__,
1391         and only when the key is a dict of the form {dim: labels}.
1392         """
1393         return _LocIndexer(self)
1394 
1395     # FIXME https://github.com/python/mypy/issues/7328
1396     @overload
1397     def __getitem__(self, key: Mapping) -> "Dataset":  # type: ignore
1398         ...
1399 
1400     @overload
1401     def __getitem__(self, key: Hashable) -> "DataArray":  # type: ignore
1402         ...
1403 
1404     @overload
1405     def __getitem__(self, key: Any) -> "Dataset":
1406         ...
1407 
1408     def __getitem__(self, key):
1409         """Access variables or coordinates this dataset as a
1410         :py:class:`~xarray.DataArray`.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**cast(Mapping, key))
1416 
1417         if hashable(key):
1418             return self._construct_dataarray(key)
1419         else:
1420             return self._copy_listed(np.asarray(key))
1421 
1422     def __setitem__(self, key: Hashable, value) -> None:
1423         """Add an array to this dataset.
1424 
1425         If value is a `DataArray`, call its `select_vars()` method, rename it
1426         to `key` and merge the contents of the resulting dataset into this
1427         dataset.
1428 
1429         If value is an `Variable` object (or tuple of form
1430         ``(dims, data[, attrs])``), add it to this dataset as a new
1431         variable.
1432         """
1433         if utils.is_dict_like(key):
1434             raise NotImplementedError(
1435                 "cannot yet use a dictionary as a key to set Dataset values"
1436             )
1437 
1438         self.update({key: value})
1439 
1440     def __delitem__(self, key: Hashable) -> None:
1441         """Remove a variable from this dataset."""
1442         del self._variables[key]
1443         self._coord_names.discard(key)
1444         if key in self.indexes:
1445             assert self._indexes is not None
1446             del self._indexes[key]
1447         self._dims = calculate_dimensions(self._variables)
1448 
1449     # mutable objects should not be hashable
1450     # https://github.com/python/mypy/issues/4266
1451     __hash__ = None  # type: ignore
1452 
1453     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1454         """Helper function for equals and identical"""
1455 
1456         # some stores (e.g., scipy) do not seem to preserve order, so don't
1457         # require matching order for equality
1458         def compat(x: Variable, y: Variable) -> bool:
1459             return getattr(x, compat_str)(y)
1460 
1461         return self._coord_names == other._coord_names and utils.dict_equiv(
1462             self._variables, other._variables, compat=compat
1463         )
1464 
1465     def broadcast_equals(self, other: "Dataset") -> bool:
1466         """Two Datasets are broadcast equal if they are equal after
1467         broadcasting all variables against each other.
1468 
1469         For example, variables that are scalar in one dataset but non-scalar in
1470         the other dataset can still be broadcast equal if the the non-scalar
1471         variable is a constant.
1472 
1473         See Also
1474         --------
1475         Dataset.equals
1476         Dataset.identical
1477         """
1478         try:
1479             return self._all_compat(other, "broadcast_equals")
1480         except (TypeError, AttributeError):
1481             return False
1482 
1483     def equals(self, other: "Dataset") -> bool:
1484         """Two Datasets are equal if they have matching variables and
1485         coordinates, all of which are equal.
1486 
1487         Datasets can still be equal (like pandas objects) if they have NaN
1488         values in the same locations.
1489 
1490         This method is necessary because `v1 == v2` for ``Dataset``
1491         does element-wise comparisons (like numpy.ndarrays).
1492 
1493         See Also
1494         --------
1495         Dataset.broadcast_equals
1496         Dataset.identical
1497         """
1498         try:
1499             return self._all_compat(other, "equals")
1500         except (TypeError, AttributeError):
1501             return False
1502 
1503     def identical(self, other: "Dataset") -> bool:
1504         """Like equals, but also checks all dataset attributes and the
1505         attributes on all variables and coordinates.
1506 
1507         See Also
1508         --------
1509         Dataset.broadcast_equals
1510         Dataset.equals
1511         """
1512         try:
1513             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1514                 other, "identical"
1515             )
1516         except (TypeError, AttributeError):
1517             return False
1518 
1519     @property
1520     def indexes(self) -> Indexes:
1521         """Mapping of pandas.Index objects used for label based indexing"""
1522         if self._indexes is None:
1523             self._indexes = default_indexes(self._variables, self._dims)
1524         return Indexes(self._indexes)
1525 
1526     @property
1527     def coords(self) -> DatasetCoordinates:
1528         """Dictionary of xarray.DataArray objects corresponding to coordinate
1529         variables
1530         """
1531         return DatasetCoordinates(self)
1532 
1533     @property
1534     def data_vars(self) -> DataVariables:
1535         """Dictionary of DataArray objects corresponding to data variables"""
1536         return DataVariables(self)
1537 
1538     def set_coords(self, names: "Union[Hashable, Iterable[Hashable]]") -> "Dataset":
1539         """Given names of one or more variables, set them as coordinates
1540 
1541         Parameters
1542         ----------
1543         names : hashable or iterable of hashable
1544             Name(s) of variables in this dataset to convert into coordinates.
1545 
1546         Returns
1547         -------
1548         Dataset
1549 
1550         See also
1551         --------
1552         Dataset.swap_dims
1553         """
1554         # TODO: allow inserting new coordinates with this method, like
1555         # DataFrame.set_index?
1556         # nb. check in self._variables, not self.data_vars to insure that the
1557         # operation is idempotent
1558         if isinstance(names, str) or not isinstance(names, Iterable):
1559             names = [names]
1560         else:
1561             names = list(names)
1562         self._assert_all_in_dataset(names)
1563         obj = self.copy()
1564         obj._coord_names.update(names)
1565         return obj
1566 
1567     def reset_coords(
1568         self,
1569         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1570         drop: bool = False,
1571     ) -> "Dataset":
1572         """Given names of coordinates, reset them to become variables
1573 
1574         Parameters
1575         ----------
1576         names : hashable or iterable of hashable, optional
1577             Name(s) of non-index coordinates in this dataset to reset into
1578             variables. By default, all non-index coordinates are reset.
1579         drop : bool, optional
1580             If True, remove coordinates instead of converting them into
1581             variables.
1582 
1583         Returns
1584         -------
1585         Dataset
1586         """
1587         if names is None:
1588             names = self._coord_names - set(self.dims)
1589         else:
1590             if isinstance(names, str) or not isinstance(names, Iterable):
1591                 names = [names]
1592             else:
1593                 names = list(names)
1594             self._assert_all_in_dataset(names)
1595             bad_coords = set(names) & set(self.dims)
1596             if bad_coords:
1597                 raise ValueError(
1598                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1599                 )
1600         obj = self.copy()
1601         obj._coord_names.difference_update(names)
1602         if drop:
1603             for name in names:
1604                 del obj._variables[name]
1605         return obj
1606 
1607     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1608         """Store dataset contents to a backends.*DataStore object."""
1609         from ..backends.api import dump_to_store
1610 
1611         # TODO: rename and/or cleanup this method to make it more consistent
1612         # with to_netcdf()
1613         dump_to_store(self, store, **kwargs)
1614 
1615     def to_netcdf(
1616         self,
1617         path=None,
1618         mode: str = "w",
1619         format: str = None,
1620         group: str = None,
1621         engine: str = None,
1622         encoding: Mapping = None,
1623         unlimited_dims: Iterable[Hashable] = None,
1624         compute: bool = True,
1625         invalid_netcdf: bool = False,
1626     ) -> Union[bytes, "Delayed", None]:
1627         """Write dataset contents to a netCDF file.
1628 
1629         Parameters
1630         ----------
1631         path : str, Path or file-like, optional
1632             Path to which to save this dataset. File-like objects are only
1633             supported by the scipy engine. If no path is provided, this
1634             function returns the resulting netCDF file as bytes; in this case,
1635             we need to use scipy, which does not support netCDF version 4 (the
1636             default format becomes NETCDF3_64BIT).
1637         mode : {"w", "a"}, default: "w"
1638             Write ('w') or append ('a') mode. If mode='w', any existing file at
1639             this location will be overwritten. If mode='a', existing variables
1640             will be overwritten.
1641         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1642                   "NETCDF3_CLASSIC"}, optional
1643             File format for the resulting netCDF file:
1644 
1645             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1646               features.
1647             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1648               netCDF 3 compatible API features.
1649             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1650               which fully supports 2+ GB files, but is only compatible with
1651               clients linked against netCDF version 3.6.0 or later.
1652             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1653               handle 2+ GB files very well.
1654 
1655             All formats are supported by the netCDF4-python library.
1656             scipy.io.netcdf only supports the last two formats.
1657 
1658             The default format is NETCDF4 if you are saving a file to disk and
1659             have the netCDF4-python library available. Otherwise, xarray falls
1660             back to using scipy to write netCDF files and defaults to the
1661             NETCDF3_64BIT format (scipy does not support netCDF4).
1662         group : str, optional
1663             Path to the netCDF4 group in the given file to open (only works for
1664             format='NETCDF4'). The group(s) will be created if necessary.
1665         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1666             Engine to use when writing netCDF files. If not provided, the
1667             default engine is chosen based on available dependencies, with a
1668             preference for 'netcdf4' if writing to a file on disk.
1669         encoding : dict, optional
1670             Nested dictionary with variable names as keys and dictionaries of
1671             variable specific encodings as values, e.g.,
1672             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1673             "zlib": True}, ...}``
1674 
1675             The `h5netcdf` engine supports both the NetCDF4-style compression
1676             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1677             ones ``{"compression": "gzip", "compression_opts": 9}``.
1678             This allows using any compression plugin installed in the HDF5
1679             library, e.g. LZF.
1680 
1681         unlimited_dims : iterable of hashable, optional
1682             Dimension(s) that should be serialized as unlimited dimensions.
1683             By default, no dimensions are treated as unlimited dimensions.
1684             Note that unlimited_dims may also be set via
1685             ``dataset.encoding["unlimited_dims"]``.
1686         compute: bool, default: True
1687             If true compute immediately, otherwise return a
1688             ``dask.delayed.Delayed`` object that can be computed later.
1689         invalid_netcdf: bool, default: False
1690             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1691             hdf5 files which are invalid netcdf as described in
1692             https://github.com/shoyer/h5netcdf.
1693         """
1694         if encoding is None:
1695             encoding = {}
1696         from ..backends.api import to_netcdf
1697 
1698         return to_netcdf(
1699             self,
1700             path,
1701             mode,
1702             format=format,
1703             group=group,
1704             engine=engine,
1705             encoding=encoding,
1706             unlimited_dims=unlimited_dims,
1707             compute=compute,
1708             invalid_netcdf=invalid_netcdf,
1709         )
1710 
1711     def to_zarr(
1712         self,
1713         store: Union[MutableMapping, str, Path] = None,
1714         chunk_store: Union[MutableMapping, str, Path] = None,
1715         mode: str = None,
1716         synchronizer=None,
1717         group: str = None,
1718         encoding: Mapping = None,
1719         compute: bool = True,
1720         consolidated: bool = False,
1721         append_dim: Hashable = None,
1722         region: Mapping[str, slice] = None,
1723     ) -> "ZarrStore":
1724         """Write dataset contents to a zarr group.
1725 
1726         .. note:: Experimental
1727                   The Zarr backend is new and experimental. Please report any
1728                   unexpected behavior via github issues.
1729 
1730         Parameters
1731         ----------
1732         store : MutableMapping, str or Path, optional
1733             Store or path to directory in file system.
1734         chunk_store : MutableMapping, str or Path, optional
1735             Store or path to directory in file system only for Zarr array chunks.
1736             Requires zarr-python v2.4.0 or later.
1737         mode : {"w", "w-", "a", None}, optional
1738             Persistence mode: "w" means create (overwrite if exists);
1739             "w-" means create (fail if exists);
1740             "a" means override existing variables (create if does not exist).
1741             If ``append_dim`` is set, ``mode`` can be omitted as it is
1742             internally set to ``"a"``. Otherwise, ``mode`` will default to
1743             `w-` if not set.
1744         synchronizer : object, optional
1745             Zarr array synchronizer.
1746         group : str, optional
1747             Group path. (a.k.a. `path` in zarr terminology.)
1748         encoding : dict, optional
1749             Nested dictionary with variable names as keys and dictionaries of
1750             variable specific encodings as values, e.g.,
1751             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1752         compute: bool, optional
1753             If True write array data immediately, otherwise return a
1754             ``dask.delayed.Delayed`` object that can be computed to write
1755             array data later. Metadata is always updated eagerly.
1756         consolidated: bool, optional
1757             If True, apply zarr's `consolidate_metadata` function to the store
1758             after writing metadata.
1759         append_dim: hashable, optional
1760             If set, the dimension along which the data will be appended. All
1761             other dimensions on overriden variables must remain the same size.
1762         region: dict, optional
1763             Optional mapping from dimension names to integer slices along
1764             dataset dimensions to indicate the region of existing zarr array(s)
1765             in which to write this dataset's data. For example,
1766             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
1767             that values should be written to the region ``0:1000`` along ``x``
1768             and ``10000:11000`` along ``y``.
1769 
1770             Two restrictions apply to the use of ``region``:
1771 
1772             - If ``region`` is set, _all_ variables in a dataset must have at
1773               least one dimension in common with the region. Other variables
1774               should be written in a separate call to ``to_zarr()``.
1775             - Dimensions cannot be included in both ``region`` and
1776               ``append_dim`` at the same time. To create empty arrays to fill
1777               in with ``region``, use a separate call to ``to_zarr()`` with
1778               ``compute=False``. See "Appending to existing Zarr stores" in
1779               the reference documentation for full details.
1780 
1781         References
1782         ----------
1783         https://zarr.readthedocs.io/
1784 
1785         Notes
1786         -----
1787         Zarr chunking behavior:
1788             If chunks are found in the encoding argument or attribute
1789             corresponding to any DataArray, those chunks are used.
1790             If a DataArray is a dask array, it is written with those chunks.
1791             If not other chunks are found, Zarr uses its own heuristics to
1792             choose automatic chunk sizes.
1793         """
1794         from ..backends.api import to_zarr
1795 
1796         if encoding is None:
1797             encoding = {}
1798 
1799         return to_zarr(
1800             self,
1801             store=store,
1802             chunk_store=chunk_store,
1803             mode=mode,
1804             synchronizer=synchronizer,
1805             group=group,
1806             encoding=encoding,
1807             compute=compute,
1808             consolidated=consolidated,
1809             append_dim=append_dim,
1810             region=region,
1811         )
1812 
1813     def __repr__(self) -> str:
1814         return formatting.dataset_repr(self)
1815 
1816     def _repr_html_(self):
1817         if OPTIONS["display_style"] == "text":
1818             return f"<pre>{escape(repr(self))}</pre>"
1819         return formatting_html.dataset_repr(self)
1820 
1821     def info(self, buf=None) -> None:
1822         """
1823         Concise summary of a Dataset variables and attributes.
1824 
1825         Parameters
1826         ----------
1827         buf : file-like, default: sys.stdout
1828             writable buffer
1829 
1830         See Also
1831         --------
1832         pandas.DataFrame.assign
1833         ncdump: netCDF's ncdump
1834         """
1835         if buf is None:  # pragma: no cover
1836             buf = sys.stdout
1837 
1838         lines = []
1839         lines.append("xarray.Dataset {")
1840         lines.append("dimensions:")
1841         for name, size in self.dims.items():
1842             lines.append(f"\t{name} = {size} ;")
1843         lines.append("\nvariables:")
1844         for name, da in self.variables.items():
1845             dims = ", ".join(da.dims)
1846             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1847             for k, v in da.attrs.items():
1848                 lines.append(f"\t\t{name}:{k} = {v} ;")
1849         lines.append("\n// global attributes:")
1850         for k, v in self.attrs.items():
1851             lines.append(f"\t:{k} = {v} ;")
1852         lines.append("}")
1853 
1854         buf.write("\n".join(lines))
1855 
1856     @property
1857     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1858         """Block dimensions for this dataset's data or None if it's not a dask
1859         array.
1860         """
1861         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1862         for v in self.variables.values():
1863             if v.chunks is not None:
1864                 for dim, c in zip(v.dims, v.chunks):
1865                     if dim in chunks and c != chunks[dim]:
1866                         raise ValueError(
1867                             f"Object has inconsistent chunks along dimension {dim}. "
1868                             "This can be fixed by calling unify_chunks()."
1869                         )
1870                     chunks[dim] = c
1871         return Frozen(SortedKeysDict(chunks))
1872 
1873     def chunk(
1874         self,
1875         chunks: Union[
1876             Number,
1877             str,
1878             Mapping[Hashable, Union[None, Number, str, Tuple[Number, ...]]],
1879         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1880         name_prefix: str = "xarray-",
1881         token: str = None,
1882         lock: bool = False,
1883     ) -> "Dataset":
1884         """Coerce all arrays in this dataset into dask arrays with the given
1885         chunks.
1886 
1887         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1888         arrays will be rechunked to the given chunk sizes.
1889 
1890         If neither chunks is not provided for one or more dimensions, chunk
1891         sizes along that dimension will not be updated; non-dask arrays will be
1892         converted into dask arrays with a single block.
1893 
1894         Parameters
1895         ----------
1896         chunks : int, 'auto' or mapping, optional
1897             Chunk sizes along each dimension, e.g., ``5`` or
1898             ``{"x": 5, "y": 5}``.
1899         name_prefix : str, optional
1900             Prefix for the name of any new dask arrays.
1901         token : str, optional
1902             Token uniquely identifying this dataset.
1903         lock : optional
1904             Passed on to :py:func:`dask.array.from_array`, if the array is not
1905             already as dask array.
1906 
1907         Returns
1908         -------
1909         chunked : xarray.Dataset
1910         """
1911         if chunks is None:
1912             warnings.warn(
1913                 "None value for 'chunks' is deprecated. "
1914                 "It will raise an error in the future. Use instead '{}'",
1915                 category=FutureWarning,
1916             )
1917             chunks = {}
1918 
1919         if isinstance(chunks, (Number, str)):
1920             chunks = dict.fromkeys(self.dims, chunks)
1921 
1922         bad_dims = chunks.keys() - self.dims.keys()
1923         if bad_dims:
1924             raise ValueError(
1925                 "some chunks keys are not dimensions on this " "object: %s" % bad_dims
1926             )
1927 
1928         variables = {
1929             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
1930             for k, v in self.variables.items()
1931         }
1932         return self._replace(variables)
1933 
1934     def _validate_indexers(
1935         self, indexers: Mapping[Hashable, Any], missing_dims: str = "raise"
1936     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1937         """Here we make sure
1938         + indexer has a valid keys
1939         + indexer is in a valid data type
1940         + string indexers are cast to the appropriate date type if the
1941           associated index is a DatetimeIndex or CFTimeIndex
1942         """
1943         from .dataarray import DataArray
1944 
1945         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1946 
1947         # all indexers should be int, slice, np.ndarrays, or Variable
1948         for k, v in indexers.items():
1949             if isinstance(v, (int, slice, Variable)):
1950                 yield k, v
1951             elif isinstance(v, DataArray):
1952                 yield k, v.variable
1953             elif isinstance(v, tuple):
1954                 yield k, as_variable(v)
1955             elif isinstance(v, Dataset):
1956                 raise TypeError("cannot use a Dataset as an indexer")
1957             elif isinstance(v, Sequence) and len(v) == 0:
1958                 yield k, np.empty((0,), dtype="int64")
1959             else:
1960                 v = np.asarray(v)
1961 
1962                 if v.dtype.kind in "US":
1963                     index = self.indexes[k]
1964                     if isinstance(index, pd.DatetimeIndex):
1965                         v = v.astype("datetime64[ns]")
1966                     elif isinstance(index, xr.CFTimeIndex):
1967                         v = _parse_array_of_cftime_strings(v, index.date_type)
1968 
1969                 if v.ndim > 1:
1970                     raise IndexError(
1971                         "Unlabeled multi-dimensional array cannot be "
1972                         "used for indexing: {}".format(k)
1973                     )
1974                 yield k, v
1975 
1976     def _validate_interp_indexers(
1977         self, indexers: Mapping[Hashable, Any]
1978     ) -> Iterator[Tuple[Hashable, Variable]]:
1979         """Variant of _validate_indexers to be used for interpolation"""
1980         for k, v in self._validate_indexers(indexers):
1981             if isinstance(v, Variable):
1982                 if v.ndim == 1:
1983                     yield k, v.to_index_variable()
1984                 else:
1985                     yield k, v
1986             elif isinstance(v, int):
1987                 yield k, Variable((), v)
1988             elif isinstance(v, np.ndarray):
1989                 if v.ndim == 0:
1990                     yield k, Variable((), v)
1991                 elif v.ndim == 1:
1992                     yield k, IndexVariable((k,), v)
1993                 else:
1994                     raise AssertionError()  # Already tested by _validate_indexers
1995             else:
1996                 raise TypeError(type(v))
1997 
1998     def _get_indexers_coords_and_indexes(self, indexers):
1999         """Extract coordinates and indexes from indexers.
2000 
2001         Only coordinate with a name different from any of self.variables will
2002         be attached.
2003         """
2004         from .dataarray import DataArray
2005 
2006         coords_list = []
2007         for k, v in indexers.items():
2008             if isinstance(v, DataArray):
2009                 if v.dtype.kind == "b":
2010                     if v.ndim != 1:  # we only support 1-d boolean array
2011                         raise ValueError(
2012                             "{:d}d-boolean array is used for indexing along "
2013                             "dimension {!r}, but only 1d boolean arrays are "
2014                             "supported.".format(v.ndim, k)
2015                         )
2016                     # Make sure in case of boolean DataArray, its
2017                     # coordinate also should be indexed.
2018                     v_coords = v[v.values.nonzero()[0]].coords
2019                 else:
2020                     v_coords = v.coords
2021                 coords_list.append(v_coords)
2022 
2023         # we don't need to call align() explicitly or check indexes for
2024         # alignment, because merge_variables already checks for exact alignment
2025         # between dimension coordinates
2026         coords, indexes = merge_coordinates_without_align(coords_list)
2027         assert_coordinate_consistent(self, coords)
2028 
2029         # silently drop the conflicted variables.
2030         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2031         attached_indexes = {
2032             k: v for k, v in indexes.items() if k not in self._variables
2033         }
2034         return attached_coords, attached_indexes
2035 
2036     def isel(
2037         self,
2038         indexers: Mapping[Hashable, Any] = None,
2039         drop: bool = False,
2040         missing_dims: str = "raise",
2041         **indexers_kwargs: Any,
2042     ) -> "Dataset":
2043         """Returns a new dataset with each array indexed along the specified
2044         dimension(s).
2045 
2046         This method selects values from each array using its `__getitem__`
2047         method, except this method does not require knowing the order of
2048         each array's dimensions.
2049 
2050         Parameters
2051         ----------
2052         indexers : dict, optional
2053             A dict with keys matching dimensions and values given
2054             by integers, slice objects or arrays.
2055             indexer can be a integer, slice, array-like or DataArray.
2056             If DataArrays are passed as indexers, xarray-style indexing will be
2057             carried out. See :ref:`indexing` for the details.
2058             One of indexers or indexers_kwargs must be provided.
2059         drop : bool, optional
2060             If ``drop=True``, drop coordinates variables indexed by integers
2061             instead of making them scalar.
2062         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2063             What to do if dimensions that should be selected from are not present in the
2064             Dataset:
2065             - "raise": raise an exception
2066             - "warning": raise a warning, and ignore the missing dimensions
2067             - "ignore": ignore the missing dimensions
2068         **indexers_kwargs : {dim: indexer, ...}, optional
2069             The keyword arguments form of ``indexers``.
2070             One of indexers or indexers_kwargs must be provided.
2071 
2072         Returns
2073         -------
2074         obj : Dataset
2075             A new Dataset with the same contents as this dataset, except each
2076             array and dimension is indexed by the appropriate indexers.
2077             If indexer DataArrays have coordinates that do not conflict with
2078             this object, then these coordinates will be attached.
2079             In general, each array's data will be a view of the array's data
2080             in this dataset, unless vectorized indexing was triggered by using
2081             an array indexer, in which case the data will be a copy.
2082 
2083         See Also
2084         --------
2085         Dataset.sel
2086         DataArray.isel
2087         """
2088         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2089         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2090             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2091 
2092         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2093         # lists, or zero or one-dimensional np.ndarray's
2094         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2095 
2096         variables = {}
2097         dims: Dict[Hashable, Tuple[int, ...]] = {}
2098         coord_names = self._coord_names.copy()
2099         indexes = self._indexes.copy() if self._indexes is not None else None
2100 
2101         for var_name, var_value in self._variables.items():
2102             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
2103             if var_indexers:
2104                 var_value = var_value.isel(var_indexers)
2105                 if drop and var_value.ndim == 0 and var_name in coord_names:
2106                     coord_names.remove(var_name)
2107                     if indexes:
2108                         indexes.pop(var_name, None)
2109                     continue
2110                 if indexes and var_name in indexes:
2111                     if var_value.ndim == 1:
2112                         indexes[var_name] = var_value.to_index()
2113                     else:
2114                         del indexes[var_name]
2115             variables[var_name] = var_value
2116             dims.update(zip(var_value.dims, var_value.shape))
2117 
2118         return self._construct_direct(
2119             variables=variables,
2120             coord_names=coord_names,
2121             dims=dims,
2122             attrs=self._attrs,
2123             indexes=indexes,
2124             encoding=self._encoding,
2125             file_obj=self._file_obj,
2126         )
2127 
2128     def _isel_fancy(
2129         self,
2130         indexers: Mapping[Hashable, Any],
2131         *,
2132         drop: bool,
2133         missing_dims: str = "raise",
2134     ) -> "Dataset":
2135         # Note: we need to preserve the original indexers variable in order to merge the
2136         # coords below
2137         indexers_list = list(self._validate_indexers(indexers, missing_dims))
2138 
2139         variables: Dict[Hashable, Variable] = {}
2140         indexes: Dict[Hashable, pd.Index] = {}
2141 
2142         for name, var in self.variables.items():
2143             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
2144             if drop and name in var_indexers:
2145                 continue  # drop this variable
2146 
2147             if name in self.indexes:
2148                 new_var, new_index = isel_variable_and_index(
2149                     name, var, self.indexes[name], var_indexers
2150                 )
2151                 if new_index is not None:
2152                     indexes[name] = new_index
2153             elif var_indexers:
2154                 new_var = var.isel(indexers=var_indexers)
2155             else:
2156                 new_var = var.copy(deep=False)
2157 
2158             variables[name] = new_var
2159 
2160         coord_names = self._coord_names & variables.keys()
2161         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2162 
2163         # Extract coordinates from indexers
2164         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2165         variables.update(coord_vars)
2166         indexes.update(new_indexes)
2167         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2168         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2169 
2170     def sel(
2171         self,
2172         indexers: Mapping[Hashable, Any] = None,
2173         method: str = None,
2174         tolerance: Number = None,
2175         drop: bool = False,
2176         **indexers_kwargs: Any,
2177     ) -> "Dataset":
2178         """Returns a new dataset with each array indexed by tick labels
2179         along the specified dimension(s).
2180 
2181         In contrast to `Dataset.isel`, indexers for this method should use
2182         labels instead of integers.
2183 
2184         Under the hood, this method is powered by using pandas's powerful Index
2185         objects. This makes label based indexing essentially just as fast as
2186         using integer indexing.
2187 
2188         It also means this method uses pandas's (well documented) logic for
2189         indexing. This means you can use string shortcuts for datetime indexes
2190         (e.g., '2000-01' to select all values in January 2000). It also means
2191         that slices are treated as inclusive of both the start and stop values,
2192         unlike normal Python indexing.
2193 
2194         Parameters
2195         ----------
2196         indexers : dict, optional
2197             A dict with keys matching dimensions and values given
2198             by scalars, slices or arrays of tick labels. For dimensions with
2199             multi-index, the indexer may also be a dict-like object with keys
2200             matching index level names.
2201             If DataArrays are passed as indexers, xarray-style indexing will be
2202             carried out. See :ref:`indexing` for the details.
2203             One of indexers or indexers_kwargs must be provided.
2204         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2205             Method to use for inexact matches:
2206 
2207             * None (default): only exact matches
2208             * pad / ffill: propagate last valid index value forward
2209             * backfill / bfill: propagate next valid index value backward
2210             * nearest: use nearest valid index value
2211         tolerance : optional
2212             Maximum distance between original and new labels for inexact
2213             matches. The values of the index at the matching locations must
2214             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2215         drop : bool, optional
2216             If ``drop=True``, drop coordinates variables in `indexers` instead
2217             of making them scalar.
2218         **indexers_kwargs : {dim: indexer, ...}, optional
2219             The keyword arguments form of ``indexers``.
2220             One of indexers or indexers_kwargs must be provided.
2221 
2222         Returns
2223         -------
2224         obj : Dataset
2225             A new Dataset with the same contents as this dataset, except each
2226             variable and dimension is indexed by the appropriate indexers.
2227             If indexer DataArrays have coordinates that do not conflict with
2228             this object, then these coordinates will be attached.
2229             In general, each array's data will be a view of the array's data
2230             in this dataset, unless vectorized indexing was triggered by using
2231             an array indexer, in which case the data will be a copy.
2232 
2233 
2234         See Also
2235         --------
2236         Dataset.isel
2237         DataArray.sel
2238         """
2239         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2240         pos_indexers, new_indexes = remap_label_indexers(
2241             self, indexers=indexers, method=method, tolerance=tolerance
2242         )
2243         result = self.isel(indexers=pos_indexers, drop=drop)
2244         return result._overwrite_indexes(new_indexes)
2245 
2246     def head(
2247         self,
2248         indexers: Union[Mapping[Hashable, int], int] = None,
2249         **indexers_kwargs: Any,
2250     ) -> "Dataset":
2251         """Returns a new dataset with the first `n` values of each array
2252         for the specified dimension(s).
2253 
2254         Parameters
2255         ----------
2256         indexers : dict or int, default: 5
2257             A dict with keys matching dimensions and integer values `n`
2258             or a single integer `n` applied over all dimensions.
2259             One of indexers or indexers_kwargs must be provided.
2260         **indexers_kwargs : {dim: n, ...}, optional
2261             The keyword arguments form of ``indexers``.
2262             One of indexers or indexers_kwargs must be provided.
2263 
2264 
2265         See Also
2266         --------
2267         Dataset.tail
2268         Dataset.thin
2269         DataArray.head
2270         """
2271         if not indexers_kwargs:
2272             if indexers is None:
2273                 indexers = 5
2274             if not isinstance(indexers, int) and not is_dict_like(indexers):
2275                 raise TypeError("indexers must be either dict-like or a single integer")
2276         if isinstance(indexers, int):
2277             indexers = {dim: indexers for dim in self.dims}
2278         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2279         for k, v in indexers.items():
2280             if not isinstance(v, int):
2281                 raise TypeError(
2282                     "expected integer type indexer for "
2283                     "dimension %r, found %r" % (k, type(v))
2284                 )
2285             elif v < 0:
2286                 raise ValueError(
2287                     "expected positive integer as indexer "
2288                     "for dimension %r, found %s" % (k, v)
2289                 )
2290         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2291         return self.isel(indexers_slices)
2292 
2293     def tail(
2294         self,
2295         indexers: Union[Mapping[Hashable, int], int] = None,
2296         **indexers_kwargs: Any,
2297     ) -> "Dataset":
2298         """Returns a new dataset with the last `n` values of each array
2299         for the specified dimension(s).
2300 
2301         Parameters
2302         ----------
2303         indexers : dict or int, default: 5
2304             A dict with keys matching dimensions and integer values `n`
2305             or a single integer `n` applied over all dimensions.
2306             One of indexers or indexers_kwargs must be provided.
2307         **indexers_kwargs : {dim: n, ...}, optional
2308             The keyword arguments form of ``indexers``.
2309             One of indexers or indexers_kwargs must be provided.
2310 
2311 
2312         See Also
2313         --------
2314         Dataset.head
2315         Dataset.thin
2316         DataArray.tail
2317         """
2318         if not indexers_kwargs:
2319             if indexers is None:
2320                 indexers = 5
2321             if not isinstance(indexers, int) and not is_dict_like(indexers):
2322                 raise TypeError("indexers must be either dict-like or a single integer")
2323         if isinstance(indexers, int):
2324             indexers = {dim: indexers for dim in self.dims}
2325         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2326         for k, v in indexers.items():
2327             if not isinstance(v, int):
2328                 raise TypeError(
2329                     "expected integer type indexer for "
2330                     "dimension %r, found %r" % (k, type(v))
2331                 )
2332             elif v < 0:
2333                 raise ValueError(
2334                     "expected positive integer as indexer "
2335                     "for dimension %r, found %s" % (k, v)
2336                 )
2337         indexers_slices = {
2338             k: slice(-val, None) if val != 0 else slice(val)
2339             for k, val in indexers.items()
2340         }
2341         return self.isel(indexers_slices)
2342 
2343     def thin(
2344         self,
2345         indexers: Union[Mapping[Hashable, int], int] = None,
2346         **indexers_kwargs: Any,
2347     ) -> "Dataset":
2348         """Returns a new dataset with each array indexed along every `n`-th
2349         value for the specified dimension(s)
2350 
2351         Parameters
2352         ----------
2353         indexers : dict or int
2354             A dict with keys matching dimensions and integer values `n`
2355             or a single integer `n` applied over all dimensions.
2356             One of indexers or indexers_kwargs must be provided.
2357         **indexers_kwargs : {dim: n, ...}, optional
2358             The keyword arguments form of ``indexers``.
2359             One of indexers or indexers_kwargs must be provided.
2360 
2361 
2362         See Also
2363         --------
2364         Dataset.head
2365         Dataset.tail
2366         DataArray.thin
2367         """
2368         if (
2369             not indexers_kwargs
2370             and not isinstance(indexers, int)
2371             and not is_dict_like(indexers)
2372         ):
2373             raise TypeError("indexers must be either dict-like or a single integer")
2374         if isinstance(indexers, int):
2375             indexers = {dim: indexers for dim in self.dims}
2376         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2377         for k, v in indexers.items():
2378             if not isinstance(v, int):
2379                 raise TypeError(
2380                     "expected integer type indexer for "
2381                     "dimension %r, found %r" % (k, type(v))
2382                 )
2383             elif v < 0:
2384                 raise ValueError(
2385                     "expected positive integer as indexer "
2386                     "for dimension %r, found %s" % (k, v)
2387                 )
2388             elif v == 0:
2389                 raise ValueError("step cannot be zero")
2390         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2391         return self.isel(indexers_slices)
2392 
2393     def broadcast_like(
2394         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2395     ) -> "Dataset":
2396         """Broadcast this DataArray against another Dataset or DataArray.
2397         This is equivalent to xr.broadcast(other, self)[1]
2398 
2399         Parameters
2400         ----------
2401         other : Dataset or DataArray
2402             Object against which to broadcast this array.
2403         exclude : iterable of hashable, optional
2404             Dimensions that must not be broadcasted
2405 
2406         """
2407         if exclude is None:
2408             exclude = set()
2409         else:
2410             exclude = set(exclude)
2411         args = align(other, self, join="outer", copy=False, exclude=exclude)
2412 
2413         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2414 
2415         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2416 
2417     def reindex_like(
2418         self,
2419         other: Union["Dataset", "DataArray"],
2420         method: str = None,
2421         tolerance: Number = None,
2422         copy: bool = True,
2423         fill_value: Any = dtypes.NA,
2424     ) -> "Dataset":
2425         """Conform this object onto the indexes of another object, filling in
2426         missing values with ``fill_value``. The default fill value is NaN.
2427 
2428         Parameters
2429         ----------
2430         other : Dataset or DataArray
2431             Object with an 'indexes' attribute giving a mapping from dimension
2432             names to pandas.Index objects, which provides coordinates upon
2433             which to index the variables in this dataset. The indexes on this
2434             other object need not be the same as the indexes on this
2435             dataset. Any mis-matched index values will be filled in with
2436             NaN, and any mis-matched dimension names will simply be ignored.
2437         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2438             Method to use for filling index values from other not found in this
2439             dataset:
2440 
2441             * None (default): don't fill gaps
2442             * pad / ffill: propagate last valid index value forward
2443             * backfill / bfill: propagate next valid index value backward
2444             * nearest: use nearest valid index value
2445         tolerance : optional
2446             Maximum distance between original and new labels for inexact
2447             matches. The values of the index at the matching locations must
2448             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2449         copy : bool, optional
2450             If ``copy=True``, data in the return value is always copied. If
2451             ``copy=False`` and reindexing is unnecessary, or can be performed
2452             with only slice operations, then the output may share memory with
2453             the input. In either case, a new xarray object is always returned.
2454         fill_value : scalar or dict-like, optional
2455             Value to use for newly missing values. If a dict-like maps
2456             variable names to fill values.
2457 
2458         Returns
2459         -------
2460         reindexed : Dataset
2461             Another dataset, with this dataset's data but coordinates from the
2462             other object.
2463 
2464         See Also
2465         --------
2466         Dataset.reindex
2467         align
2468         """
2469         indexers = alignment.reindex_like_indexers(self, other)
2470         return self.reindex(
2471             indexers=indexers,
2472             method=method,
2473             copy=copy,
2474             fill_value=fill_value,
2475             tolerance=tolerance,
2476         )
2477 
2478     def reindex(
2479         self,
2480         indexers: Mapping[Hashable, Any] = None,
2481         method: str = None,
2482         tolerance: Number = None,
2483         copy: bool = True,
2484         fill_value: Any = dtypes.NA,
2485         **indexers_kwargs: Any,
2486     ) -> "Dataset":
2487         """Conform this object onto a new set of indexes, filling in
2488         missing values with ``fill_value``. The default fill value is NaN.
2489 
2490         Parameters
2491         ----------
2492         indexers : dict, optional
2493             Dictionary with keys given by dimension names and values given by
2494             arrays of coordinates tick labels. Any mis-matched coordinate
2495             values will be filled in with NaN, and any mis-matched dimension
2496             names will simply be ignored.
2497             One of indexers or indexers_kwargs must be provided.
2498         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2499             Method to use for filling index values in ``indexers`` not found in
2500             this dataset:
2501 
2502             * None (default): don't fill gaps
2503             * pad / ffill: propagate last valid index value forward
2504             * backfill / bfill: propagate next valid index value backward
2505             * nearest: use nearest valid index value
2506         tolerance : optional
2507             Maximum distance between original and new labels for inexact
2508             matches. The values of the index at the matching locations must
2509             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2510         copy : bool, optional
2511             If ``copy=True``, data in the return value is always copied. If
2512             ``copy=False`` and reindexing is unnecessary, or can be performed
2513             with only slice operations, then the output may share memory with
2514             the input. In either case, a new xarray object is always returned.
2515         fill_value : scalar or dict-like, optional
2516             Value to use for newly missing values. If a dict-like,
2517             maps variable names (including coordinates) to fill values.
2518         sparse : bool, default: False
2519             use sparse-array.
2520         **indexers_kwargs : {dim: indexer, ...}, optional
2521             Keyword arguments in the same form as ``indexers``.
2522             One of indexers or indexers_kwargs must be provided.
2523 
2524         Returns
2525         -------
2526         reindexed : Dataset
2527             Another dataset, with this dataset's data but replaced coordinates.
2528 
2529         See Also
2530         --------
2531         Dataset.reindex_like
2532         align
2533         pandas.Index.get_indexer
2534 
2535         Examples
2536         --------
2537 
2538         Create a dataset with some fictional data.
2539 
2540         >>> import xarray as xr
2541         >>> import pandas as pd
2542         >>> x = xr.Dataset(
2543         ...     {
2544         ...         "temperature": ("station", 20 * np.random.rand(4)),
2545         ...         "pressure": ("station", 500 * np.random.rand(4)),
2546         ...     },
2547         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2548         ... )
2549         >>> x
2550         <xarray.Dataset>
2551         Dimensions:      (station: 4)
2552         Coordinates:
2553           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2554         Data variables:
2555             temperature  (station) float64 10.98 14.3 12.06 10.9
2556             pressure     (station) float64 211.8 322.9 218.8 445.9
2557         >>> x.indexes
2558         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2559 
2560         Create a new index and reindex the dataset. By default values in the new index that
2561         do not have corresponding records in the dataset are assigned `NaN`.
2562 
2563         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2564         >>> x.reindex({"station": new_index})
2565         <xarray.Dataset>
2566         Dimensions:      (station: 4)
2567         Coordinates:
2568           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2569         Data variables:
2570             temperature  (station) float64 10.98 nan 12.06 nan
2571             pressure     (station) float64 211.8 nan 218.8 nan
2572 
2573         We can fill in the missing values by passing a value to the keyword `fill_value`.
2574 
2575         >>> x.reindex({"station": new_index}, fill_value=0)
2576         <xarray.Dataset>
2577         Dimensions:      (station: 4)
2578         Coordinates:
2579           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2580         Data variables:
2581             temperature  (station) float64 10.98 0.0 12.06 0.0
2582             pressure     (station) float64 211.8 0.0 218.8 0.0
2583 
2584         We can also use different fill values for each variable.
2585 
2586         >>> x.reindex(
2587         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
2588         ... )
2589         <xarray.Dataset>
2590         Dimensions:      (station: 4)
2591         Coordinates:
2592           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2593         Data variables:
2594             temperature  (station) float64 10.98 0.0 12.06 0.0
2595             pressure     (station) float64 211.8 100.0 218.8 100.0
2596 
2597         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2598         to the keyword method to fill the `NaN` values.
2599 
2600         >>> x.reindex({"station": new_index}, method="nearest")
2601         Traceback (most recent call last):
2602         ...
2603             raise ValueError('index must be monotonic increasing or decreasing')
2604         ValueError: index must be monotonic increasing or decreasing
2605 
2606         To further illustrate the filling functionality in reindex, we will create a
2607         dataset with a monotonically increasing index (for example, a sequence of dates).
2608 
2609         >>> x2 = xr.Dataset(
2610         ...     {
2611         ...         "temperature": (
2612         ...             "time",
2613         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2614         ...         ),
2615         ...         "pressure": ("time", 500 * np.random.rand(6)),
2616         ...     },
2617         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2618         ... )
2619         >>> x2
2620         <xarray.Dataset>
2621         Dimensions:      (time: 6)
2622         Coordinates:
2623           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2624         Data variables:
2625             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2626             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
2627 
2628         Suppose we decide to expand the dataset to cover a wider date range.
2629 
2630         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2631         >>> x2.reindex({"time": time_index2})
2632         <xarray.Dataset>
2633         Dimensions:      (time: 10)
2634         Coordinates:
2635           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2636         Data variables:
2637             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2638             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
2639 
2640         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2641         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2642 
2643         For example, to back-propagate the last valid value to fill the `NaN` values,
2644         pass `bfill` as an argument to the `method` keyword.
2645 
2646         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2647         >>> x3
2648         <xarray.Dataset>
2649         Dimensions:      (time: 10)
2650         Coordinates:
2651           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2652         Data variables:
2653             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2654             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
2655 
2656         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2657         will not be filled by any of the value propagation schemes.
2658 
2659         >>> x2.where(x2.temperature.isnull(), drop=True)
2660         <xarray.Dataset>
2661         Dimensions:      (time: 1)
2662         Coordinates:
2663           * time         (time) datetime64[ns] 2019-01-03
2664         Data variables:
2665             temperature  (time) float64 nan
2666             pressure     (time) float64 395.9
2667         >>> x3.where(x3.temperature.isnull(), drop=True)
2668         <xarray.Dataset>
2669         Dimensions:      (time: 2)
2670         Coordinates:
2671           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2672         Data variables:
2673             temperature  (time) float64 nan nan
2674             pressure     (time) float64 395.9 nan
2675 
2676         This is because filling while reindexing does not look at dataset values, but only compares
2677         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2678         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2679 
2680         """
2681         return self._reindex(
2682             indexers,
2683             method,
2684             tolerance,
2685             copy,
2686             fill_value,
2687             sparse=False,
2688             **indexers_kwargs,
2689         )
2690 
2691     def _reindex(
2692         self,
2693         indexers: Mapping[Hashable, Any] = None,
2694         method: str = None,
2695         tolerance: Number = None,
2696         copy: bool = True,
2697         fill_value: Any = dtypes.NA,
2698         sparse: bool = False,
2699         **indexers_kwargs: Any,
2700     ) -> "Dataset":
2701         """
2702         same to _reindex but support sparse option
2703         """
2704         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2705 
2706         bad_dims = [d for d in indexers if d not in self.dims]
2707         if bad_dims:
2708             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2709 
2710         variables, indexes = alignment.reindex_variables(
2711             self.variables,
2712             self.sizes,
2713             self.indexes,
2714             indexers,
2715             method,
2716             tolerance,
2717             copy=copy,
2718             fill_value=fill_value,
2719             sparse=sparse,
2720         )
2721         coord_names = set(self._coord_names)
2722         coord_names.update(indexers)
2723         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2724 
2725     def interp(
2726         self,
2727         coords: Mapping[Hashable, Any] = None,
2728         method: str = "linear",
2729         assume_sorted: bool = False,
2730         kwargs: Mapping[str, Any] = None,
2731         **coords_kwargs: Any,
2732     ) -> "Dataset":
2733         """Multidimensional interpolation of Dataset.
2734 
2735         Parameters
2736         ----------
2737         coords : dict, optional
2738             Mapping from dimension names to the new coordinates.
2739             New coordinate can be a scalar, array-like or DataArray.
2740             If DataArrays are passed as new coordinates, their dimensions are
2741             used for the broadcasting. Missing values are skipped.
2742         method : str, optional
2743             {"linear", "nearest"} for multidimensional array,
2744             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2745             for 1-dimensional array. "linear" is used by default.
2746         assume_sorted : bool, optional
2747             If False, values of coordinates that are interpolated over can be
2748             in any order and they are sorted first. If True, interpolated
2749             coordinates are assumed to be an array of monotonically increasing
2750             values.
2751         kwargs: dict, optional
2752             Additional keyword arguments passed to scipy's interpolator. Valid
2753             options and their behavior depend on if 1-dimensional or
2754             multi-dimensional interpolation is used.
2755         **coords_kwargs : {dim: coordinate, ...}, optional
2756             The keyword arguments form of ``coords``.
2757             One of coords or coords_kwargs must be provided.
2758 
2759         Returns
2760         -------
2761         interpolated : Dataset
2762             New dataset on the new coordinates.
2763 
2764         Notes
2765         -----
2766         scipy is required.
2767 
2768         See Also
2769         --------
2770         scipy.interpolate.interp1d
2771         scipy.interpolate.interpn
2772 
2773         Examples
2774         --------
2775         >>> ds = xr.Dataset(
2776         ...     data_vars={
2777         ...         "a": ("x", [5, 7, 4]),
2778         ...         "b": (
2779         ...             ("x", "y"),
2780         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2781         ...         ),
2782         ...     },
2783         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2784         ... )
2785         >>> ds
2786         <xarray.Dataset>
2787         Dimensions:  (x: 3, y: 4)
2788         Coordinates:
2789           * x        (x) int64 0 1 2
2790           * y        (y) int64 10 12 14 16
2791         Data variables:
2792             a        (x) int64 5 7 4
2793             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
2794 
2795         1D interpolation with the default method (linear):
2796 
2797         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
2798         <xarray.Dataset>
2799         Dimensions:  (x: 4, y: 4)
2800         Coordinates:
2801           * y        (y) int64 10 12 14 16
2802           * x        (x) float64 0.0 0.75 1.25 1.75
2803         Data variables:
2804             a        (x) float64 5.0 6.5 6.25 4.75
2805             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
2806 
2807         1D interpolation with a different method:
2808 
2809         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2810         <xarray.Dataset>
2811         Dimensions:  (x: 4, y: 4)
2812         Coordinates:
2813           * y        (y) int64 10 12 14 16
2814           * x        (x) float64 0.0 0.75 1.25 1.75
2815         Data variables:
2816             a        (x) float64 5.0 7.0 7.0 4.0
2817             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
2818 
2819         1D extrapolation:
2820 
2821         >>> ds.interp(
2822         ...     x=[1, 1.5, 2.5, 3.5],
2823         ...     method="linear",
2824         ...     kwargs={"fill_value": "extrapolate"},
2825         ... )
2826         <xarray.Dataset>
2827         Dimensions:  (x: 4, y: 4)
2828         Coordinates:
2829           * y        (y) int64 10 12 14 16
2830           * x        (x) float64 1.0 1.5 2.5 3.5
2831         Data variables:
2832             a        (x) float64 7.0 5.5 2.5 -0.5
2833             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
2834 
2835         2D interpolation:
2836 
2837         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2838         <xarray.Dataset>
2839         Dimensions:  (x: 4, y: 3)
2840         Coordinates:
2841           * x        (x) float64 0.0 0.75 1.25 1.75
2842           * y        (y) int64 11 13 15
2843         Data variables:
2844             a        (x) float64 5.0 6.5 6.25 4.75
2845             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
2846         """
2847         from . import missing
2848 
2849         if kwargs is None:
2850             kwargs = {}
2851 
2852         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2853         indexers = dict(self._validate_interp_indexers(coords))
2854 
2855         if coords:
2856             # This avoids broadcasting over coordinates that are both in
2857             # the original array AND in the indexing array. It essentially
2858             # forces interpolation along the shared coordinates.
2859             sdims = (
2860                 set(self.dims)
2861                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2862                 .difference(coords.keys())
2863             )
2864             indexers.update({d: self.variables[d] for d in sdims})
2865 
2866         obj = self if assume_sorted else self.sortby([k for k in coords])
2867 
2868         def maybe_variable(obj, k):
2869             # workaround to get variable for dimension without coordinate.
2870             try:
2871                 return obj._variables[k]
2872             except KeyError:
2873                 return as_variable((k, range(obj.dims[k])))
2874 
2875         def _validate_interp_indexer(x, new_x):
2876             # In the case of datetimes, the restrictions placed on indexers
2877             # used with interp are stronger than those which are placed on
2878             # isel, so we need an additional check after _validate_indexers.
2879             if _contains_datetime_like_objects(
2880                 x
2881             ) and not _contains_datetime_like_objects(new_x):
2882                 raise TypeError(
2883                     "When interpolating over a datetime-like "
2884                     "coordinate, the coordinates to "
2885                     "interpolate to must be either datetime "
2886                     "strings or datetimes. "
2887                     "Instead got\n{}".format(new_x)
2888                 )
2889             return x, new_x
2890 
2891         variables: Dict[Hashable, Variable] = {}
2892         for name, var in obj._variables.items():
2893             if name in indexers:
2894                 continue
2895 
2896             if var.dtype.kind in "uifc":
2897                 var_indexers = {
2898                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2899                     for k, v in indexers.items()
2900                     if k in var.dims
2901                 }
2902                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2903             elif all(d not in indexers for d in var.dims):
2904                 # keep unrelated object array
2905                 variables[name] = var
2906 
2907         coord_names = obj._coord_names & variables.keys()
2908         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2909         selected = self._replace_with_new_dims(
2910             variables.copy(), coord_names, indexes=indexes
2911         )
2912 
2913         # attach indexer as coordinate
2914         variables.update(indexers)
2915         for k, v in indexers.items():
2916             assert isinstance(v, Variable)
2917             if v.dims == (k,):
2918                 indexes[k] = v.to_index()
2919 
2920         # Extract coordinates from indexers
2921         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2922         variables.update(coord_vars)
2923         indexes.update(new_indexes)
2924 
2925         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2926         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2927 
2928     def interp_like(
2929         self,
2930         other: Union["Dataset", "DataArray"],
2931         method: str = "linear",
2932         assume_sorted: bool = False,
2933         kwargs: Mapping[str, Any] = None,
2934     ) -> "Dataset":
2935         """Interpolate this object onto the coordinates of another object,
2936         filling the out of range values with NaN.
2937 
2938         Parameters
2939         ----------
2940         other : Dataset or DataArray
2941             Object with an 'indexes' attribute giving a mapping from dimension
2942             names to an 1d array-like, which provides coordinates upon
2943             which to index the variables in this dataset. Missing values are skipped.
2944         method : str, optional
2945             {"linear", "nearest"} for multidimensional array,
2946             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2947             for 1-dimensional array. 'linear' is used by default.
2948         assume_sorted : bool, optional
2949             If False, values of coordinates that are interpolated over can be
2950             in any order and they are sorted first. If True, interpolated
2951             coordinates are assumed to be an array of monotonically increasing
2952             values.
2953         kwargs: dict, optional
2954             Additional keyword passed to scipy's interpolator.
2955 
2956         Returns
2957         -------
2958         interpolated : Dataset
2959             Another dataset by interpolating this dataset's data along the
2960             coordinates of the other object.
2961 
2962         Notes
2963         -----
2964         scipy is required.
2965         If the dataset has object-type coordinates, reindex is used for these
2966         coordinates instead of the interpolation.
2967 
2968         See Also
2969         --------
2970         Dataset.interp
2971         Dataset.reindex_like
2972         """
2973         if kwargs is None:
2974             kwargs = {}
2975         coords = alignment.reindex_like_indexers(self, other)
2976 
2977         numeric_coords: Dict[Hashable, pd.Index] = {}
2978         object_coords: Dict[Hashable, pd.Index] = {}
2979         for k, v in coords.items():
2980             if v.dtype.kind in "uifcMm":
2981                 numeric_coords[k] = v
2982             else:
2983                 object_coords[k] = v
2984 
2985         ds = self
2986         if object_coords:
2987             # We do not support interpolation along object coordinate.
2988             # reindex instead.
2989             ds = self.reindex(object_coords)
2990         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2991 
2992     # Helper methods for rename()
2993     def _rename_vars(self, name_dict, dims_dict):
2994         variables = {}
2995         coord_names = set()
2996         for k, v in self.variables.items():
2997             var = v.copy(deep=False)
2998             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2999             name = name_dict.get(k, k)
3000             if name in variables:
3001                 raise ValueError(f"the new name {name!r} conflicts")
3002             variables[name] = var
3003             if k in self._coord_names:
3004                 coord_names.add(name)
3005         return variables, coord_names
3006 
3007     def _rename_dims(self, name_dict):
3008         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3009 
3010     def _rename_indexes(self, name_dict, dims_set):
3011         if self._indexes is None:
3012             return None
3013         indexes = {}
3014         for k, v in self.indexes.items():
3015             new_name = name_dict.get(k, k)
3016             if new_name not in dims_set:
3017                 continue
3018             if isinstance(v, pd.MultiIndex):
3019                 new_names = [name_dict.get(k, k) for k in v.names]
3020                 index = v.rename(names=new_names)
3021             else:
3022                 index = v.rename(new_name)
3023             indexes[new_name] = index
3024         return indexes
3025 
3026     def _rename_all(self, name_dict, dims_dict):
3027         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3028         dims = self._rename_dims(dims_dict)
3029         indexes = self._rename_indexes(name_dict, dims.keys())
3030         return variables, coord_names, dims, indexes
3031 
3032     def rename(
3033         self,
3034         name_dict: Mapping[Hashable, Hashable] = None,
3035         **names: Hashable,
3036     ) -> "Dataset":
3037         """Returns a new object with renamed variables and dimensions.
3038 
3039         Parameters
3040         ----------
3041         name_dict : dict-like, optional
3042             Dictionary whose keys are current variable or dimension names and
3043             whose values are the desired names.
3044         **names : optional
3045             Keyword form of ``name_dict``.
3046             One of name_dict or names must be provided.
3047 
3048         Returns
3049         -------
3050         renamed : Dataset
3051             Dataset with renamed variables and dimensions.
3052 
3053         See Also
3054         --------
3055         Dataset.swap_dims
3056         Dataset.rename_vars
3057         Dataset.rename_dims
3058         DataArray.rename
3059         """
3060         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3061         for k in name_dict.keys():
3062             if k not in self and k not in self.dims:
3063                 raise ValueError(
3064                     "cannot rename %r because it is not a "
3065                     "variable or dimension in this dataset" % k
3066                 )
3067 
3068         variables, coord_names, dims, indexes = self._rename_all(
3069             name_dict=name_dict, dims_dict=name_dict
3070         )
3071         assert_unique_multiindex_level_names(variables)
3072         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3073 
3074     def rename_dims(
3075         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
3076     ) -> "Dataset":
3077         """Returns a new object with renamed dimensions only.
3078 
3079         Parameters
3080         ----------
3081         dims_dict : dict-like, optional
3082             Dictionary whose keys are current dimension names and
3083             whose values are the desired names. The desired names must
3084             not be the name of an existing dimension or Variable in the Dataset.
3085         **dims : optional
3086             Keyword form of ``dims_dict``.
3087             One of dims_dict or dims must be provided.
3088 
3089         Returns
3090         -------
3091         renamed : Dataset
3092             Dataset with renamed dimensions.
3093 
3094         See Also
3095         --------
3096         Dataset.swap_dims
3097         Dataset.rename
3098         Dataset.rename_vars
3099         DataArray.rename
3100         """
3101         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3102         for k, v in dims_dict.items():
3103             if k not in self.dims:
3104                 raise ValueError(
3105                     "cannot rename %r because it is not a "
3106                     "dimension in this dataset" % k
3107                 )
3108             if v in self.dims or v in self:
3109                 raise ValueError(
3110                     f"Cannot rename {k} to {v} because {v} already exists. "
3111                     "Try using swap_dims instead."
3112                 )
3113 
3114         variables, coord_names, sizes, indexes = self._rename_all(
3115             name_dict={}, dims_dict=dims_dict
3116         )
3117         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3118 
3119     def rename_vars(
3120         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
3121     ) -> "Dataset":
3122         """Returns a new object with renamed variables including coordinates
3123 
3124         Parameters
3125         ----------
3126         name_dict : dict-like, optional
3127             Dictionary whose keys are current variable or coordinate names and
3128             whose values are the desired names.
3129         **names : optional
3130             Keyword form of ``name_dict``.
3131             One of name_dict or names must be provided.
3132 
3133         Returns
3134         -------
3135         renamed : Dataset
3136             Dataset with renamed variables including coordinates
3137 
3138         See Also
3139         --------
3140         Dataset.swap_dims
3141         Dataset.rename
3142         Dataset.rename_dims
3143         DataArray.rename
3144         """
3145         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3146         for k in name_dict:
3147             if k not in self:
3148                 raise ValueError(
3149                     "cannot rename %r because it is not a "
3150                     "variable or coordinate in this dataset" % k
3151                 )
3152         variables, coord_names, dims, indexes = self._rename_all(
3153             name_dict=name_dict, dims_dict={}
3154         )
3155         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3156 
3157     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "Dataset":
3158         """Returns a new object with swapped dimensions.
3159 
3160         Parameters
3161         ----------
3162         dims_dict : dict-like
3163             Dictionary whose keys are current dimension names and whose values
3164             are new names.
3165 
3166         Returns
3167         -------
3168         swapped : Dataset
3169             Dataset with swapped dimensions.
3170 
3171         Examples
3172         --------
3173         >>> ds = xr.Dataset(
3174         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3175         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3176         ... )
3177         >>> ds
3178         <xarray.Dataset>
3179         Dimensions:  (x: 2)
3180         Coordinates:
3181           * x        (x) <U1 'a' 'b'
3182             y        (x) int64 0 1
3183         Data variables:
3184             a        (x) int64 5 7
3185             b        (x) float64 0.1 2.4
3186 
3187         >>> ds.swap_dims({"x": "y"})
3188         <xarray.Dataset>
3189         Dimensions:  (y: 2)
3190         Coordinates:
3191             x        (y) <U1 'a' 'b'
3192           * y        (y) int64 0 1
3193         Data variables:
3194             a        (y) int64 5 7
3195             b        (y) float64 0.1 2.4
3196 
3197         >>> ds.swap_dims({"x": "z"})
3198         <xarray.Dataset>
3199         Dimensions:  (z: 2)
3200         Coordinates:
3201             x        (z) <U1 'a' 'b'
3202             y        (z) int64 0 1
3203         Dimensions without coordinates: z
3204         Data variables:
3205             a        (z) int64 5 7
3206             b        (z) float64 0.1 2.4
3207 
3208         See Also
3209         --------
3210 
3211         Dataset.rename
3212         DataArray.swap_dims
3213         """
3214         # TODO: deprecate this method in favor of a (less confusing)
3215         # rename_dims() method that only renames dimensions.
3216         for k, v in dims_dict.items():
3217             if k not in self.dims:
3218                 raise ValueError(
3219                     "cannot swap from dimension %r because it is "
3220                     "not an existing dimension" % k
3221                 )
3222             if v in self.variables and self.variables[v].dims != (k,):
3223                 raise ValueError(
3224                     "replacement dimension %r is not a 1D "
3225                     "variable along the old dimension %r" % (v, k)
3226                 )
3227 
3228         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3229 
3230         coord_names = self._coord_names.copy()
3231         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3232 
3233         variables: Dict[Hashable, Variable] = {}
3234         indexes: Dict[Hashable, pd.Index] = {}
3235         for k, v in self.variables.items():
3236             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3237             if k in result_dims:
3238                 var = v.to_index_variable()
3239                 if k in self.indexes:
3240                     indexes[k] = self.indexes[k]
3241                 else:
3242                     new_index = var.to_index()
3243                     if new_index.nlevels == 1:
3244                         # make sure index name matches dimension name
3245                         new_index = new_index.rename(k)
3246                     indexes[k] = new_index
3247             else:
3248                 var = v.to_base_variable()
3249             var.dims = dims
3250             variables[k] = var
3251 
3252         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3253 
3254     def expand_dims(
3255         self,
3256         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
3257         axis: Union[None, int, Sequence[int]] = None,
3258         **dim_kwargs: Any,
3259     ) -> "Dataset":
3260         """Return a new object with an additional axis (or axes) inserted at
3261         the corresponding position in the array shape.  The new object is a
3262         view into the underlying array, not a copy.
3263 
3264         If dim is already a scalar coordinate, it will be promoted to a 1D
3265         coordinate consisting of a single value.
3266 
3267         Parameters
3268         ----------
3269         dim : hashable, sequence of hashable, mapping, or None
3270             Dimensions to include on the new variable. If provided as hashable
3271             or sequence of hashable, then dimensions are inserted with length
3272             1. If provided as a mapping, then the keys are the new dimensions
3273             and the values are either integers (giving the length of the new
3274             dimensions) or array-like (giving the coordinates of the new
3275             dimensions).
3276         axis : int, sequence of int, or None
3277             Axis position(s) where new axis is to be inserted (position(s) on
3278             the result array). If a list (or tuple) of integers is passed,
3279             multiple axes are inserted. In this case, dim arguments should be
3280             same length list. If axis=None is passed, all the axes will be
3281             inserted to the start of the result array.
3282         **dim_kwargs : int or sequence or ndarray
3283             The keywords are arbitrary dimensions being inserted and the values
3284             are either the lengths of the new dims (if int is given), or their
3285             coordinates. Note, this is an alternative to passing a dict to the
3286             dim kwarg and will only be used if dim is None.
3287 
3288         Returns
3289         -------
3290         expanded : same type as caller
3291             This object, but with an additional dimension(s).
3292         """
3293         if dim is None:
3294             pass
3295         elif isinstance(dim, Mapping):
3296             # We're later going to modify dim in place; don't tamper with
3297             # the input
3298             dim = dict(dim)
3299         elif isinstance(dim, int):
3300             raise TypeError(
3301                 "dim should be hashable or sequence of hashables or mapping"
3302             )
3303         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3304             dim = {dim: 1}
3305         elif isinstance(dim, Sequence):
3306             if len(dim) != len(set(dim)):
3307                 raise ValueError("dims should not contain duplicate values.")
3308             dim = {d: 1 for d in dim}
3309 
3310         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3311         assert isinstance(dim, MutableMapping)
3312 
3313         if axis is None:
3314             axis = list(range(len(dim)))
3315         elif not isinstance(axis, Sequence):
3316             axis = [axis]
3317 
3318         if len(dim) != len(axis):
3319             raise ValueError("lengths of dim and axis should be identical.")
3320         for d in dim:
3321             if d in self.dims:
3322                 raise ValueError(f"Dimension {d} already exists.")
3323             if d in self._variables and not utils.is_scalar(self._variables[d]):
3324                 raise ValueError(
3325                     "{dim} already exists as coordinate or"
3326                     " variable name.".format(dim=d)
3327                 )
3328 
3329         variables: Dict[Hashable, Variable] = {}
3330         coord_names = self._coord_names.copy()
3331         # If dim is a dict, then ensure that the values are either integers
3332         # or iterables.
3333         for k, v in dim.items():
3334             if hasattr(v, "__iter__"):
3335                 # If the value for the new dimension is an iterable, then
3336                 # save the coordinates to the variables dict, and set the
3337                 # value within the dim dict to the length of the iterable
3338                 # for later use.
3339                 variables[k] = xr.IndexVariable((k,), v)
3340                 coord_names.add(k)
3341                 dim[k] = variables[k].size
3342             elif isinstance(v, int):
3343                 pass  # Do nothing if the dimensions value is just an int
3344             else:
3345                 raise TypeError(
3346                     "The value of new dimension {k} must be "
3347                     "an iterable or an int".format(k=k)
3348                 )
3349 
3350         for k, v in self._variables.items():
3351             if k not in dim:
3352                 if k in coord_names:  # Do not change coordinates
3353                     variables[k] = v
3354                 else:
3355                     result_ndim = len(v.dims) + len(axis)
3356                     for a in axis:
3357                         if a < -result_ndim or result_ndim - 1 < a:
3358                             raise IndexError(
3359                                 f"Axis {a} of variable {k} is out of bounds of the "
3360                                 f"expanded dimension size {result_ndim}"
3361                             )
3362 
3363                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3364                     if len(axis_pos) != len(set(axis_pos)):
3365                         raise ValueError("axis should not contain duplicate values")
3366                     # We need to sort them to make sure `axis` equals to the
3367                     # axis positions of the result array.
3368                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3369 
3370                     all_dims = list(zip(v.dims, v.shape))
3371                     for d, c in zip_axis_dim:
3372                         all_dims.insert(d, c)
3373                     variables[k] = v.set_dims(dict(all_dims))
3374             else:
3375                 # If dims includes a label of a non-dimension coordinate,
3376                 # it will be promoted to a 1D coordinate with a single value.
3377                 variables[k] = v.set_dims(k).to_index_variable()
3378 
3379         new_dims = self._dims.copy()
3380         new_dims.update(dim)
3381 
3382         return self._replace_vars_and_dims(
3383             variables, dims=new_dims, coord_names=coord_names
3384         )
3385 
3386     def set_index(
3387         self,
3388         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3389         append: bool = False,
3390         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3391     ) -> "Dataset":
3392         """Set Dataset (multi-)indexes using one or more existing coordinates
3393         or variables.
3394 
3395         Parameters
3396         ----------
3397         indexes : {dim: index, ...}
3398             Mapping from names matching dimensions and values given
3399             by (lists of) the names of existing coordinates or variables to set
3400             as new (multi-)index.
3401         append : bool, optional
3402             If True, append the supplied index(es) to the existing index(es).
3403             Otherwise replace the existing index(es) (default).
3404         **indexes_kwargs : optional
3405             The keyword arguments form of ``indexes``.
3406             One of indexes or indexes_kwargs must be provided.
3407 
3408         Returns
3409         -------
3410         obj : Dataset
3411             Another dataset, with this dataset's data but replaced coordinates.
3412 
3413         Examples
3414         --------
3415         >>> arr = xr.DataArray(
3416         ...     data=np.ones((2, 3)),
3417         ...     dims=["x", "y"],
3418         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3419         ... )
3420         >>> ds = xr.Dataset({"v": arr})
3421         >>> ds
3422         <xarray.Dataset>
3423         Dimensions:  (x: 2, y: 3)
3424         Coordinates:
3425           * x        (x) int64 0 1
3426           * y        (y) int64 0 1 2
3427             a        (x) int64 3 4
3428         Data variables:
3429             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3430         >>> ds.set_index(x="a")
3431         <xarray.Dataset>
3432         Dimensions:  (x: 2, y: 3)
3433         Coordinates:
3434           * x        (x) int64 3 4
3435           * y        (y) int64 0 1 2
3436         Data variables:
3437             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3438 
3439         See Also
3440         --------
3441         Dataset.reset_index
3442         Dataset.swap_dims
3443         """
3444         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3445         variables, coord_names = merge_indexes(
3446             indexes, self._variables, self._coord_names, append=append
3447         )
3448         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3449 
3450     def reset_index(
3451         self,
3452         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3453         drop: bool = False,
3454     ) -> "Dataset":
3455         """Reset the specified index(es) or multi-index level(s).
3456 
3457         Parameters
3458         ----------
3459         dims_or_levels : str or list
3460             Name(s) of the dimension(s) and/or multi-index level(s) that will
3461             be reset.
3462         drop : bool, optional
3463             If True, remove the specified indexes and/or multi-index levels
3464             instead of extracting them as new coordinates (default: False).
3465 
3466         Returns
3467         -------
3468         obj : Dataset
3469             Another dataset, with this dataset's data but replaced coordinates.
3470 
3471         See Also
3472         --------
3473         Dataset.set_index
3474         """
3475         variables, coord_names = split_indexes(
3476             dims_or_levels,
3477             self._variables,
3478             self._coord_names,
3479             cast(Mapping[Hashable, Hashable], self._level_coords),
3480             drop=drop,
3481         )
3482         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3483 
3484     def reorder_levels(
3485         self,
3486         dim_order: Mapping[Hashable, Sequence[int]] = None,
3487         **dim_order_kwargs: Sequence[int],
3488     ) -> "Dataset":
3489         """Rearrange index levels using input order.
3490 
3491         Parameters
3492         ----------
3493         dim_order : optional
3494             Mapping from names matching dimensions and values given
3495             by lists representing new level orders. Every given dimension
3496             must have a multi-index.
3497         **dim_order_kwargs : optional
3498             The keyword arguments form of ``dim_order``.
3499             One of dim_order or dim_order_kwargs must be provided.
3500 
3501         Returns
3502         -------
3503         obj : Dataset
3504             Another dataset, with this dataset's data but replaced
3505             coordinates.
3506         """
3507         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3508         variables = self._variables.copy()
3509         indexes = dict(self.indexes)
3510         for dim, order in dim_order.items():
3511             coord = self._variables[dim]
3512             index = self.indexes[dim]
3513             if not isinstance(index, pd.MultiIndex):
3514                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3515             new_index = index.reorder_levels(order)
3516             variables[dim] = IndexVariable(coord.dims, new_index)
3517             indexes[dim] = new_index
3518 
3519         return self._replace(variables, indexes=indexes)
3520 
3521     def _stack_once(self, dims, new_dim):
3522         if ... in dims:
3523             dims = list(infix_dims(dims, self.dims))
3524         variables = {}
3525         for name, var in self.variables.items():
3526             if name not in dims:
3527                 if any(d in var.dims for d in dims):
3528                     add_dims = [d for d in dims if d not in var.dims]
3529                     vdims = list(var.dims) + add_dims
3530                     shape = [self.dims[d] for d in vdims]
3531                     exp_var = var.set_dims(vdims, shape)
3532                     stacked_var = exp_var.stack(**{new_dim: dims})
3533                     variables[name] = stacked_var
3534                 else:
3535                     variables[name] = var.copy(deep=False)
3536 
3537         # consider dropping levels that are unused?
3538         levels = [self.get_index(dim) for dim in dims]
3539         idx = utils.multiindex_from_product_levels(levels, names=dims)
3540         variables[new_dim] = IndexVariable(new_dim, idx)
3541 
3542         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3543 
3544         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3545         indexes[new_dim] = idx
3546 
3547         return self._replace_with_new_dims(
3548             variables, coord_names=coord_names, indexes=indexes
3549         )
3550 
3551     def stack(
3552         self,
3553         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3554         **dimensions_kwargs: Sequence[Hashable],
3555     ) -> "Dataset":
3556         """
3557         Stack any number of existing dimensions into a single new dimension.
3558 
3559         New dimensions will be added at the end, and the corresponding
3560         coordinate variables will be combined into a MultiIndex.
3561 
3562         Parameters
3563         ----------
3564         dimensions : mapping of hashable to sequence of hashable
3565             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
3566             dimensions, and the existing dimensions that they replace. An
3567             ellipsis (`...`) will be replaced by all unlisted dimensions.
3568             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
3569             all dimensions.
3570         **dimensions_kwargs
3571             The keyword arguments form of ``dimensions``.
3572             One of dimensions or dimensions_kwargs must be provided.
3573 
3574         Returns
3575         -------
3576         stacked : Dataset
3577             Dataset with stacked data.
3578 
3579         See also
3580         --------
3581         Dataset.unstack
3582         """
3583         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3584         result = self
3585         for new_dim, dims in dimensions.items():
3586             result = result._stack_once(dims, new_dim)
3587         return result
3588 
3589     def to_stacked_array(
3590         self,
3591         new_dim: Hashable,
3592         sample_dims: Sequence[Hashable],
3593         variable_dim: str = "variable",
3594         name: Hashable = None,
3595     ) -> "DataArray":
3596         """Combine variables of differing dimensionality into a DataArray
3597         without broadcasting.
3598 
3599         This method is similar to Dataset.to_array but does not broadcast the
3600         variables.
3601 
3602         Parameters
3603         ----------
3604         new_dim : hashable
3605             Name of the new stacked coordinate
3606         sample_dims : sequence of hashable
3607             Dimensions that **will not** be stacked. Each array in the dataset
3608             must share these dimensions. For machine learning applications,
3609             these define the dimensions over which samples are drawn.
3610         variable_dim : str, optional
3611             Name of the level in the stacked coordinate which corresponds to
3612             the variables.
3613         name : str, optional
3614             Name of the new data array.
3615 
3616         Returns
3617         -------
3618         stacked : DataArray
3619             DataArray with the specified dimensions and data variables
3620             stacked together. The stacked coordinate is named ``new_dim``
3621             and represented by a MultiIndex object with a level containing the
3622             data variable names. The name of this level is controlled using
3623             the ``variable_dim`` argument.
3624 
3625         See Also
3626         --------
3627         Dataset.to_array
3628         Dataset.stack
3629         DataArray.to_unstacked_dataset
3630 
3631         Examples
3632         --------
3633         >>> data = xr.Dataset(
3634         ...     data_vars={
3635         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3636         ...         "b": ("x", [6, 7]),
3637         ...     },
3638         ...     coords={"y": ["u", "v", "w"]},
3639         ... )
3640 
3641         >>> data
3642         <xarray.Dataset>
3643         Dimensions:  (x: 2, y: 3)
3644         Coordinates:
3645           * y        (y) <U1 'u' 'v' 'w'
3646         Dimensions without coordinates: x
3647         Data variables:
3648             a        (x, y) int64 0 1 2 3 4 5
3649             b        (x) int64 6 7
3650 
3651         >>> data.to_stacked_array("z", sample_dims=["x"])
3652         <xarray.DataArray 'a' (x: 2, z: 4)>
3653         array([[0, 1, 2, 6],
3654                [3, 4, 5, 7]])
3655         Coordinates:
3656           * z         (z) MultiIndex
3657           - variable  (z) object 'a' 'a' 'a' 'b'
3658           - y         (z) object 'u' 'v' 'w' nan
3659         Dimensions without coordinates: x
3660 
3661         """
3662         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3663 
3664         for variable in self:
3665             dims = self[variable].dims
3666             dims_include_sample_dims = set(sample_dims) <= set(dims)
3667             if not dims_include_sample_dims:
3668                 raise ValueError(
3669                     "All variables in the dataset must contain the "
3670                     "dimensions {}.".format(dims)
3671                 )
3672 
3673         def ensure_stackable(val):
3674             assign_coords = {variable_dim: val.name}
3675             for dim in stacking_dims:
3676                 if dim not in val.dims:
3677                     assign_coords[dim] = None
3678 
3679             expand_dims = set(stacking_dims).difference(set(val.dims))
3680             expand_dims.add(variable_dim)
3681             # must be list for .expand_dims
3682             expand_dims = list(expand_dims)
3683 
3684             return (
3685                 val.assign_coords(**assign_coords)
3686                 .expand_dims(expand_dims)
3687                 .stack({new_dim: (variable_dim,) + stacking_dims})
3688             )
3689 
3690         # concatenate the arrays
3691         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3692         data_array = xr.concat(stackable_vars, dim=new_dim)
3693 
3694         # coerce the levels of the MultiIndex to have the same type as the
3695         # input dimensions. This code is messy, so it might be better to just
3696         # input a dummy value for the singleton dimension.
3697         idx = data_array.indexes[new_dim]
3698         levels = [idx.levels[0]] + [
3699             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3700         ]
3701         new_idx = idx.set_levels(levels)
3702         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3703 
3704         if name is not None:
3705             data_array.name = name
3706 
3707         return data_array
3708 
3709     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> "Dataset":
3710         index = self.get_index(dim)
3711         index = remove_unused_levels_categories(index)
3712         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3713 
3714         # take a shortcut in case the MultiIndex was not modified.
3715         if index.equals(full_idx):
3716             obj = self
3717         else:
3718             obj = self._reindex(
3719                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3720             )
3721 
3722         new_dim_names = index.names
3723         new_dim_sizes = [lev.size for lev in index.levels]
3724 
3725         variables: Dict[Hashable, Variable] = {}
3726         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3727 
3728         for name, var in obj.variables.items():
3729             if name != dim:
3730                 if dim in var.dims:
3731                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3732                     variables[name] = var.unstack({dim: new_dims})
3733                 else:
3734                     variables[name] = var
3735 
3736         for name, lev in zip(new_dim_names, index.levels):
3737             variables[name] = IndexVariable(name, lev)
3738             indexes[name] = lev
3739 
3740         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3741 
3742         return self._replace_with_new_dims(
3743             variables, coord_names=coord_names, indexes=indexes
3744         )
3745 
3746     def unstack(
3747         self,
3748         dim: Union[Hashable, Iterable[Hashable]] = None,
3749         fill_value: Any = dtypes.NA,
3750         sparse: bool = False,
3751     ) -> "Dataset":
3752         """
3753         Unstack existing dimensions corresponding to MultiIndexes into
3754         multiple new dimensions.
3755 
3756         New dimensions will be added at the end.
3757 
3758         Parameters
3759         ----------
3760         dim : hashable or iterable of hashable, optional
3761             Dimension(s) over which to unstack. By default unstacks all
3762             MultiIndexes.
3763         fill_value : scalar or dict-like, default: nan
3764             value to be filled. If a dict-like, maps variable names to
3765             fill values. If not provided or if the dict-like does not
3766             contain all variables, the dtype's NA value will be used.
3767         sparse : bool, default: False
3768             use sparse-array if True
3769 
3770         Returns
3771         -------
3772         unstacked : Dataset
3773             Dataset with unstacked data.
3774 
3775         See also
3776         --------
3777         Dataset.stack
3778         """
3779         if dim is None:
3780             dims = [
3781                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3782             ]
3783         else:
3784             if isinstance(dim, str) or not isinstance(dim, Iterable):
3785                 dims = [dim]
3786             else:
3787                 dims = list(dim)
3788 
3789             missing_dims = [d for d in dims if d not in self.dims]
3790             if missing_dims:
3791                 raise ValueError(
3792                     "Dataset does not contain the dimensions: %s" % missing_dims
3793                 )
3794 
3795             non_multi_dims = [
3796                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3797             ]
3798             if non_multi_dims:
3799                 raise ValueError(
3800                     "cannot unstack dimensions that do not "
3801                     "have a MultiIndex: %s" % non_multi_dims
3802                 )
3803 
3804         result = self.copy(deep=False)
3805         for dim in dims:
3806             result = result._unstack_once(dim, fill_value, sparse)
3807         return result
3808 
3809     def update(self, other: "CoercibleMapping") -> "Dataset":
3810         """Update this dataset's variables with those from another dataset.
3811 
3812         Parameters
3813         ----------
3814         other : Dataset or mapping
3815             Variables with which to update this dataset. One of:
3816 
3817             - Dataset
3818             - mapping {var name: DataArray}
3819             - mapping {var name: Variable}
3820             - mapping {var name: (dimension name, array-like)}
3821             - mapping {var name: (tuple of dimension names, array-like)}
3822 
3823 
3824         Returns
3825         -------
3826         updated : Dataset
3827             Updated dataset.
3828 
3829         Raises
3830         ------
3831         ValueError
3832             If any dimensions would have inconsistent sizes in the updated
3833             dataset.
3834         """
3835         merge_result = dataset_update_method(self, other)
3836         return self._replace(inplace=True, **merge_result._asdict())
3837 
3838     def merge(
3839         self,
3840         other: Union["CoercibleMapping", "DataArray"],
3841         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3842         compat: str = "no_conflicts",
3843         join: str = "outer",
3844         fill_value: Any = dtypes.NA,
3845     ) -> "Dataset":
3846         """Merge the arrays of two datasets into a single dataset.
3847 
3848         This method generally does not allow for overriding data, with the
3849         exception of attributes, which are ignored on the second dataset.
3850         Variables with the same name are checked for conflicts via the equals
3851         or identical methods.
3852 
3853         Parameters
3854         ----------
3855         other : Dataset or mapping
3856             Dataset or variables to merge with this dataset.
3857         overwrite_vars : hashable or iterable of hashable, optional
3858             If provided, update variables of these name(s) without checking for
3859             conflicts in this dataset.
3860         compat : {"broadcast_equals", "equals", "identical", \
3861                   "no_conflicts"}, optional
3862             String indicating how to compare variables of the same name for
3863             potential conflicts:
3864 
3865             - 'broadcast_equals': all values must be equal when variables are
3866               broadcast against each other to ensure common dimensions.
3867             - 'equals': all values and dimensions must be the same.
3868             - 'identical': all values, dimensions and attributes must be the
3869               same.
3870             - 'no_conflicts': only values which are not null in both datasets
3871               must be equal. The returned dataset then contains the combination
3872               of all non-null values.
3873 
3874         join : {"outer", "inner", "left", "right", "exact"}, optional
3875             Method for joining ``self`` and ``other`` along shared dimensions:
3876 
3877             - 'outer': use the union of the indexes
3878             - 'inner': use the intersection of the indexes
3879             - 'left': use indexes from ``self``
3880             - 'right': use indexes from ``other``
3881             - 'exact': error instead of aligning non-equal indexes
3882         fill_value : scalar or dict-like, optional
3883             Value to use for newly missing values. If a dict-like, maps
3884             variable names (including coordinates) to fill values.
3885 
3886         Returns
3887         -------
3888         merged : Dataset
3889             Merged dataset.
3890 
3891         Raises
3892         ------
3893         MergeError
3894             If any variables conflict (see ``compat``).
3895         """
3896         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3897         merge_result = dataset_merge_method(
3898             self,
3899             other,
3900             overwrite_vars=overwrite_vars,
3901             compat=compat,
3902             join=join,
3903             fill_value=fill_value,
3904         )
3905         return self._replace(**merge_result._asdict())
3906 
3907     def _assert_all_in_dataset(
3908         self, names: Iterable[Hashable], virtual_okay: bool = False
3909     ) -> None:
3910         bad_names = set(names) - set(self._variables)
3911         if virtual_okay:
3912             bad_names -= self.virtual_variables
3913         if bad_names:
3914             raise ValueError(
3915                 "One or more of the specified variables "
3916                 "cannot be found in this dataset"
3917             )
3918 
3919     def drop_vars(
3920         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3921     ) -> "Dataset":
3922         """Drop variables from this dataset.
3923 
3924         Parameters
3925         ----------
3926         names : hashable or iterable of hashable
3927             Name(s) of variables to drop.
3928         errors : {"raise", "ignore"}, optional
3929             If 'raise' (default), raises a ValueError error if any of the variable
3930             passed are not in the dataset. If 'ignore', any given names that are in the
3931             dataset are dropped and no error is raised.
3932 
3933         Returns
3934         -------
3935         dropped : Dataset
3936 
3937         """
3938         # the Iterable check is required for mypy
3939         if is_scalar(names) or not isinstance(names, Iterable):
3940             names = {names}
3941         else:
3942             names = set(names)
3943         if errors == "raise":
3944             self._assert_all_in_dataset(names)
3945 
3946         variables = {k: v for k, v in self._variables.items() if k not in names}
3947         coord_names = {k for k in self._coord_names if k in variables}
3948         indexes = {k: v for k, v in self.indexes.items() if k not in names}
3949         return self._replace_with_new_dims(
3950             variables, coord_names=coord_names, indexes=indexes
3951         )
3952 
3953     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
3954         """Backward compatible method based on `drop_vars` and `drop_sel`
3955 
3956         Using either `drop_vars` or `drop_sel` is encouraged
3957 
3958         See Also
3959         --------
3960         Dataset.drop_vars
3961         Dataset.drop_sel
3962         """
3963         if errors not in ["raise", "ignore"]:
3964             raise ValueError('errors must be either "raise" or "ignore"')
3965 
3966         if is_dict_like(labels) and not isinstance(labels, dict):
3967             warnings.warn(
3968                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
3969                 FutureWarning,
3970                 stacklevel=2,
3971             )
3972             return self.drop_vars(labels, errors=errors)
3973 
3974         if labels_kwargs or isinstance(labels, dict):
3975             if dim is not None:
3976                 raise ValueError("cannot specify dim and dict-like arguments.")
3977             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3978 
3979         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
3980             warnings.warn(
3981                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
3982                 PendingDeprecationWarning,
3983                 stacklevel=2,
3984             )
3985             return self.drop_vars(labels, errors=errors)
3986         if dim is not None:
3987             warnings.warn(
3988                 "dropping labels using list-like labels is deprecated; using "
3989                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
3990                 DeprecationWarning,
3991                 stacklevel=2,
3992             )
3993             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
3994 
3995         warnings.warn(
3996             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
3997             PendingDeprecationWarning,
3998             stacklevel=2,
3999         )
4000         return self.drop_sel(labels, errors=errors)
4001 
4002     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
4003         """Drop index labels from this dataset.
4004 
4005         Parameters
4006         ----------
4007         labels : mapping of hashable to Any
4008             Index labels to drop
4009         errors : {"raise", "ignore"}, optional
4010             If 'raise' (default), raises a ValueError error if
4011             any of the index labels passed are not
4012             in the dataset. If 'ignore', any given labels that are in the
4013             dataset are dropped and no error is raised.
4014         **labels_kwargs : {dim: label, ...}, optional
4015             The keyword arguments form of ``dim`` and ``labels``
4016 
4017         Returns
4018         -------
4019         dropped : Dataset
4020 
4021         Examples
4022         --------
4023         >>> data = np.random.randn(2, 3)
4024         >>> labels = ["a", "b", "c"]
4025         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4026         >>> ds.drop_sel(y=["a", "c"])
4027         <xarray.Dataset>
4028         Dimensions:  (x: 2, y: 1)
4029         Coordinates:
4030           * y        (y) <U1 'b'
4031         Dimensions without coordinates: x
4032         Data variables:
4033             A        (x, y) float64 0.4002 1.868
4034         >>> ds.drop_sel(y="b")
4035         <xarray.Dataset>
4036         Dimensions:  (x: 2, y: 2)
4037         Coordinates:
4038           * y        (y) <U1 'a' 'c'
4039         Dimensions without coordinates: x
4040         Data variables:
4041             A        (x, y) float64 1.764 0.9787 2.241 -0.9773
4042         """
4043         if errors not in ["raise", "ignore"]:
4044             raise ValueError('errors must be either "raise" or "ignore"')
4045 
4046         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
4047 
4048         ds = self
4049         for dim, labels_for_dim in labels.items():
4050             # Don't cast to set, as it would harm performance when labels
4051             # is a large numpy array
4052             if utils.is_scalar(labels_for_dim):
4053                 labels_for_dim = [labels_for_dim]
4054             labels_for_dim = np.asarray(labels_for_dim)
4055             try:
4056                 index = self.indexes[dim]
4057             except KeyError:
4058                 raise ValueError("dimension %r does not have coordinate labels" % dim)
4059             new_index = index.drop(labels_for_dim, errors=errors)
4060             ds = ds.loc[{dim: new_index}]
4061         return ds
4062 
4063     def drop_dims(
4064         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
4065     ) -> "Dataset":
4066         """Drop dimensions and associated variables from this dataset.
4067 
4068         Parameters
4069         ----------
4070         drop_dims : hashable or iterable of hashable
4071             Dimension or dimensions to drop.
4072         errors : {"raise", "ignore"}, optional
4073             If 'raise' (default), raises a ValueError error if any of the
4074             dimensions passed are not in the dataset. If 'ignore', any given
4075             labels that are in the dataset are dropped and no error is raised.
4076 
4077         Returns
4078         -------
4079         obj : Dataset
4080             The dataset without the given dimensions (or any variables
4081             containing those dimensions)
4082         errors : {"raise", "ignore"}, optional
4083             If 'raise' (default), raises a ValueError error if
4084             any of the dimensions passed are not
4085             in the dataset. If 'ignore', any given dimensions that are in the
4086             dataset are dropped and no error is raised.
4087         """
4088         if errors not in ["raise", "ignore"]:
4089             raise ValueError('errors must be either "raise" or "ignore"')
4090 
4091         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
4092             drop_dims = {drop_dims}
4093         else:
4094             drop_dims = set(drop_dims)
4095 
4096         if errors == "raise":
4097             missing_dims = drop_dims - set(self.dims)
4098             if missing_dims:
4099                 raise ValueError(
4100                     "Dataset does not contain the dimensions: %s" % missing_dims
4101                 )
4102 
4103         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
4104         return self.drop_vars(drop_vars)
4105 
4106     def transpose(self, *dims: Hashable) -> "Dataset":
4107         """Return a new Dataset object with all array dimensions transposed.
4108 
4109         Although the order of dimensions on each array will change, the dataset
4110         dimensions themselves will remain in fixed (sorted) order.
4111 
4112         Parameters
4113         ----------
4114         *dims : hashable, optional
4115             By default, reverse the dimensions on each array. Otherwise,
4116             reorder the dimensions to this order.
4117 
4118         Returns
4119         -------
4120         transposed : Dataset
4121             Each array in the dataset (including) coordinates will be
4122             transposed to the given order.
4123 
4124         Notes
4125         -----
4126         This operation returns a view of each array's data. It is
4127         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
4128         -- the data will be fully loaded into memory.
4129 
4130         See Also
4131         --------
4132         numpy.transpose
4133         DataArray.transpose
4134         """
4135         if dims:
4136             if set(dims) ^ set(self.dims) and ... not in dims:
4137                 raise ValueError(
4138                     "arguments to transpose (%s) must be "
4139                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
4140                 )
4141         ds = self.copy()
4142         for name, var in self._variables.items():
4143             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
4144             ds._variables[name] = var.transpose(*var_dims)
4145         return ds
4146 
4147     def dropna(
4148         self,
4149         dim: Hashable,
4150         how: str = "any",
4151         thresh: int = None,
4152         subset: Iterable[Hashable] = None,
4153     ):
4154         """Returns a new dataset with dropped labels for missing values along
4155         the provided dimension.
4156 
4157         Parameters
4158         ----------
4159         dim : hashable
4160             Dimension along which to drop missing values. Dropping along
4161             multiple dimensions simultaneously is not yet supported.
4162         how : {"any", "all"}, default: "any"
4163             * any : if any NA values are present, drop that label
4164             * all : if all values are NA, drop that label
4165         thresh : int, default: None
4166             If supplied, require this many non-NA values.
4167         subset : iterable of hashable, optional
4168             Which variables to check for missing values. By default, all
4169             variables in the dataset are checked.
4170 
4171         Returns
4172         -------
4173         Dataset
4174         """
4175         # TODO: consider supporting multiple dimensions? Or not, given that
4176         # there are some ugly edge cases, e.g., pandas's dropna differs
4177         # depending on the order of the supplied axes.
4178 
4179         if dim not in self.dims:
4180             raise ValueError("%s must be a single dataset dimension" % dim)
4181 
4182         if subset is None:
4183             subset = iter(self.data_vars)
4184 
4185         count = np.zeros(self.dims[dim], dtype=np.int64)
4186         size = 0
4187 
4188         for k in subset:
4189             array = self._variables[k]
4190             if dim in array.dims:
4191                 dims = [d for d in array.dims if d != dim]
4192                 count += np.asarray(array.count(dims))  # type: ignore
4193                 size += np.prod([self.dims[d] for d in dims])
4194 
4195         if thresh is not None:
4196             mask = count >= thresh
4197         elif how == "any":
4198             mask = count == size
4199         elif how == "all":
4200             mask = count > 0
4201         elif how is not None:
4202             raise ValueError("invalid how option: %s" % how)
4203         else:
4204             raise TypeError("must specify how or thresh")
4205 
4206         return self.isel({dim: mask})
4207 
4208     def fillna(self, value: Any) -> "Dataset":
4209         """Fill missing values in this object.
4210 
4211         This operation follows the normal broadcasting and alignment rules that
4212         xarray uses for binary arithmetic, except the result is aligned to this
4213         object (``join='left'``) instead of aligned to the intersection of
4214         index coordinates (``join='inner'``).
4215 
4216         Parameters
4217         ----------
4218         value : scalar, ndarray, DataArray, dict or Dataset
4219             Used to fill all matching missing values in this dataset's data
4220             variables. Scalars, ndarrays or DataArrays arguments are used to
4221             fill all data with aligned coordinates (for DataArrays).
4222             Dictionaries or datasets match data variables and then align
4223             coordinates if necessary.
4224 
4225         Returns
4226         -------
4227         Dataset
4228 
4229         Examples
4230         --------
4231 
4232         >>> import numpy as np
4233         >>> import xarray as xr
4234         >>> ds = xr.Dataset(
4235         ...     {
4236         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
4237         ...         "B": ("x", [3, 4, np.nan, 1]),
4238         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
4239         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
4240         ...     },
4241         ...     coords={"x": [0, 1, 2, 3]},
4242         ... )
4243         >>> ds
4244         <xarray.Dataset>
4245         Dimensions:  (x: 4)
4246         Coordinates:
4247           * x        (x) int64 0 1 2 3
4248         Data variables:
4249             A        (x) float64 nan 2.0 nan 0.0
4250             B        (x) float64 3.0 4.0 nan 1.0
4251             C        (x) float64 nan nan nan 5.0
4252             D        (x) float64 nan 3.0 nan 4.0
4253 
4254         Replace all `NaN` values with 0s.
4255 
4256         >>> ds.fillna(0)
4257         <xarray.Dataset>
4258         Dimensions:  (x: 4)
4259         Coordinates:
4260           * x        (x) int64 0 1 2 3
4261         Data variables:
4262             A        (x) float64 0.0 2.0 0.0 0.0
4263             B        (x) float64 3.0 4.0 0.0 1.0
4264             C        (x) float64 0.0 0.0 0.0 5.0
4265             D        (x) float64 0.0 3.0 0.0 4.0
4266 
4267         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
4268 
4269         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4270         >>> ds.fillna(value=values)
4271         <xarray.Dataset>
4272         Dimensions:  (x: 4)
4273         Coordinates:
4274           * x        (x) int64 0 1 2 3
4275         Data variables:
4276             A        (x) float64 0.0 2.0 0.0 0.0
4277             B        (x) float64 3.0 4.0 1.0 1.0
4278             C        (x) float64 2.0 2.0 2.0 5.0
4279             D        (x) float64 3.0 3.0 3.0 4.0
4280         """
4281         if utils.is_dict_like(value):
4282             value_keys = getattr(value, "data_vars", value).keys()
4283             if not set(value_keys) <= set(self.data_vars.keys()):
4284                 raise ValueError(
4285                     "all variables in the argument to `fillna` "
4286                     "must be contained in the original dataset"
4287                 )
4288         out = ops.fillna(self, value)
4289         return out
4290 
4291     def interpolate_na(
4292         self,
4293         dim: Hashable = None,
4294         method: str = "linear",
4295         limit: int = None,
4296         use_coordinate: Union[bool, Hashable] = True,
4297         max_gap: Union[
4298             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4299         ] = None,
4300         **kwargs: Any,
4301     ) -> "Dataset":
4302         """Fill in NaNs by interpolating according to different methods.
4303 
4304         Parameters
4305         ----------
4306         dim : str
4307             Specifies the dimension along which to interpolate.
4308 
4309         method : str, optional
4310             String indicating which method to use for interpolation:
4311 
4312             - 'linear': linear interpolation (Default). Additional keyword
4313               arguments are passed to :py:func:`numpy.interp`
4314             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4315               are passed to :py:func:`scipy.interpolate.interp1d`. If
4316               ``method='polynomial'``, the ``order`` keyword argument must also be
4317               provided.
4318             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4319               respective :py:class:`scipy.interpolate` classes.
4320 
4321         use_coordinate : bool, str, default: True
4322             Specifies which index to use as the x values in the interpolation
4323             formulated as `y = f(x)`. If False, values are treated as if
4324             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4325             used. If ``use_coordinate`` is a string, it specifies the name of a
4326             coordinate variariable to use as the index.
4327         limit : int, default: None
4328             Maximum number of consecutive NaNs to fill. Must be greater than 0
4329             or None for no limit. This filling is done regardless of the size of
4330             the gap in the data. To only interpolate over gaps less than a given length,
4331             see ``max_gap``.
4332         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
4333             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4334             Use None for no limit. When interpolating along a datetime64 dimension
4335             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4336 
4337             - a string that is valid input for pandas.to_timedelta
4338             - a :py:class:`numpy.timedelta64` object
4339             - a :py:class:`pandas.Timedelta` object
4340             - a :py:class:`datetime.timedelta` object
4341 
4342             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4343             dimensions has not been implemented yet. Gap length is defined as the difference
4344             between coordinate values at the first data point after a gap and the last value
4345             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4346             between coordinate values at the first (last) valid data point and the first (last) NaN.
4347             For example, consider::
4348 
4349                 <xarray.DataArray (x: 9)>
4350                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4351                 Coordinates:
4352                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4353 
4354             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4355         kwargs : dict, optional
4356             parameters passed verbatim to the underlying interpolation function
4357 
4358         Returns
4359         -------
4360         interpolated: Dataset
4361             Filled in Dataset.
4362 
4363         See also
4364         --------
4365         numpy.interp
4366         scipy.interpolate
4367 
4368         Examples
4369         --------
4370         >>> ds = xr.Dataset(
4371         ...     {
4372         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
4373         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
4374         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
4375         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
4376         ...     },
4377         ...     coords={"x": [0, 1, 2, 3, 4]},
4378         ... )
4379         >>> ds
4380         <xarray.Dataset>
4381         Dimensions:  (x: 5)
4382         Coordinates:
4383           * x        (x) int64 0 1 2 3 4
4384         Data variables:
4385             A        (x) float64 nan 2.0 3.0 nan 0.0
4386             B        (x) float64 3.0 4.0 nan 1.0 7.0
4387             C        (x) float64 nan nan nan 5.0 0.0
4388             D        (x) float64 nan 3.0 nan -1.0 4.0
4389 
4390         >>> ds.interpolate_na(dim="x", method="linear")
4391         <xarray.Dataset>
4392         Dimensions:  (x: 5)
4393         Coordinates:
4394           * x        (x) int64 0 1 2 3 4
4395         Data variables:
4396             A        (x) float64 nan 2.0 3.0 1.5 0.0
4397             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4398             C        (x) float64 nan nan nan 5.0 0.0
4399             D        (x) float64 nan 3.0 1.0 -1.0 4.0
4400 
4401         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
4402         <xarray.Dataset>
4403         Dimensions:  (x: 5)
4404         Coordinates:
4405           * x        (x) int64 0 1 2 3 4
4406         Data variables:
4407             A        (x) float64 1.0 2.0 3.0 1.5 0.0
4408             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4409             C        (x) float64 20.0 15.0 10.0 5.0 0.0
4410             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
4411         """
4412         from .missing import _apply_over_vars_with_dim, interp_na
4413 
4414         new = _apply_over_vars_with_dim(
4415             interp_na,
4416             self,
4417             dim=dim,
4418             method=method,
4419             limit=limit,
4420             use_coordinate=use_coordinate,
4421             max_gap=max_gap,
4422             **kwargs,
4423         )
4424         return new
4425 
4426     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4427         """Fill NaN values by propogating values forward
4428 
4429         *Requires bottleneck.*
4430 
4431         Parameters
4432         ----------
4433         dim : Hashable
4434             Specifies the dimension along which to propagate values when
4435             filling.
4436         limit : int, default: None
4437             The maximum number of consecutive NaN values to forward fill. In
4438             other words, if there is a gap with more than this number of
4439             consecutive NaNs, it will only be partially filled. Must be greater
4440             than 0 or None for no limit.
4441 
4442         Returns
4443         -------
4444         Dataset
4445         """
4446         from .missing import _apply_over_vars_with_dim, ffill
4447 
4448         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4449         return new
4450 
4451     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4452         """Fill NaN values by propogating values backward
4453 
4454         *Requires bottleneck.*
4455 
4456         Parameters
4457         ----------
4458         dim : str
4459             Specifies the dimension along which to propagate values when
4460             filling.
4461         limit : int, default: None
4462             The maximum number of consecutive NaN values to backward fill. In
4463             other words, if there is a gap with more than this number of
4464             consecutive NaNs, it will only be partially filled. Must be greater
4465             than 0 or None for no limit.
4466 
4467         Returns
4468         -------
4469         Dataset
4470         """
4471         from .missing import _apply_over_vars_with_dim, bfill
4472 
4473         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4474         return new
4475 
4476     def combine_first(self, other: "Dataset") -> "Dataset":
4477         """Combine two Datasets, default to data_vars of self.
4478 
4479         The new coordinates follow the normal broadcasting and alignment rules
4480         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4481         filled with np.nan.
4482 
4483         Parameters
4484         ----------
4485         other : Dataset
4486             Used to fill all matching missing values in this array.
4487 
4488         Returns
4489         -------
4490         Dataset
4491         """
4492         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4493         return out
4494 
4495     def reduce(
4496         self,
4497         func: Callable,
4498         dim: Union[Hashable, Iterable[Hashable]] = None,
4499         keep_attrs: bool = None,
4500         keepdims: bool = False,
4501         numeric_only: bool = False,
4502         **kwargs: Any,
4503     ) -> "Dataset":
4504         """Reduce this dataset by applying `func` along some dimension(s).
4505 
4506         Parameters
4507         ----------
4508         func : callable
4509             Function which can be called in the form
4510             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4511             np.ndarray over an integer valued axis.
4512         dim : str or sequence of str, optional
4513             Dimension(s) over which to apply `func`.  By default `func` is
4514             applied over all dimensions.
4515         keep_attrs : bool, optional
4516             If True, the dataset's attributes (`attrs`) will be copied from
4517             the original object to the new one.  If False (default), the new
4518             object will be returned without attributes.
4519         keepdims : bool, default: False
4520             If True, the dimensions which are reduced are left in the result
4521             as dimensions of size one. Coordinates that use these dimensions
4522             are removed.
4523         numeric_only : bool, optional
4524             If True, only apply ``func`` to variables with a numeric dtype.
4525         **kwargs : Any
4526             Additional keyword arguments passed on to ``func``.
4527 
4528         Returns
4529         -------
4530         reduced : Dataset
4531             Dataset with this object's DataArrays replaced with new DataArrays
4532             of summarized data and the indicated dimension(s) removed.
4533         """
4534         if dim is None or dim is ...:
4535             dims = set(self.dims)
4536         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4537             dims = {dim}
4538         else:
4539             dims = set(dim)
4540 
4541         missing_dimensions = [d for d in dims if d not in self.dims]
4542         if missing_dimensions:
4543             raise ValueError(
4544                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4545             )
4546 
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=False)
4549 
4550         variables: Dict[Hashable, Variable] = {}
4551         for name, var in self._variables.items():
4552             reduce_dims = [d for d in var.dims if d in dims]
4553             if name in self.coords:
4554                 if not reduce_dims:
4555                     variables[name] = var
4556             else:
4557                 if (
4558                     not numeric_only
4559                     or np.issubdtype(var.dtype, np.number)
4560                     or (var.dtype == np.bool_)
4561                 ):
4562                     if len(reduce_dims) == 1:
4563                         # unpack dimensions for the benefit of functions
4564                         # like np.argmin which can't handle tuple arguments
4565                         (reduce_dims,) = reduce_dims
4566                     elif len(reduce_dims) == var.ndim:
4567                         # prefer to aggregate over axis=None rather than
4568                         # axis=(0, 1) if they will be equivalent, because
4569                         # the former is often more efficient
4570                         reduce_dims = None  # type: ignore
4571                     variables[name] = var.reduce(
4572                         func,
4573                         dim=reduce_dims,
4574                         keep_attrs=keep_attrs,
4575                         keepdims=keepdims,
4576                         **kwargs,
4577                     )
4578 
4579         coord_names = {k for k in self.coords if k in variables}
4580         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4581         attrs = self.attrs if keep_attrs else None
4582         return self._replace_with_new_dims(
4583             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4584         )
4585 
4586     def map(
4587         self,
4588         func: Callable,
4589         keep_attrs: bool = None,
4590         args: Iterable[Any] = (),
4591         **kwargs: Any,
4592     ) -> "Dataset":
4593         """Apply a function to each variable in this dataset
4594 
4595         Parameters
4596         ----------
4597         func : callable
4598             Function which can be called in the form `func(x, *args, **kwargs)`
4599             to transform each DataArray `x` in this dataset into another
4600             DataArray.
4601         keep_attrs : bool, optional
4602             If True, the dataset's attributes (`attrs`) will be copied from
4603             the original object to the new one. If False, the new object will
4604             be returned without attributes.
4605         args : tuple, optional
4606             Positional arguments passed on to `func`.
4607         **kwargs : Any
4608             Keyword arguments passed on to `func`.
4609 
4610         Returns
4611         -------
4612         applied : Dataset
4613             Resulting dataset from applying ``func`` to each data variable.
4614 
4615         Examples
4616         --------
4617         >>> da = xr.DataArray(np.random.randn(2, 3))
4618         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4619         >>> ds
4620         <xarray.Dataset>
4621         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4622         Dimensions without coordinates: dim_0, dim_1, x
4623         Data variables:
4624             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
4625             bar      (x) int64 -1 2
4626         >>> ds.map(np.fabs)
4627         <xarray.Dataset>
4628         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4629         Dimensions without coordinates: dim_0, dim_1, x
4630         Data variables:
4631             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
4632             bar      (x) float64 1.0 2.0
4633         """
4634         if keep_attrs is None:
4635             keep_attrs = _get_keep_attrs(default=False)
4636         variables = {
4637             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4638             for k, v in self.data_vars.items()
4639         }
4640         if keep_attrs:
4641             for k, v in variables.items():
4642                 v._copy_attrs_from(self.data_vars[k])
4643         attrs = self.attrs if keep_attrs else None
4644         return type(self)(variables, attrs=attrs)
4645 
4646     def apply(
4647         self,
4648         func: Callable,
4649         keep_attrs: bool = None,
4650         args: Iterable[Any] = (),
4651         **kwargs: Any,
4652     ) -> "Dataset":
4653         """
4654         Backward compatible implementation of ``map``
4655 
4656         See Also
4657         --------
4658         Dataset.map
4659         """
4660         warnings.warn(
4661             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4662             PendingDeprecationWarning,
4663             stacklevel=2,
4664         )
4665         return self.map(func, keep_attrs, args, **kwargs)
4666 
4667     def assign(
4668         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4669     ) -> "Dataset":
4670         """Assign new data variables to a Dataset, returning a new object
4671         with all the original variables in addition to the new ones.
4672 
4673         Parameters
4674         ----------
4675         variables : mapping of hashable to Any
4676             Mapping from variables names to the new values. If the new values
4677             are callable, they are computed on the Dataset and assigned to new
4678             data variables. If the values are not callable, (e.g. a DataArray,
4679             scalar, or array), they are simply assigned.
4680         **variables_kwargs
4681             The keyword arguments form of ``variables``.
4682             One of variables or variables_kwargs must be provided.
4683 
4684         Returns
4685         -------
4686         ds : Dataset
4687             A new Dataset with the new variables in addition to all the
4688             existing variables.
4689 
4690         Notes
4691         -----
4692         Since ``kwargs`` is a dictionary, the order of your arguments may not
4693         be preserved, and so the order of the new variables is not well
4694         defined. Assigning multiple variables within the same ``assign`` is
4695         possible, but you cannot reference other variables created within the
4696         same ``assign`` call.
4697 
4698         See Also
4699         --------
4700         pandas.DataFrame.assign
4701 
4702         Examples
4703         --------
4704         >>> x = xr.Dataset(
4705         ...     {
4706         ...         "temperature_c": (
4707         ...             ("lat", "lon"),
4708         ...             20 * np.random.rand(4).reshape(2, 2),
4709         ...         ),
4710         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4711         ...     },
4712         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4713         ... )
4714         >>> x
4715         <xarray.Dataset>
4716         Dimensions:        (lat: 2, lon: 2)
4717         Coordinates:
4718           * lat            (lat) int64 10 20
4719           * lon            (lon) int64 150 160
4720         Data variables:
4721             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4722             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4723 
4724         Where the value is a callable, evaluated on dataset:
4725 
4726         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4727         <xarray.Dataset>
4728         Dimensions:        (lat: 2, lon: 2)
4729         Coordinates:
4730           * lat            (lat) int64 10 20
4731           * lon            (lon) int64 150 160
4732         Data variables:
4733             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4734             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4735             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4736 
4737         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4738 
4739         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4740         <xarray.Dataset>
4741         Dimensions:        (lat: 2, lon: 2)
4742         Coordinates:
4743           * lat            (lat) int64 10 20
4744           * lon            (lon) int64 150 160
4745         Data variables:
4746             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4747             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4748             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4749 
4750         """
4751         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4752         data = self.copy()
4753         # do all calculations first...
4754         results = data._calc_assign_results(variables)
4755         # ... and then assign
4756         data.update(results)
4757         return data
4758 
4759     def to_array(self, dim="variable", name=None):
4760         """Convert this dataset into an xarray.DataArray
4761 
4762         The data variables of this dataset will be broadcast against each other
4763         and stacked along the first axis of the new array. All coordinates of
4764         this dataset will remain coordinates.
4765 
4766         Parameters
4767         ----------
4768         dim : str, optional
4769             Name of the new dimension.
4770         name : str, optional
4771             Name of the new data array.
4772 
4773         Returns
4774         -------
4775         array : xarray.DataArray
4776         """
4777         from .dataarray import DataArray
4778 
4779         data_vars = [self.variables[k] for k in self.data_vars]
4780         broadcast_vars = broadcast_variables(*data_vars)
4781         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4782 
4783         coords = dict(self.coords)
4784         coords[dim] = list(self.data_vars)
4785         indexes = propagate_indexes(self._indexes)
4786 
4787         dims = (dim,) + broadcast_vars[0].dims
4788 
4789         return DataArray(
4790             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4791         )
4792 
4793     def _normalize_dim_order(
4794         self, dim_order: List[Hashable] = None
4795     ) -> Dict[Hashable, int]:
4796         """
4797         Check the validity of the provided dimensions if any and return the mapping
4798         between dimension name and their size.
4799 
4800         Parameters
4801         ----------
4802         dim_order
4803             Dimension order to validate (default to the alphabetical order if None).
4804 
4805         Returns
4806         -------
4807         result
4808             Validated dimensions mapping.
4809 
4810         """
4811         if dim_order is None:
4812             dim_order = list(self.dims)
4813         elif set(dim_order) != set(self.dims):
4814             raise ValueError(
4815                 "dim_order {} does not match the set of dimensions of this "
4816                 "Dataset: {}".format(dim_order, list(self.dims))
4817             )
4818 
4819         ordered_dims = {k: self.dims[k] for k in dim_order}
4820 
4821         return ordered_dims
4822 
4823     def _to_dataframe(self, ordered_dims: Mapping[Hashable, int]):
4824         columns = [k for k in self.variables if k not in self.dims]
4825         data = [
4826             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4827             for k in columns
4828         ]
4829         index = self.coords.to_index([*ordered_dims])
4830         return pd.DataFrame(dict(zip(columns, data)), index=index)
4831 
4832     def to_dataframe(self, dim_order: List[Hashable] = None) -> pd.DataFrame:
4833         """Convert this dataset into a pandas.DataFrame.
4834 
4835         Non-index variables in this dataset form the columns of the
4836         DataFrame. The DataFrame is indexed by the Cartesian product of
4837         this dataset's indices.
4838 
4839         Parameters
4840         ----------
4841         dim_order
4842             Hierarchical dimension order for the resulting dataframe. All
4843             arrays are transposed to this order and then written out as flat
4844             vectors in contiguous order, so the last dimension in this list
4845             will be contiguous in the resulting DataFrame. This has a major
4846             influence on which operations are efficient on the resulting
4847             dataframe.
4848 
4849             If provided, must include all dimensions of this dataset. By
4850             default, dimensions are sorted alphabetically.
4851 
4852         Returns
4853         -------
4854         result
4855             Dataset as a pandas DataFrame.
4856 
4857         """
4858 
4859         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
4860 
4861         return self._to_dataframe(ordered_dims=ordered_dims)
4862 
4863     def _set_sparse_data_from_dataframe(
4864         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
4865     ) -> None:
4866         from sparse import COO
4867 
4868         if isinstance(idx, pd.MultiIndex):
4869             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
4870             is_sorted = idx.is_lexsorted()
4871             shape = tuple(lev.size for lev in idx.levels)
4872         else:
4873             coords = np.arange(idx.size).reshape(1, -1)
4874             is_sorted = True
4875             shape = (idx.size,)
4876 
4877         for name, values in arrays:
4878             # In virtually all real use cases, the sparse array will now have
4879             # missing values and needs a fill_value. For consistency, don't
4880             # special case the rare exceptions (e.g., dtype=int without a
4881             # MultiIndex).
4882             dtype, fill_value = dtypes.maybe_promote(values.dtype)
4883             values = np.asarray(values, dtype=dtype)
4884 
4885             data = COO(
4886                 coords,
4887                 values,
4888                 shape,
4889                 has_duplicates=False,
4890                 sorted=is_sorted,
4891                 fill_value=fill_value,
4892             )
4893             self[name] = (dims, data)
4894 
4895     def _set_numpy_data_from_dataframe(
4896         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
4897     ) -> None:
4898         if not isinstance(idx, pd.MultiIndex):
4899             for name, values in arrays:
4900                 self[name] = (dims, values)
4901             return
4902 
4903         shape = tuple(lev.size for lev in idx.levels)
4904         indexer = tuple(idx.codes)
4905 
4906         # We already verified that the MultiIndex has all unique values, so
4907         # there are missing values if and only if the size of output arrays is
4908         # larger that the index.
4909         missing_values = np.prod(shape) > idx.shape[0]
4910 
4911         for name, values in arrays:
4912             # NumPy indexing is much faster than using DataFrame.reindex() to
4913             # fill in missing values:
4914             # https://stackoverflow.com/a/35049899/809705
4915             if missing_values:
4916                 dtype, fill_value = dtypes.maybe_promote(values.dtype)
4917                 data = np.full(shape, fill_value, dtype)
4918             else:
4919                 # If there are no missing values, keep the existing dtype
4920                 # instead of promoting to support NA, e.g., keep integer
4921                 # columns as integers.
4922                 # TODO: consider removing this special case, which doesn't
4923                 # exist for sparse=True.
4924                 data = np.zeros(shape, values.dtype)
4925             data[indexer] = values
4926             self[name] = (dims, data)
4927 
4928     @classmethod
4929     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
4930         """Convert a pandas.DataFrame into an xarray.Dataset
4931 
4932         Each column will be converted into an independent variable in the
4933         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
4934         into a tensor product of one-dimensional indices (filling in missing
4935         values with NaN). This method will produce a Dataset very similar to
4936         that on which the 'to_dataframe' method was called, except with
4937         possibly redundant dimensions (since all dataset variables will have
4938         the same dimensionality)
4939 
4940         Parameters
4941         ----------
4942         dataframe : DataFrame
4943             DataFrame from which to copy data and indices.
4944         sparse : bool, default: False
4945             If true, create a sparse arrays instead of dense numpy arrays. This
4946             can potentially save a large amount of memory if the DataFrame has
4947             a MultiIndex. Requires the sparse package (sparse.pydata.org).
4948 
4949         Returns
4950         -------
4951         New Dataset.
4952 
4953         See also
4954         --------
4955         xarray.DataArray.from_series
4956         pandas.DataFrame.to_xarray
4957         """
4958         # TODO: Add an option to remove dimensions along which the variables
4959         # are constant, to enable consistent serialization to/from a dataframe,
4960         # even if some variables have different dimensionality.
4961 
4962         if not dataframe.columns.is_unique:
4963             raise ValueError("cannot convert DataFrame with non-unique columns")
4964 
4965         idx = remove_unused_levels_categories(dataframe.index)
4966 
4967         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
4968             raise ValueError(
4969                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
4970             )
4971 
4972         # Cast to a NumPy array first, in case the Series is a pandas Extension
4973         # array (which doesn't have a valid NumPy dtype)
4974         # TODO: allow users to control how this casting happens, e.g., by
4975         # forwarding arguments to pandas.Series.to_numpy?
4976         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
4977 
4978         obj = cls()
4979 
4980         if isinstance(idx, pd.MultiIndex):
4981             dims = tuple(
4982                 name if name is not None else "level_%i" % n
4983                 for n, name in enumerate(idx.names)
4984             )
4985             for dim, lev in zip(dims, idx.levels):
4986                 obj[dim] = (dim, lev)
4987         else:
4988             index_name = idx.name if idx.name is not None else "index"
4989             dims = (index_name,)
4990             obj[index_name] = (dims, idx)
4991 
4992         if sparse:
4993             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
4994         else:
4995             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
4996         return obj
4997 
4998     def to_dask_dataframe(self, dim_order=None, set_index=False):
4999         """
5000         Convert this dataset into a dask.dataframe.DataFrame.
5001 
5002         The dimensions, coordinates and data variables in this dataset form
5003         the columns of the DataFrame.
5004 
5005         Parameters
5006         ----------
5007         dim_order : list, optional
5008             Hierarchical dimension order for the resulting dataframe. All
5009             arrays are transposed to this order and then written out as flat
5010             vectors in contiguous order, so the last dimension in this list
5011             will be contiguous in the resulting DataFrame. This has a major
5012             influence on which operations are efficient on the resulting dask
5013             dataframe.
5014 
5015             If provided, must include all dimensions of this dataset. By
5016             default, dimensions are sorted alphabetically.
5017         set_index : bool, optional
5018             If set_index=True, the dask DataFrame is indexed by this dataset's
5019             coordinate. Since dask DataFrames do not support multi-indexes,
5020             set_index only works if the dataset only contains one dimension.
5021 
5022         Returns
5023         -------
5024         dask.dataframe.DataFrame
5025         """
5026 
5027         import dask.array as da
5028         import dask.dataframe as dd
5029 
5030         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
5031 
5032         columns = list(ordered_dims)
5033         columns.extend(k for k in self.coords if k not in self.dims)
5034         columns.extend(self.data_vars)
5035 
5036         series_list = []
5037         for name in columns:
5038             try:
5039                 var = self.variables[name]
5040             except KeyError:
5041                 # dimension without a matching coordinate
5042                 size = self.dims[name]
5043                 data = da.arange(size, chunks=size, dtype=np.int64)
5044                 var = Variable((name,), data)
5045 
5046             # IndexVariable objects have a dummy .chunk() method
5047             if isinstance(var, IndexVariable):
5048                 var = var.to_base_variable()
5049 
5050             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
5051             series = dd.from_array(dask_array.reshape(-1), columns=[name])
5052             series_list.append(series)
5053 
5054         df = dd.concat(series_list, axis=1)
5055 
5056         if set_index:
5057             dim_order = [*ordered_dims]
5058 
5059             if len(dim_order) == 1:
5060                 (dim,) = dim_order
5061                 df = df.set_index(dim)
5062             else:
5063                 # triggers an error about multi-indexes, even if only one
5064                 # dimension is passed
5065                 df = df.set_index(dim_order)
5066 
5067         return df
5068 
5069     def to_dict(self, data=True):
5070         """
5071         Convert this dataset to a dictionary following xarray naming
5072         conventions.
5073 
5074         Converts all variables and attributes to native Python objects
5075         Useful for converting to json. To avoid datetime incompatibility
5076         use decode_times=False kwarg in xarrray.open_dataset.
5077 
5078         Parameters
5079         ----------
5080         data : bool, optional
5081             Whether to include the actual data in the dictionary. When set to
5082             False, returns just the schema.
5083 
5084         See also
5085         --------
5086         Dataset.from_dict
5087         """
5088         d = {
5089             "coords": {},
5090             "attrs": decode_numpy_dict_values(self.attrs),
5091             "dims": dict(self.dims),
5092             "data_vars": {},
5093         }
5094         for k in self.coords:
5095             d["coords"].update({k: self[k].variable.to_dict(data=data)})
5096         for k in self.data_vars:
5097             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
5098         return d
5099 
5100     @classmethod
5101     def from_dict(cls, d):
5102         """
5103         Convert a dictionary into an xarray.Dataset.
5104 
5105         Input dict can take several forms:
5106 
5107         .. code:: python
5108 
5109             d = {
5110                 "t": {"dims": ("t"), "data": t},
5111                 "a": {"dims": ("t"), "data": x},
5112                 "b": {"dims": ("t"), "data": y},
5113             }
5114 
5115             d = {
5116                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
5117                 "attrs": {"title": "air temperature"},
5118                 "dims": "t",
5119                 "data_vars": {
5120                     "a": {"dims": "t", "data": x},
5121                     "b": {"dims": "t", "data": y},
5122                 },
5123             }
5124 
5125         where "t" is the name of the dimesion, "a" and "b" are names of data
5126         variables and t, x, and y are lists, numpy.arrays or pandas objects.
5127 
5128         Parameters
5129         ----------
5130         d : dict-like
5131             Mapping with a minimum structure of
5132                 ``{"var_0": {"dims": [..], "data": [..]}, \
5133                             ...}``
5134 
5135         Returns
5136         -------
5137         obj : xarray.Dataset
5138 
5139         See also
5140         --------
5141         Dataset.to_dict
5142         DataArray.from_dict
5143         """
5144 
5145         if not {"coords", "data_vars"}.issubset(set(d)):
5146             variables = d.items()
5147         else:
5148             import itertools
5149 
5150             variables = itertools.chain(
5151                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
5152             )
5153         try:
5154             variable_dict = {
5155                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
5156             }
5157         except KeyError as e:
5158             raise ValueError(
5159                 "cannot convert dict without the key "
5160                 "'{dims_data}'".format(dims_data=str(e.args[0]))
5161             )
5162         obj = cls(variable_dict)
5163 
5164         # what if coords aren't dims?
5165         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
5166         obj = obj.set_coords(coords)
5167 
5168         obj.attrs.update(d.get("attrs", {}))
5169 
5170         return obj
5171 
5172     @staticmethod
5173     def _unary_op(f):
5174         @functools.wraps(f)
5175         def func(self, *args, **kwargs):
5176             variables = {}
5177             keep_attrs = kwargs.pop("keep_attrs", None)
5178             if keep_attrs is None:
5179                 keep_attrs = _get_keep_attrs(default=True)
5180             for k, v in self._variables.items():
5181                 if k in self._coord_names:
5182                     variables[k] = v
5183                 else:
5184                     variables[k] = f(v, *args, **kwargs)
5185                     if keep_attrs:
5186                         variables[k].attrs = v._attrs
5187             attrs = self._attrs if keep_attrs else None
5188             return self._replace_with_new_dims(variables, attrs=attrs)
5189 
5190         return func
5191 
5192     @staticmethod
5193     def _binary_op(f, reflexive=False, join=None):
5194         @functools.wraps(f)
5195         def func(self, other):
5196             from .dataarray import DataArray
5197 
5198             if isinstance(other, groupby.GroupBy):
5199                 return NotImplemented
5200             align_type = OPTIONS["arithmetic_join"] if join is None else join
5201             if isinstance(other, (DataArray, Dataset)):
5202                 self, other = align(self, other, join=align_type, copy=False)
5203             g = f if not reflexive else lambda x, y: f(y, x)
5204             ds = self._calculate_binary_op(g, other, join=align_type)
5205             return ds
5206 
5207         return func
5208 
5209     @staticmethod
5210     def _inplace_binary_op(f):
5211         @functools.wraps(f)
5212         def func(self, other):
5213             from .dataarray import DataArray
5214 
5215             if isinstance(other, groupby.GroupBy):
5216                 raise TypeError(
5217                     "in-place operations between a Dataset and "
5218                     "a grouped object are not permitted"
5219                 )
5220             # we don't actually modify arrays in-place with in-place Dataset
5221             # arithmetic -- this lets us automatically align things
5222             if isinstance(other, (DataArray, Dataset)):
5223                 other = other.reindex_like(self, copy=False)
5224             g = ops.inplace_to_noninplace_op(f)
5225             ds = self._calculate_binary_op(g, other, inplace=True)
5226             self._replace_with_new_dims(
5227                 ds._variables,
5228                 ds._coord_names,
5229                 attrs=ds._attrs,
5230                 indexes=ds._indexes,
5231                 inplace=True,
5232             )
5233             return self
5234 
5235         return func
5236 
5237     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
5238         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
5239             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
5240                 raise ValueError(
5241                     "datasets must have the same data variables "
5242                     "for in-place arithmetic operations: %s, %s"
5243                     % (list(lhs_data_vars), list(rhs_data_vars))
5244                 )
5245 
5246             dest_vars = {}
5247 
5248             for k in lhs_data_vars:
5249                 if k in rhs_data_vars:
5250                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
5251                 elif join in ["left", "outer"]:
5252                     dest_vars[k] = f(lhs_vars[k], np.nan)
5253             for k in rhs_data_vars:
5254                 if k not in dest_vars and join in ["right", "outer"]:
5255                     dest_vars[k] = f(rhs_vars[k], np.nan)
5256             return dest_vars
5257 
5258         if utils.is_dict_like(other) and not isinstance(other, Dataset):
5259             # can't use our shortcut of doing the binary operation with
5260             # Variable objects, so apply over our data vars instead.
5261             new_data_vars = apply_over_both(
5262                 self.data_vars, other, self.data_vars, other
5263             )
5264             return Dataset(new_data_vars)
5265 
5266         other_coords = getattr(other, "coords", None)
5267         ds = self.coords.merge(other_coords)
5268 
5269         if isinstance(other, Dataset):
5270             new_vars = apply_over_both(
5271                 self.data_vars, other.data_vars, self.variables, other.variables
5272             )
5273         else:
5274             other_variable = getattr(other, "variable", other)
5275             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
5276         ds._variables.update(new_vars)
5277         ds._dims = calculate_dimensions(ds._variables)
5278         return ds
5279 
5280     def _copy_attrs_from(self, other):
5281         self.attrs = other.attrs
5282         for v in other.variables:
5283             if v in self.variables:
5284                 self.variables[v].attrs = other.variables[v].attrs
5285 
5286     def diff(self, dim, n=1, label="upper"):
5287         """Calculate the n-th order discrete difference along given axis.
5288 
5289         Parameters
5290         ----------
5291         dim : str
5292             Dimension over which to calculate the finite difference.
5293         n : int, optional
5294             The number of times values are differenced.
5295         label : str, optional
5296             The new coordinate in dimension ``dim`` will have the
5297             values of either the minuend's or subtrahend's coordinate
5298             for values 'upper' and 'lower', respectively.  Other
5299             values are not supported.
5300 
5301         Returns
5302         -------
5303         difference : same type as caller
5304             The n-th order finite difference of this object.
5305 
5306         .. note::
5307 
5308             `n` matches numpy's behavior and is different from pandas' first
5309             argument named `periods`.
5310 
5311         Examples
5312         --------
5313         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
5314         >>> ds.diff("x")
5315         <xarray.Dataset>
5316         Dimensions:  (x: 3)
5317         Dimensions without coordinates: x
5318         Data variables:
5319             foo      (x) int64 0 1 0
5320         >>> ds.diff("x", 2)
5321         <xarray.Dataset>
5322         Dimensions:  (x: 2)
5323         Dimensions without coordinates: x
5324         Data variables:
5325             foo      (x) int64 1 -1
5326 
5327         See Also
5328         --------
5329         Dataset.differentiate
5330         """
5331         if n == 0:
5332             return self
5333         if n < 0:
5334             raise ValueError(f"order `n` must be non-negative but got {n}")
5335 
5336         # prepare slices
5337         kwargs_start = {dim: slice(None, -1)}
5338         kwargs_end = {dim: slice(1, None)}
5339 
5340         # prepare new coordinate
5341         if label == "upper":
5342             kwargs_new = kwargs_end
5343         elif label == "lower":
5344             kwargs_new = kwargs_start
5345         else:
5346             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
5347 
5348         variables = {}
5349 
5350         for name, var in self.variables.items():
5351             if dim in var.dims:
5352                 if name in self.data_vars:
5353                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
5354                 else:
5355                     variables[name] = var.isel(**kwargs_new)
5356             else:
5357                 variables[name] = var
5358 
5359         indexes = dict(self.indexes)
5360         if dim in indexes:
5361             indexes[dim] = indexes[dim][kwargs_new[dim]]
5362 
5363         difference = self._replace_with_new_dims(variables, indexes=indexes)
5364 
5365         if n > 1:
5366             return difference.diff(dim, n - 1)
5367         else:
5368             return difference
5369 
5370     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
5371         """Shift this dataset by an offset along one or more dimensions.
5372 
5373         Only data variables are moved; coordinates stay in place. This is
5374         consistent with the behavior of ``shift`` in pandas.
5375 
5376         Parameters
5377         ----------
5378         shifts : mapping of hashable to int
5379             Integer offset to shift along each of the given dimensions.
5380             Positive offsets shift to the right; negative offsets shift to the
5381             left.
5382         fill_value : scalar or dict-like, optional
5383             Value to use for newly missing values. If a dict-like, maps
5384             variable names (including coordinates) to fill values.
5385         **shifts_kwargs
5386             The keyword arguments form of ``shifts``.
5387             One of shifts or shifts_kwargs must be provided.
5388 
5389         Returns
5390         -------
5391         shifted : Dataset
5392             Dataset with the same coordinates and attributes but shifted data
5393             variables.
5394 
5395         See also
5396         --------
5397         roll
5398 
5399         Examples
5400         --------
5401 
5402         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5403         >>> ds.shift(x=2)
5404         <xarray.Dataset>
5405         Dimensions:  (x: 5)
5406         Dimensions without coordinates: x
5407         Data variables:
5408             foo      (x) object nan nan 'a' 'b' 'c'
5409         """
5410         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5411         invalid = [k for k in shifts if k not in self.dims]
5412         if invalid:
5413             raise ValueError("dimensions %r do not exist" % invalid)
5414 
5415         variables = {}
5416         for name, var in self.variables.items():
5417             if name in self.data_vars:
5418                 fill_value_ = (
5419                     fill_value.get(name, dtypes.NA)
5420                     if isinstance(fill_value, dict)
5421                     else fill_value
5422                 )
5423 
5424                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5425                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
5426             else:
5427                 variables[name] = var
5428 
5429         return self._replace(variables)
5430 
5431     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5432         """Roll this dataset by an offset along one or more dimensions.
5433 
5434         Unlike shift, roll may rotate all variables, including coordinates
5435         if specified. The direction of rotation is consistent with
5436         :py:func:`numpy.roll`.
5437 
5438         Parameters
5439         ----------
5440 
5441         shifts : dict, optional
5442             A dict with keys matching dimensions and values given
5443             by integers to rotate each of the given dimensions. Positive
5444             offsets roll to the right; negative offsets roll to the left.
5445         roll_coords : bool
5446             Indicates whether to  roll the coordinates by the offset
5447             The current default of roll_coords (None, equivalent to True) is
5448             deprecated and will change to False in a future version.
5449             Explicitly pass roll_coords to silence the warning.
5450         **shifts_kwargs : {dim: offset, ...}, optional
5451             The keyword arguments form of ``shifts``.
5452             One of shifts or shifts_kwargs must be provided.
5453         Returns
5454         -------
5455         rolled : Dataset
5456             Dataset with the same coordinates and attributes but rolled
5457             variables.
5458 
5459         See also
5460         --------
5461         shift
5462 
5463         Examples
5464         --------
5465 
5466         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5467         >>> ds.roll(x=2)
5468         <xarray.Dataset>
5469         Dimensions:  (x: 5)
5470         Dimensions without coordinates: x
5471         Data variables:
5472             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
5473         """
5474         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5475         invalid = [k for k in shifts if k not in self.dims]
5476         if invalid:
5477             raise ValueError("dimensions %r do not exist" % invalid)
5478 
5479         if roll_coords is None:
5480             warnings.warn(
5481                 "roll_coords will be set to False in the future."
5482                 " Explicitly set roll_coords to silence warning.",
5483                 FutureWarning,
5484                 stacklevel=2,
5485             )
5486             roll_coords = True
5487 
5488         unrolled_vars = () if roll_coords else self.coords
5489 
5490         variables = {}
5491         for k, v in self.variables.items():
5492             if k not in unrolled_vars:
5493                 variables[k] = v.roll(
5494                     **{k: s for k, s in shifts.items() if k in v.dims}
5495                 )
5496             else:
5497                 variables[k] = v
5498 
5499         if roll_coords:
5500             indexes = {}
5501             for k, v in self.indexes.items():
5502                 (dim,) = self.variables[k].dims
5503                 if dim in shifts:
5504                     indexes[k] = roll_index(v, shifts[dim])
5505                 else:
5506                     indexes[k] = v
5507         else:
5508             indexes = dict(self.indexes)
5509 
5510         return self._replace(variables, indexes=indexes)
5511 
5512     def sortby(self, variables, ascending=True):
5513         """
5514         Sort object by labels or values (along an axis).
5515 
5516         Sorts the dataset, either along specified dimensions,
5517         or according to values of 1-D dataarrays that share dimension
5518         with calling object.
5519 
5520         If the input variables are dataarrays, then the dataarrays are aligned
5521         (via left-join) to the calling object prior to sorting by cell values.
5522         NaNs are sorted to the end, following Numpy convention.
5523 
5524         If multiple sorts along the same dimension is
5525         given, numpy's lexsort is performed along that dimension:
5526         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5527         and the FIRST key in the sequence is used as the primary sort key,
5528         followed by the 2nd key, etc.
5529 
5530         Parameters
5531         ----------
5532         variables: str, DataArray, or list of str or DataArray
5533             1D DataArray objects or name(s) of 1D variable(s) in
5534             coords/data_vars whose values are used to sort the dataset.
5535         ascending: bool, optional
5536             Whether to sort by ascending or descending order.
5537 
5538         Returns
5539         -------
5540         sorted : Dataset
5541             A new dataset where all the specified dims are sorted by dim
5542             labels.
5543         """
5544         from .dataarray import DataArray
5545 
5546         if not isinstance(variables, list):
5547             variables = [variables]
5548         else:
5549             variables = variables
5550         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5551         aligned_vars = align(self, *variables, join="left")
5552         aligned_self = aligned_vars[0]
5553         aligned_other_vars = aligned_vars[1:]
5554         vars_by_dim = defaultdict(list)
5555         for data_array in aligned_other_vars:
5556             if data_array.ndim != 1:
5557                 raise ValueError("Input DataArray is not 1-D.")
5558             (key,) = data_array.dims
5559             vars_by_dim[key].append(data_array)
5560 
5561         indices = {}
5562         for key, arrays in vars_by_dim.items():
5563             order = np.lexsort(tuple(reversed(arrays)))
5564             indices[key] = order if ascending else order[::-1]
5565         return aligned_self.isel(**indices)
5566 
5567     def quantile(
5568         self,
5569         q,
5570         dim=None,
5571         interpolation="linear",
5572         numeric_only=False,
5573         keep_attrs=None,
5574         skipna=True,
5575     ):
5576         """Compute the qth quantile of the data along the specified dimension.
5577 
5578         Returns the qth quantiles(s) of the array elements for each variable
5579         in the Dataset.
5580 
5581         Parameters
5582         ----------
5583         q : float or array-like of float
5584             Quantile to compute, which must be between 0 and 1 inclusive.
5585         dim : str or sequence of str, optional
5586             Dimension(s) over which to apply quantile.
5587         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
5588             This optional parameter specifies the interpolation method to
5589             use when the desired quantile lies between two data points
5590             ``i < j``:
5591 
5592                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5593                   the fractional part of the index surrounded by ``i`` and
5594                   ``j``.
5595                 * lower: ``i``.
5596                 * higher: ``j``.
5597                 * nearest: ``i`` or ``j``, whichever is nearest.
5598                 * midpoint: ``(i + j) / 2``.
5599         keep_attrs : bool, optional
5600             If True, the dataset's attributes (`attrs`) will be copied from
5601             the original object to the new one.  If False (default), the new
5602             object will be returned without attributes.
5603         numeric_only : bool, optional
5604             If True, only apply ``func`` to variables with a numeric dtype.
5605         skipna : bool, optional
5606             Whether to skip missing values when aggregating.
5607 
5608         Returns
5609         -------
5610         quantiles : Dataset
5611             If `q` is a single quantile, then the result is a scalar for each
5612             variable in data_vars. If multiple percentiles are given, first
5613             axis of the result corresponds to the quantile and a quantile
5614             dimension is added to the return Dataset. The other dimensions are
5615             the dimensions that remain after the reduction of the array.
5616 
5617         See Also
5618         --------
5619         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5620 
5621         Examples
5622         --------
5623 
5624         >>> ds = xr.Dataset(
5625         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5626         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5627         ... )
5628         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5629         <xarray.Dataset>
5630         Dimensions:   ()
5631         Coordinates:
5632             quantile  float64 0.0
5633         Data variables:
5634             a         float64 0.7
5635         >>> ds.quantile(0, dim="x")
5636         <xarray.Dataset>
5637         Dimensions:   (y: 4)
5638         Coordinates:
5639           * y         (y) float64 1.0 1.5 2.0 2.5
5640             quantile  float64 0.0
5641         Data variables:
5642             a         (y) float64 0.7 4.2 2.6 1.5
5643         >>> ds.quantile([0, 0.5, 1])
5644         <xarray.Dataset>
5645         Dimensions:   (quantile: 3)
5646         Coordinates:
5647           * quantile  (quantile) float64 0.0 0.5 1.0
5648         Data variables:
5649             a         (quantile) float64 0.7 3.4 9.4
5650         >>> ds.quantile([0, 0.5, 1], dim="x")
5651         <xarray.Dataset>
5652         Dimensions:   (quantile: 3, y: 4)
5653         Coordinates:
5654           * y         (y) float64 1.0 1.5 2.0 2.5
5655           * quantile  (quantile) float64 0.0 0.5 1.0
5656         Data variables:
5657             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5658         """
5659 
5660         if isinstance(dim, str):
5661             dims = {dim}
5662         elif dim in [None, ...]:
5663             dims = set(self.dims)
5664         else:
5665             dims = set(dim)
5666 
5667         _assert_empty(
5668             [d for d in dims if d not in self.dims],
5669             "Dataset does not contain the dimensions: %s",
5670         )
5671 
5672         q = np.asarray(q, dtype=np.float64)
5673 
5674         variables = {}
5675         for name, var in self.variables.items():
5676             reduce_dims = [d for d in var.dims if d in dims]
5677             if reduce_dims or not var.dims:
5678                 if name not in self.coords:
5679                     if (
5680                         not numeric_only
5681                         or np.issubdtype(var.dtype, np.number)
5682                         or var.dtype == np.bool_
5683                     ):
5684                         if len(reduce_dims) == var.ndim:
5685                             # prefer to aggregate over axis=None rather than
5686                             # axis=(0, 1) if they will be equivalent, because
5687                             # the former is often more efficient
5688                             reduce_dims = None
5689                         variables[name] = var.quantile(
5690                             q,
5691                             dim=reduce_dims,
5692                             interpolation=interpolation,
5693                             keep_attrs=keep_attrs,
5694                             skipna=skipna,
5695                         )
5696 
5697             else:
5698                 variables[name] = var
5699 
5700         # construct the new dataset
5701         coord_names = {k for k in self.coords if k in variables}
5702         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5703         if keep_attrs is None:
5704             keep_attrs = _get_keep_attrs(default=False)
5705         attrs = self.attrs if keep_attrs else None
5706         new = self._replace_with_new_dims(
5707             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5708         )
5709         return new.assign_coords(quantile=q)
5710 
5711     def rank(self, dim, pct=False, keep_attrs=None):
5712         """Ranks the data.
5713 
5714         Equal values are assigned a rank that is the average of the ranks that
5715         would have been otherwise assigned to all of the values within
5716         that set.
5717         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5718 
5719         NaNs in the input array are returned as NaNs.
5720 
5721         The `bottleneck` library is required.
5722 
5723         Parameters
5724         ----------
5725         dim : str
5726             Dimension over which to compute rank.
5727         pct : bool, optional
5728             If True, compute percentage ranks, otherwise compute integer ranks.
5729         keep_attrs : bool, optional
5730             If True, the dataset's attributes (`attrs`) will be copied from
5731             the original object to the new one.  If False (default), the new
5732             object will be returned without attributes.
5733 
5734         Returns
5735         -------
5736         ranked : Dataset
5737             Variables that do not depend on `dim` are dropped.
5738         """
5739         if dim not in self.dims:
5740             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5741 
5742         variables = {}
5743         for name, var in self.variables.items():
5744             if name in self.data_vars:
5745                 if dim in var.dims:
5746                     variables[name] = var.rank(dim, pct=pct)
5747             else:
5748                 variables[name] = var
5749 
5750         coord_names = set(self.coords)
5751         if keep_attrs is None:
5752             keep_attrs = _get_keep_attrs(default=False)
5753         attrs = self.attrs if keep_attrs else None
5754         return self._replace(variables, coord_names, attrs=attrs)
5755 
5756     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5757         """ Differentiate with the second order accurate central
5758         differences.
5759 
5760         .. note::
5761             This feature is limited to simple cartesian geometry, i.e. coord
5762             must be one dimensional.
5763 
5764         Parameters
5765         ----------
5766         coord : str
5767             The coordinate to be used to compute the gradient.
5768         edge_order : {1, 2}, default: 1
5769             N-th order accurate differences at the boundaries.
5770         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5771             "us", "ns", "ps", "fs", "as"}, default: None
5772             Unit to compute gradient. Only valid for datetime coordinate.
5773 
5774         Returns
5775         -------
5776         differentiated: Dataset
5777 
5778         See also
5779         --------
5780         numpy.gradient: corresponding numpy function
5781         """
5782         from .variable import Variable
5783 
5784         if coord not in self.variables and coord not in self.dims:
5785             raise ValueError(f"Coordinate {coord} does not exist.")
5786 
5787         coord_var = self[coord].variable
5788         if coord_var.ndim != 1:
5789             raise ValueError(
5790                 "Coordinate {} must be 1 dimensional but is {}"
5791                 " dimensional".format(coord, coord_var.ndim)
5792             )
5793 
5794         dim = coord_var.dims[0]
5795         if _contains_datetime_like_objects(coord_var):
5796             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5797                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5798             elif datetime_unit is None:
5799                 datetime_unit = "s"  # Default to seconds for cftime objects
5800             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5801 
5802         variables = {}
5803         for k, v in self.variables.items():
5804             if k in self.data_vars and dim in v.dims and k not in self.coords:
5805                 if _contains_datetime_like_objects(v):
5806                     v = v._to_numeric(datetime_unit=datetime_unit)
5807                 grad = duck_array_ops.gradient(
5808                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5809                 )
5810                 variables[k] = Variable(v.dims, grad)
5811             else:
5812                 variables[k] = v
5813         return self._replace(variables)
5814 
5815     def integrate(self, coord, datetime_unit=None):
5816         """ integrate the array with the trapezoidal rule.
5817 
5818         .. note::
5819             This feature is limited to simple cartesian geometry, i.e. coord
5820             must be one dimensional.
5821 
5822         Parameters
5823         ----------
5824         coord: str, or sequence of str
5825             Coordinate(s) used for the integration.
5826         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", "us", "ns", \
5827                          "ps", "fs", "as"}, optional
5828             Can be specify the unit if datetime coordinate is used.
5829 
5830         Returns
5831         -------
5832         integrated : Dataset
5833 
5834         See also
5835         --------
5836         DataArray.integrate
5837         numpy.trapz: corresponding numpy function
5838 
5839         Examples
5840         --------
5841         >>> ds = xr.Dataset(
5842         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5843         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5844         ... )
5845         >>> ds
5846         <xarray.Dataset>
5847         Dimensions:  (x: 4)
5848         Coordinates:
5849           * x        (x) int64 0 1 2 3
5850             y        (x) int64 1 7 3 5
5851         Data variables:
5852             a        (x) int64 5 5 6 6
5853             b        (x) int64 1 2 1 0
5854         >>> ds.integrate("x")
5855         <xarray.Dataset>
5856         Dimensions:  ()
5857         Data variables:
5858             a        float64 16.5
5859             b        float64 3.5
5860         >>> ds.integrate("y")
5861         <xarray.Dataset>
5862         Dimensions:  ()
5863         Data variables:
5864             a        float64 20.0
5865             b        float64 4.0
5866         """
5867         if not isinstance(coord, (list, tuple)):
5868             coord = (coord,)
5869         result = self
5870         for c in coord:
5871             result = result._integrate_one(c, datetime_unit=datetime_unit)
5872         return result
5873 
5874     def _integrate_one(self, coord, datetime_unit=None):
5875         from .variable import Variable
5876 
5877         if coord not in self.variables and coord not in self.dims:
5878             raise ValueError(f"Coordinate {coord} does not exist.")
5879 
5880         coord_var = self[coord].variable
5881         if coord_var.ndim != 1:
5882             raise ValueError(
5883                 "Coordinate {} must be 1 dimensional but is {}"
5884                 " dimensional".format(coord, coord_var.ndim)
5885             )
5886 
5887         dim = coord_var.dims[0]
5888         if _contains_datetime_like_objects(coord_var):
5889             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5890                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5891             elif datetime_unit is None:
5892                 datetime_unit = "s"  # Default to seconds for cftime objects
5893             coord_var = coord_var._replace(
5894                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
5895             )
5896 
5897         variables = {}
5898         coord_names = set()
5899         for k, v in self.variables.items():
5900             if k in self.coords:
5901                 if dim not in v.dims:
5902                     variables[k] = v
5903                     coord_names.add(k)
5904             else:
5905                 if k in self.data_vars and dim in v.dims:
5906                     if _contains_datetime_like_objects(v):
5907                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
5908                     integ = duck_array_ops.trapz(
5909                         v.data, coord_var.data, axis=v.get_axis_num(dim)
5910                     )
5911                     v_dims = list(v.dims)
5912                     v_dims.remove(dim)
5913                     variables[k] = Variable(v_dims, integ)
5914                 else:
5915                     variables[k] = v
5916         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5917         return self._replace_with_new_dims(
5918             variables, coord_names=coord_names, indexes=indexes
5919         )
5920 
5921     @property
5922     def real(self):
5923         return self.map(lambda x: x.real, keep_attrs=True)
5924 
5925     @property
5926     def imag(self):
5927         return self.map(lambda x: x.imag, keep_attrs=True)
5928 
5929     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
5930 
5931     def filter_by_attrs(self, **kwargs):
5932         """Returns a ``Dataset`` with variables that match specific conditions.
5933 
5934         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
5935         containing only the variables for which all the filter tests pass.
5936         These tests are either ``key=value`` for which the attribute ``key``
5937         has the exact value ``value`` or the callable passed into
5938         ``key=callable`` returns True. The callable will be passed a single
5939         value, either the value of the attribute ``key`` or ``None`` if the
5940         DataArray does not have an attribute with the name ``key``.
5941 
5942         Parameters
5943         ----------
5944         **kwargs
5945             key : str
5946                 Attribute name.
5947             value : callable or obj
5948                 If value is a callable, it should return a boolean in the form
5949                 of bool = func(attr) where attr is da.attrs[key].
5950                 Otherwise, value will be compared to the each
5951                 DataArray's attrs[key].
5952 
5953         Returns
5954         -------
5955         new : Dataset
5956             New dataset with variables filtered by attribute.
5957 
5958         Examples
5959         --------
5960         >>> # Create an example dataset:
5961         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
5962         >>> precip = 10 * np.random.rand(2, 2, 3)
5963         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
5964         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
5965         >>> dims = ["x", "y", "time"]
5966         >>> temp_attr = dict(standard_name="air_potential_temperature")
5967         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
5968         >>> ds = xr.Dataset(
5969         ...     {
5970         ...         "temperature": (dims, temp, temp_attr),
5971         ...         "precipitation": (dims, precip, precip_attr),
5972         ...     },
5973         ...     coords={
5974         ...         "lon": (["x", "y"], lon),
5975         ...         "lat": (["x", "y"], lat),
5976         ...         "time": pd.date_range("2014-09-06", periods=3),
5977         ...         "reference_time": pd.Timestamp("2014-09-05"),
5978         ...     },
5979         ... )
5980         >>> # Get variables matching a specific standard_name.
5981         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
5982         <xarray.Dataset>
5983         Dimensions:         (time: 3, x: 2, y: 2)
5984         Coordinates:
5985             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5986             lat             (x, y) float64 42.25 42.21 42.63 42.59
5987           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5988             reference_time  datetime64[ns] 2014-09-05
5989         Dimensions without coordinates: x, y
5990         Data variables:
5991             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
5992         >>> # Get all variables that have a standard_name attribute.
5993         >>> standard_name = lambda v: v is not None
5994         >>> ds.filter_by_attrs(standard_name=standard_name)
5995         <xarray.Dataset>
5996         Dimensions:         (time: 3, x: 2, y: 2)
5997         Coordinates:
5998             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5999             lat             (x, y) float64 42.25 42.21 42.63 42.59
6000           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
6001             reference_time  datetime64[ns] 2014-09-05
6002         Dimensions without coordinates: x, y
6003         Data variables:
6004             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
6005             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
6006 
6007         """
6008         selection = []
6009         for var_name, variable in self.variables.items():
6010             has_value_flag = False
6011             for attr_name, pattern in kwargs.items():
6012                 attr_value = variable.attrs.get(attr_name)
6013                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
6014                     has_value_flag = True
6015                 else:
6016                     has_value_flag = False
6017                     break
6018             if has_value_flag is True:
6019                 selection.append(var_name)
6020         return self[selection]
6021 
6022     def unify_chunks(self) -> "Dataset":
6023         """Unify chunk size along all chunked dimensions of this Dataset.
6024 
6025         Returns
6026         -------
6027 
6028         Dataset with consistent chunk sizes for all dask-array variables
6029 
6030         See Also
6031         --------
6032 
6033         dask.array.core.unify_chunks
6034         """
6035 
6036         try:
6037             self.chunks
6038         except ValueError:  # "inconsistent chunks"
6039             pass
6040         else:
6041             # No variables with dask backend, or all chunks are already aligned
6042             return self.copy()
6043 
6044         # import dask is placed after the quick exit test above to allow
6045         # running this method if dask isn't installed and there are no chunks
6046         import dask.array
6047 
6048         ds = self.copy()
6049 
6050         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
6051 
6052         dask_array_names = []
6053         dask_unify_args = []
6054         for name, variable in ds.variables.items():
6055             if isinstance(variable.data, dask.array.Array):
6056                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
6057                 dask_array_names.append(name)
6058                 dask_unify_args.append(variable.data)
6059                 dask_unify_args.append(dims_tuple)
6060 
6061         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
6062 
6063         for name, new_array in zip(dask_array_names, rechunked_arrays):
6064             ds.variables[name]._data = new_array
6065 
6066         return ds
6067 
6068     def map_blocks(
6069         self,
6070         func: "Callable[..., T_DSorDA]",
6071         args: Sequence[Any] = (),
6072         kwargs: Mapping[str, Any] = None,
6073         template: Union["DataArray", "Dataset"] = None,
6074     ) -> "T_DSorDA":
6075         """
6076         Apply a function to each block of this Dataset.
6077 
6078         .. warning::
6079             This method is experimental and its signature may change.
6080 
6081         Parameters
6082         ----------
6083         func : callable
6084             User-provided function that accepts a Dataset as its first
6085             parameter. The function will receive a subset or 'block' of this Dataset (see below),
6086             corresponding to one chunk along each chunked dimension. ``func`` will be
6087             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
6088 
6089             This function must return either a single DataArray or a single Dataset.
6090 
6091             This function cannot add a new chunked dimension.
6092         args : sequence
6093             Passed to func after unpacking and subsetting any xarray objects by blocks.
6094             xarray objects in args must be aligned with obj, otherwise an error is raised.
6095         kwargs : mapping
6096             Passed verbatim to func after unpacking. xarray objects, if any, will not be
6097             subset to blocks. Passing dask collections in kwargs is not allowed.
6098         template : DataArray or Dataset, optional
6099             xarray object representing the final result after compute is called. If not provided,
6100             the function will be first run on mocked-up data, that looks like this object but
6101             has sizes 0, to determine properties of the returned object such as dtype,
6102             variable names, attributes, new dimensions and new indexes (if any).
6103             ``template`` must be provided if the function changes the size of existing dimensions.
6104             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
6105             ``attrs`` set by ``func`` will be ignored.
6106 
6107 
6108         Returns
6109         -------
6110         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
6111         function.
6112 
6113         Notes
6114         -----
6115         This function is designed for when ``func`` needs to manipulate a whole xarray object
6116         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
6117         recommended to use ``apply_ufunc``.
6118 
6119         If none of the variables in this object is backed by dask arrays, calling this function is
6120         equivalent to calling ``func(obj, *args, **kwargs)``.
6121 
6122         See Also
6123         --------
6124         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,
6125         xarray.DataArray.map_blocks
6126 
6127         Examples
6128         --------
6129 
6130         Calculate an anomaly from climatology using ``.groupby()``. Using
6131         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
6132         its indices, and its methods like ``.groupby()``.
6133 
6134         >>> def calculate_anomaly(da, groupby_type="time.month"):
6135         ...     gb = da.groupby(groupby_type)
6136         ...     clim = gb.mean(dim="time")
6137         ...     return gb - clim
6138         ...
6139         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
6140         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
6141         >>> np.random.seed(123)
6142         >>> array = xr.DataArray(
6143         ...     np.random.rand(len(time)),
6144         ...     dims=["time"],
6145         ...     coords={"time": time, "month": month},
6146         ... ).chunk()
6147         >>> ds = xr.Dataset({"a": array})
6148         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
6149         <xarray.Dataset>
6150         Dimensions:  (time: 24)
6151         Coordinates:
6152           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6153             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
6154         Data variables:
6155             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
6156 
6157         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
6158         to the function being applied in ``xr.map_blocks()``:
6159 
6160         >>> ds.map_blocks(
6161         ...     calculate_anomaly,
6162         ...     kwargs={"groupby_type": "time.year"},
6163         ...     template=ds,
6164         ... )
6165         <xarray.Dataset>
6166         Dimensions:  (time: 24)
6167         Coordinates:
6168           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6169             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
6170         Data variables:
6171             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
6172         """
6173         from .parallel import map_blocks
6174 
6175         return map_blocks(func, self, args, kwargs, template)
6176 
6177     def polyfit(
6178         self,
6179         dim: Hashable,
6180         deg: int,
6181         skipna: bool = None,
6182         rcond: float = None,
6183         w: Union[Hashable, Any] = None,
6184         full: bool = False,
6185         cov: Union[bool, str] = False,
6186     ):
6187         """
6188         Least squares polynomial fit.
6189 
6190         This replicates the behaviour of `numpy.polyfit` but differs by skipping
6191         invalid values when `skipna = True`.
6192 
6193         Parameters
6194         ----------
6195         dim : hashable
6196             Coordinate along which to fit the polynomials.
6197         deg : int
6198             Degree of the fitting polynomial.
6199         skipna : bool, optional
6200             If True, removes all invalid values before fitting each 1D slices of the array.
6201             Default is True if data is stored in a dask.array or if there is any
6202             invalid values, False otherwise.
6203         rcond : float, optional
6204             Relative condition number to the fit.
6205         w : hashable or Any, optional
6206             Weights to apply to the y-coordinate of the sample points.
6207             Can be an array-like object or the name of a coordinate in the dataset.
6208         full : bool, optional
6209             Whether to return the residuals, matrix rank and singular values in addition
6210             to the coefficients.
6211         cov : bool or str, optional
6212             Whether to return to the covariance matrix in addition to the coefficients.
6213             The matrix is not scaled if `cov='unscaled'`.
6214 
6215 
6216         Returns
6217         -------
6218         polyfit_results : Dataset
6219             A single dataset which contains (for each "var" in the input dataset):
6220 
6221             [var]_polyfit_coefficients
6222                 The coefficients of the best fit for each variable in this dataset.
6223             [var]_polyfit_residuals
6224                 The residuals of the least-square computation for each variable (only included if `full=True`)
6225                 When the matrix rank is deficient, np.nan is returned.
6226             [dim]_matrix_rank
6227                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6228                 The rank is computed ignoring the NaN values that might be skipped.
6229             [dim]_singular_values
6230                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6231             [var]_polyfit_covariance
6232                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
6233 
6234         Warns
6235         -----
6236         RankWarning
6237             The rank of the coefficient matrix in the least-squares fit is deficient.
6238             The warning is not raised with in-memory (not dask) data and `full=True`.
6239 
6240         See also
6241         --------
6242         numpy.polyfit
6243         """
6244         variables = {}
6245         skipna_da = skipna
6246 
6247         x = get_clean_interp_index(self, dim, strict=False)
6248         xname = "{}_".format(self[dim].name)
6249         order = int(deg) + 1
6250         lhs = np.vander(x, order)
6251 
6252         if rcond is None:
6253             rcond = x.shape[0] * np.core.finfo(x.dtype).eps
6254 
6255         # Weights:
6256         if w is not None:
6257             if isinstance(w, Hashable):
6258                 w = self.coords[w]
6259             w = np.asarray(w)
6260             if w.ndim != 1:
6261                 raise TypeError("Expected a 1-d array for weights.")
6262             if w.shape[0] != lhs.shape[0]:
6263                 raise TypeError("Expected w and {} to have the same length".format(dim))
6264             lhs *= w[:, np.newaxis]
6265 
6266         # Scaling
6267         scale = np.sqrt((lhs * lhs).sum(axis=0))
6268         lhs /= scale
6269 
6270         degree_dim = utils.get_temp_dimname(self.dims, "degree")
6271 
6272         rank = np.linalg.matrix_rank(lhs)
6273 
6274         if full:
6275             rank = xr.DataArray(rank, name=xname + "matrix_rank")
6276             variables[rank.name] = rank
6277             sing = np.linalg.svd(lhs, compute_uv=False)
6278             sing = xr.DataArray(
6279                 sing,
6280                 dims=(degree_dim,),
6281                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
6282                 name=xname + "singular_values",
6283             )
6284             variables[sing.name] = sing
6285 
6286         for name, da in self.data_vars.items():
6287             if dim not in da.dims:
6288                 continue
6289 
6290             if is_duck_dask_array(da.data) and (
6291                 rank != order or full or skipna is None
6292             ):
6293                 # Current algorithm with dask and skipna=False neither supports
6294                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
6295                 skipna_da = True
6296             elif skipna is None:
6297                 skipna_da = np.any(da.isnull())
6298 
6299             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
6300             stacked_coords: Dict[Hashable, DataArray] = {}
6301             if dims_to_stack:
6302                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
6303                 rhs = da.transpose(dim, *dims_to_stack).stack(
6304                     {stacked_dim: dims_to_stack}
6305                 )
6306                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
6307                 scale_da = scale[:, np.newaxis]
6308             else:
6309                 rhs = da
6310                 scale_da = scale
6311 
6312             if w is not None:
6313                 rhs *= w[:, np.newaxis]
6314 
6315             with warnings.catch_warnings():
6316                 if full:  # Copy np.polyfit behavior
6317                     warnings.simplefilter("ignore", np.RankWarning)
6318                 else:  # Raise only once per variable
6319                     warnings.simplefilter("once", np.RankWarning)
6320 
6321                 coeffs, residuals = duck_array_ops.least_squares(
6322                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
6323                 )
6324 
6325             if isinstance(name, str):
6326                 name = "{}_".format(name)
6327             else:
6328                 # Thus a ReprObject => polyfit was called on a DataArray
6329                 name = ""
6330 
6331             coeffs = xr.DataArray(
6332                 coeffs / scale_da,
6333                 dims=[degree_dim] + list(stacked_coords.keys()),
6334                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
6335                 name=name + "polyfit_coefficients",
6336             )
6337             if dims_to_stack:
6338                 coeffs = coeffs.unstack(stacked_dim)
6339             variables[coeffs.name] = coeffs
6340 
6341             if full or (cov is True):
6342                 residuals = xr.DataArray(
6343                     residuals if dims_to_stack else residuals.squeeze(),
6344                     dims=list(stacked_coords.keys()),
6345                     coords=stacked_coords,
6346                     name=name + "polyfit_residuals",
6347                 )
6348                 if dims_to_stack:
6349                     residuals = residuals.unstack(stacked_dim)
6350                 variables[residuals.name] = residuals
6351 
6352             if cov:
6353                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
6354                 Vbase /= np.outer(scale, scale)
6355                 if cov == "unscaled":
6356                     fac = 1
6357                 else:
6358                     if x.shape[0] <= order:
6359                         raise ValueError(
6360                             "The number of data points must exceed order to scale the covariance matrix."
6361                         )
6362                     fac = residuals / (x.shape[0] - order)
6363                 covariance = xr.DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
6364                 variables[name + "polyfit_covariance"] = covariance
6365 
6366         return Dataset(data_vars=variables, attrs=self.attrs.copy())
6367 
6368     def pad(
6369         self,
6370         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
6371         mode: str = "constant",
6372         stat_length: Union[
6373             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6374         ] = None,
6375         constant_values: Union[
6376             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6377         ] = None,
6378         end_values: Union[
6379             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6380         ] = None,
6381         reflect_type: str = None,
6382         **pad_width_kwargs: Any,
6383     ) -> "Dataset":
6384         """Pad this dataset along one or more dimensions.
6385 
6386         .. warning::
6387             This function is experimental and its behaviour is likely to change
6388             especially regarding padding of dimension coordinates (or IndexVariables).
6389 
6390         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
6391         coordinates will be padded with the same mode, otherwise coordinates
6392         are padded using the "constant" mode with fill_value dtypes.NA.
6393 
6394         Parameters
6395         ----------
6396         pad_width : mapping of hashable to tuple of int
6397             Mapping with the form of {dim: (pad_before, pad_after)}
6398             describing the number of values padded along each dimension.
6399             {dim: pad} is a shortcut for pad_before = pad_after = pad
6400         mode : str, default: "constant"
6401             One of the following string values (taken from numpy docs).
6402 
6403             'constant' (default)
6404                 Pads with a constant value.
6405             'edge'
6406                 Pads with the edge values of array.
6407             'linear_ramp'
6408                 Pads with the linear ramp between end_value and the
6409                 array edge value.
6410             'maximum'
6411                 Pads with the maximum value of all or part of the
6412                 vector along each axis.
6413             'mean'
6414                 Pads with the mean value of all or part of the
6415                 vector along each axis.
6416             'median'
6417                 Pads with the median value of all or part of the
6418                 vector along each axis.
6419             'minimum'
6420                 Pads with the minimum value of all or part of the
6421                 vector along each axis.
6422             'reflect'
6423                 Pads with the reflection of the vector mirrored on
6424                 the first and last values of the vector along each
6425                 axis.
6426             'symmetric'
6427                 Pads with the reflection of the vector mirrored
6428                 along the edge of the array.
6429             'wrap'
6430                 Pads with the wrap of the vector along the axis.
6431                 The first values are used to pad the end and the
6432                 end values are used to pad the beginning.
6433         stat_length : int, tuple or mapping of hashable to tuple, default: None
6434             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
6435             values at edge of each axis used to calculate the statistic value.
6436             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
6437             statistic lengths along each dimension.
6438             ((before, after),) yields same before and after statistic lengths
6439             for each dimension.
6440             (stat_length,) or int is a shortcut for before = after = statistic
6441             length for all axes.
6442             Default is ``None``, to use the entire axis.
6443         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
6444             Used in 'constant'.  The values to set the padded values for each
6445             axis.
6446             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6447             pad constants along each dimension.
6448             ``((before, after),)`` yields same before and after constants for each
6449             dimension.
6450             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6451             all dimensions.
6452             Default is 0.
6453         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
6454             Used in 'linear_ramp'.  The values used for the ending value of the
6455             linear_ramp and that will form the edge of the padded array.
6456             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6457             end values along each dimension.
6458             ``((before, after),)`` yields same before and after end values for each
6459             axis.
6460             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6461             all axes.
6462             Default is 0.
6463         reflect_type : {"even", "odd"}, optional
6464             Used in "reflect", and "symmetric".  The "even" style is the
6465             default with an unaltered reflection around the edge value.  For
6466             the "odd" style, the extended part of the array is created by
6467             subtracting the reflected values from two times the edge value.
6468         **pad_width_kwargs
6469             The keyword arguments form of ``pad_width``.
6470             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
6471 
6472         Returns
6473         -------
6474         padded : Dataset
6475             Dataset with the padded coordinates and data.
6476 
6477         See also
6478         --------
6479         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
6480 
6481         Notes
6482         -----
6483         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
6484         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
6485         specify ``constant_values=np.nan``
6486 
6487         Examples
6488         --------
6489 
6490         >>> ds = xr.Dataset({"foo": ("x", range(5))})
6491         >>> ds.pad(x=(1, 2))
6492         <xarray.Dataset>
6493         Dimensions:  (x: 8)
6494         Dimensions without coordinates: x
6495         Data variables:
6496             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
6497         """
6498         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
6499 
6500         if mode in ("edge", "reflect", "symmetric", "wrap"):
6501             coord_pad_mode = mode
6502             coord_pad_options = {
6503                 "stat_length": stat_length,
6504                 "constant_values": constant_values,
6505                 "end_values": end_values,
6506                 "reflect_type": reflect_type,
6507             }
6508         else:
6509             coord_pad_mode = "constant"
6510             coord_pad_options = {}
6511 
6512         variables = {}
6513         for name, var in self.variables.items():
6514             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
6515             if not var_pad_width:
6516                 variables[name] = var
6517             elif name in self.data_vars:
6518                 variables[name] = var.pad(
6519                     pad_width=var_pad_width,
6520                     mode=mode,
6521                     stat_length=stat_length,
6522                     constant_values=constant_values,
6523                     end_values=end_values,
6524                     reflect_type=reflect_type,
6525                 )
6526             else:
6527                 variables[name] = var.pad(
6528                     pad_width=var_pad_width,
6529                     mode=coord_pad_mode,
6530                     **coord_pad_options,  # type: ignore
6531                 )
6532 
6533         return self._replace_vars_and_dims(variables)
6534 
6535     def idxmin(
6536         self,
6537         dim: Hashable = None,
6538         skipna: bool = None,
6539         fill_value: Any = dtypes.NA,
6540         keep_attrs: bool = None,
6541     ) -> "Dataset":
6542         """Return the coordinate label of the minimum value along a dimension.
6543 
6544         Returns a new `Dataset` named after the dimension with the values of
6545         the coordinate labels along that dimension corresponding to minimum
6546         values along that dimension.
6547 
6548         In comparison to :py:meth:`~Dataset.argmin`, this returns the
6549         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
6550 
6551         Parameters
6552         ----------
6553         dim : str, optional
6554             Dimension over which to apply `idxmin`.  This is optional for 1D
6555             variables, but required for variables with 2 or more dimensions.
6556         skipna : bool or None, default: None
6557             If True, skip missing values (as marked by NaN). By default, only
6558             skips missing values for ``float``, ``complex``, and ``object``
6559             dtypes; other dtypes either do not have a sentinel missing value
6560             (``int``) or ``skipna=True`` has not been implemented
6561             (``datetime64`` or ``timedelta64``).
6562         fill_value : Any, default: NaN
6563             Value to be filled in case all of the values along a dimension are
6564             null.  By default this is NaN.  The fill value and result are
6565             automatically converted to a compatible dtype if possible.
6566             Ignored if ``skipna`` is False.
6567         keep_attrs : bool, default: False
6568             If True, the attributes (``attrs``) will be copied from the
6569             original object to the new one.  If False (default), the new object
6570             will be returned without attributes.
6571 
6572         Returns
6573         -------
6574         reduced : Dataset
6575             New `Dataset` object with `idxmin` applied to its data and the
6576             indicated dimension removed.
6577 
6578         See also
6579         --------
6580         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
6581 
6582         Examples
6583         --------
6584 
6585         >>> array1 = xr.DataArray(
6586         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6587         ... )
6588         >>> array2 = xr.DataArray(
6589         ...     [
6590         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6591         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6592         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6593         ...     ],
6594         ...     dims=["y", "x"],
6595         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6596         ... )
6597         >>> ds = xr.Dataset({"int": array1, "float": array2})
6598         >>> ds.min(dim="x")
6599         <xarray.Dataset>
6600         Dimensions:  (y: 3)
6601         Coordinates:
6602           * y        (y) int64 -1 0 1
6603         Data variables:
6604             int      int64 -2
6605             float    (y) float64 -2.0 -4.0 1.0
6606         >>> ds.argmin(dim="x")
6607         <xarray.Dataset>
6608         Dimensions:  (y: 3)
6609         Coordinates:
6610           * y        (y) int64 -1 0 1
6611         Data variables:
6612             int      int64 4
6613             float    (y) int64 4 0 2
6614         >>> ds.idxmin(dim="x")
6615         <xarray.Dataset>
6616         Dimensions:  (y: 3)
6617         Coordinates:
6618           * y        (y) int64 -1 0 1
6619         Data variables:
6620             int      <U1 'e'
6621             float    (y) object 'e' 'a' 'c'
6622         """
6623         return self.map(
6624             methodcaller(
6625                 "idxmin",
6626                 dim=dim,
6627                 skipna=skipna,
6628                 fill_value=fill_value,
6629                 keep_attrs=keep_attrs,
6630             )
6631         )
6632 
6633     def idxmax(
6634         self,
6635         dim: Hashable = None,
6636         skipna: bool = None,
6637         fill_value: Any = dtypes.NA,
6638         keep_attrs: bool = None,
6639     ) -> "Dataset":
6640         """Return the coordinate label of the maximum value along a dimension.
6641 
6642         Returns a new `Dataset` named after the dimension with the values of
6643         the coordinate labels along that dimension corresponding to maximum
6644         values along that dimension.
6645 
6646         In comparison to :py:meth:`~Dataset.argmax`, this returns the
6647         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
6648 
6649         Parameters
6650         ----------
6651         dim : str, optional
6652             Dimension over which to apply `idxmax`.  This is optional for 1D
6653             variables, but required for variables with 2 or more dimensions.
6654         skipna : bool or None, default: None
6655             If True, skip missing values (as marked by NaN). By default, only
6656             skips missing values for ``float``, ``complex``, and ``object``
6657             dtypes; other dtypes either do not have a sentinel missing value
6658             (``int``) or ``skipna=True`` has not been implemented
6659             (``datetime64`` or ``timedelta64``).
6660         fill_value : Any, default: NaN
6661             Value to be filled in case all of the values along a dimension are
6662             null.  By default this is NaN.  The fill value and result are
6663             automatically converted to a compatible dtype if possible.
6664             Ignored if ``skipna`` is False.
6665         keep_attrs : bool, default: False
6666             If True, the attributes (``attrs``) will be copied from the
6667             original object to the new one.  If False (default), the new object
6668             will be returned without attributes.
6669 
6670         Returns
6671         -------
6672         reduced : Dataset
6673             New `Dataset` object with `idxmax` applied to its data and the
6674             indicated dimension removed.
6675 
6676         See also
6677         --------
6678         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
6679 
6680         Examples
6681         --------
6682 
6683         >>> array1 = xr.DataArray(
6684         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6685         ... )
6686         >>> array2 = xr.DataArray(
6687         ...     [
6688         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6689         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6690         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6691         ...     ],
6692         ...     dims=["y", "x"],
6693         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6694         ... )
6695         >>> ds = xr.Dataset({"int": array1, "float": array2})
6696         >>> ds.max(dim="x")
6697         <xarray.Dataset>
6698         Dimensions:  (y: 3)
6699         Coordinates:
6700           * y        (y) int64 -1 0 1
6701         Data variables:
6702             int      int64 2
6703             float    (y) float64 2.0 2.0 1.0
6704         >>> ds.argmax(dim="x")
6705         <xarray.Dataset>
6706         Dimensions:  (y: 3)
6707         Coordinates:
6708           * y        (y) int64 -1 0 1
6709         Data variables:
6710             int      int64 1
6711             float    (y) int64 0 2 2
6712         >>> ds.idxmax(dim="x")
6713         <xarray.Dataset>
6714         Dimensions:  (y: 3)
6715         Coordinates:
6716           * y        (y) int64 -1 0 1
6717         Data variables:
6718             int      <U1 'b'
6719             float    (y) object 'a' 'c' 'c'
6720         """
6721         return self.map(
6722             methodcaller(
6723                 "idxmax",
6724                 dim=dim,
6725                 skipna=skipna,
6726                 fill_value=fill_value,
6727                 keep_attrs=keep_attrs,
6728             )
6729         )
6730 
6731     def argmin(self, dim=None, axis=None, **kwargs):
6732         """Indices of the minima of the member variables.
6733 
6734         If there are multiple minima, the indices of the first one found will be
6735         returned.
6736 
6737         Parameters
6738         ----------
6739         dim : str, optional
6740             The dimension over which to find the minimum. By default, finds minimum over
6741             all dimensions - for now returning an int for backward compatibility, but
6742             this is deprecated, in future will be an error, since DataArray.argmin will
6743             return a dict with indices for all dimensions, which does not make sense for
6744             a Dataset.
6745         axis : int, optional
6746             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
6747             can be supplied.
6748         keep_attrs : bool, optional
6749             If True, the attributes (`attrs`) will be copied from the original
6750             object to the new one.  If False (default), the new object will be
6751             returned without attributes.
6752         skipna : bool, optional
6753             If True, skip missing values (as marked by NaN). By default, only
6754             skips missing values for float dtypes; other dtypes either do not
6755             have a sentinel missing value (int) or skipna=True has not been
6756             implemented (object, datetime64 or timedelta64).
6757 
6758         Returns
6759         -------
6760         result : Dataset
6761 
6762         See also
6763         --------
6764         DataArray.argmin
6765 
6766         """
6767         if dim is None and axis is None:
6768             warnings.warn(
6769                 "Once the behaviour of DataArray.argmin() and Variable.argmin() with "
6770                 "neither dim nor axis argument changes to return a dict of indices of "
6771                 "each dimension, for consistency it will be an error to call "
6772                 "Dataset.argmin() with no argument, since we don't return a dict of "
6773                 "Datasets.",
6774                 DeprecationWarning,
6775                 stacklevel=2,
6776             )
6777         if (
6778             dim is None
6779             or axis is not None
6780             or (not isinstance(dim, Sequence) and dim is not ...)
6781             or isinstance(dim, str)
6782         ):
6783             # Return int index if single dimension is passed, and is not part of a
6784             # sequence
6785             argmin_func = getattr(duck_array_ops, "argmin")
6786             return self.reduce(argmin_func, dim=dim, axis=axis, **kwargs)
6787         else:
6788             raise ValueError(
6789                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6790                 "dicts cannot be contained in a Dataset, so cannot call "
6791                 "Dataset.argmin() with a sequence or ... for dim"
6792             )
6793 
6794     def argmax(self, dim=None, axis=None, **kwargs):
6795         """Indices of the maxima of the member variables.
6796 
6797         If there are multiple maxima, the indices of the first one found will be
6798         returned.
6799 
6800         Parameters
6801         ----------
6802         dim : str, optional
6803             The dimension over which to find the maximum. By default, finds maximum over
6804             all dimensions - for now returning an int for backward compatibility, but
6805             this is deprecated, in future will be an error, since DataArray.argmax will
6806             return a dict with indices for all dimensions, which does not make sense for
6807             a Dataset.
6808         axis : int, optional
6809             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
6810             can be supplied.
6811         keep_attrs : bool, optional
6812             If True, the attributes (`attrs`) will be copied from the original
6813             object to the new one.  If False (default), the new object will be
6814             returned without attributes.
6815         skipna : bool, optional
6816             If True, skip missing values (as marked by NaN). By default, only
6817             skips missing values for float dtypes; other dtypes either do not
6818             have a sentinel missing value (int) or skipna=True has not been
6819             implemented (object, datetime64 or timedelta64).
6820 
6821         Returns
6822         -------
6823         result : Dataset
6824 
6825         See also
6826         --------
6827         DataArray.argmax
6828 
6829         """
6830         if dim is None and axis is None:
6831             warnings.warn(
6832                 "Once the behaviour of DataArray.argmax() and Variable.argmax() with "
6833                 "neither dim nor axis argument changes to return a dict of indices of "
6834                 "each dimension, for consistency it will be an error to call "
6835                 "Dataset.argmax() with no argument, since we don't return a dict of "
6836                 "Datasets.",
6837                 DeprecationWarning,
6838                 stacklevel=2,
6839             )
6840         if (
6841             dim is None
6842             or axis is not None
6843             or (not isinstance(dim, Sequence) and dim is not ...)
6844             or isinstance(dim, str)
6845         ):
6846             # Return int index if single dimension is passed, and is not part of a
6847             # sequence
6848             argmax_func = getattr(duck_array_ops, "argmax")
6849             return self.reduce(argmax_func, dim=dim, axis=axis, **kwargs)
6850         else:
6851             raise ValueError(
6852                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6853                 "dicts cannot be contained in a Dataset, so cannot call "
6854                 "Dataset.argmin() with a sequence or ... for dim"
6855             )

</code><intended_behavior>The intended behavior at this location is to add a new method named `drop_isel` which would allow users to drop indices from a dimension without the need for coordinate labels. The `drop_isel` method should mimic the behavior of the `drop_sel` method but utilize integer-based indexing instead of label-based. This method should be designed to accept integer indices or boolean masks as inputs and remove the corresponding elements from the dataset. It should support multi-dimensional operations and ensure compatibility with the existing API designs. Exception handling should be robust, providing informative errors or warnings when invalid indices are provided, or when an operation cannot be performed due to dataset constraints. The existing functionality of the Dataset class should be preserved, ensuring that this new method integrates seamlessly without affecting the current capabilities. Integration with the dataset's indexing, selection, and manipulation interface should be considered, providing users with a coherent and flexible API for handling data within xarray.</intended_behavior>]
2025-10-18 15:29:58.147 | INFO     | Search completed. Bug locations: [<file>xarray/core/dataset.py</file>
<class>Dataset</class>
<method>None</method>
<code>
511 class Dataset(Mapping, ImplementsDatasetReduce, DataWithCoords):
512     """A multi-dimensional, in memory, array database.
513 
514     A dataset resembles an in-memory representation of a NetCDF file,
515     and consists of variables, coordinates and attributes which
516     together form a self describing dataset.
517 
518     Dataset implements the mapping interface with keys given by variable
519     names and values given by DataArray objects for each variable name.
520 
521     One dimensional variables with name equal to their dimension are
522     index coordinates used for label based indexing.
523 
524     To load data from a file or file-like object, use the `open_dataset`
525     function.
526 
527     Parameters
528     ----------
529     data_vars : dict-like, optional
530         A mapping from variable names to :py:class:`~xarray.DataArray`
531         objects, :py:class:`~xarray.Variable` objects or to tuples of
532         the form ``(dims, data[, attrs])`` which can be used as
533         arguments to create a new ``Variable``. Each dimension must
534         have the same length in all variables in which it appears.
535 
536         The following notations are accepted:
537 
538         - mapping {var name: DataArray}
539         - mapping {var name: Variable}
540         - mapping {var name: (dimension name, array-like)}
541         - mapping {var name: (tuple of dimension names, array-like)}
542         - mapping {dimension name: array-like}
543           (it will be automatically moved to coords, see below)
544 
545         Each dimension must have the same length in all variables in
546         which it appears.
547     coords : dict-like, optional
548         Another mapping in similar form as the `data_vars` argument,
549         except the each item is saved on the dataset as a "coordinate".
550         These variables have an associated meaning: they describe
551         constant/fixed/independent quantities, unlike the
552         varying/measured/dependent quantities that belong in
553         `variables`. Coordinates values may be given by 1-dimensional
554         arrays or scalars, in which case `dims` do not need to be
555         supplied: 1D arrays will be assumed to give index values along
556         the dimension with the same name.
557 
558         The following notations are accepted:
559 
560         - mapping {coord name: DataArray}
561         - mapping {coord name: Variable}
562         - mapping {coord name: (dimension name, array-like)}
563         - mapping {coord name: (tuple of dimension names, array-like)}
564         - mapping {dimension name: array-like}
565           (the dimension name is implicitly set to be the same as the
566           coord name)
567 
568         The last notation implies that the coord name is the same as
569         the dimension name.
570 
571     attrs : dict-like, optional
572         Global attributes to save on this dataset.
573 
574     Examples
575     --------
576     Create data:
577 
578     >>> np.random.seed(0)
579     >>> temperature = 15 + 8 * np.random.randn(2, 2, 3)
580     >>> precipitation = 10 * np.random.rand(2, 2, 3)
581     >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
582     >>> lat = [[42.25, 42.21], [42.63, 42.59]]
583     >>> time = pd.date_range("2014-09-06", periods=3)
584     >>> reference_time = pd.Timestamp("2014-09-05")
585 
586     Initialize a dataset with multiple dimensions:
587 
588     >>> ds = xr.Dataset(
589     ...     data_vars=dict(
590     ...         temperature=(["x", "y", "time"], temperature),
591     ...         precipitation=(["x", "y", "time"], precipitation),
592     ...     ),
593     ...     coords=dict(
594     ...         lon=(["x", "y"], lon),
595     ...         lat=(["x", "y"], lat),
596     ...         time=time,
597     ...         reference_time=reference_time,
598     ...     ),
599     ...     attrs=dict(description="Weather related data."),
600     ... )
601     >>> ds
602     <xarray.Dataset>
603     Dimensions:         (time: 3, x: 2, y: 2)
604     Coordinates:
605         lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
606         lat             (x, y) float64 42.25 42.21 42.63 42.59
607       * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
608         reference_time  datetime64[ns] 2014-09-05
609     Dimensions without coordinates: x, y
610     Data variables:
611         temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
612         precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
613     Attributes:
614         description:  Weather related data.
615 
616     Find out where the coldest temperature was and what values the
617     other variables had:
618 
619     >>> ds.isel(ds.temperature.argmin(...))
620     <xarray.Dataset>
621     Dimensions:         ()
622     Coordinates:
623         lon             float64 -99.32
624         lat             float64 42.21
625         time            datetime64[ns] 2014-09-08
626         reference_time  datetime64[ns] 2014-09-05
627     Data variables:
628         temperature     float64 7.182
629         precipitation   float64 8.326
630     Attributes:
631         description:  Weather related data.
632     """
633 
634     _attrs: Optional[Dict[Hashable, Any]]
635     _cache: Dict[str, Any]
636     _coord_names: Set[Hashable]
637     _dims: Dict[Hashable, int]
638     _encoding: Optional[Dict[Hashable, Any]]
639     _indexes: Optional[Dict[Hashable, pd.Index]]
640     _variables: Dict[Hashable, Variable]
641 
642     __slots__ = (
643         "_attrs",
644         "_cache",
645         "_coord_names",
646         "_dims",
647         "_encoding",
648         "_file_obj",
649         "_indexes",
650         "_variables",
651         "__weakref__",
652     )
653 
654     _groupby_cls = groupby.DatasetGroupBy
655     _rolling_cls = rolling.DatasetRolling
656     _coarsen_cls = rolling.DatasetCoarsen
657     _resample_cls = resample.DatasetResample
658     _weighted_cls = weighted.DatasetWeighted
659 
660     def __init__(
661         self,
662         # could make a VariableArgs to use more generally, and refine these
663         # categories
664         data_vars: Mapping[Hashable, Any] = None,
665         coords: Mapping[Hashable, Any] = None,
666         attrs: Mapping[Hashable, Any] = None,
667     ):
668         # TODO(shoyer): expose indexes as a public argument in __init__
669 
670         if data_vars is None:
671             data_vars = {}
672         if coords is None:
673             coords = {}
674 
675         both_data_and_coords = set(data_vars) & set(coords)
676         if both_data_and_coords:
677             raise ValueError(
678                 "variables %r are found in both data_vars and coords"
679                 % both_data_and_coords
680             )
681 
682         if isinstance(coords, Dataset):
683             coords = coords.variables
684 
685         variables, coord_names, dims, indexes, _ = merge_data_and_coords(
686             data_vars, coords, compat="broadcast_equals"
687         )
688 
689         self._attrs = dict(attrs) if attrs is not None else None
690         self._file_obj = None
691         self._encoding = None
692         self._variables = variables
693         self._coord_names = coord_names
694         self._dims = dims
695         self._indexes = indexes
696 
697     @classmethod
698     def load_store(cls, store, decoder=None) -> "Dataset":
699         """Create a new dataset from the contents of a backends.*DataStore
700         object
701         """
702         variables, attributes = store.load()
703         if decoder:
704             variables, attributes = decoder(variables, attributes)
705         obj = cls(variables, attrs=attributes)
706         obj._file_obj = store
707         return obj
708 
709     @property
710     def variables(self) -> Mapping[Hashable, Variable]:
711         """Low level interface to Dataset contents as dict of Variable objects.
712 
713         This ordered dictionary is frozen to prevent mutation that could
714         violate Dataset invariants. It contains all variable objects
715         constituting the Dataset, including both data variables and
716         coordinates.
717         """
718         return Frozen(self._variables)
719 
720     @property
721     def attrs(self) -> Dict[Hashable, Any]:
722         """Dictionary of global attributes on this dataset"""
723         if self._attrs is None:
724             self._attrs = {}
725         return self._attrs
726 
727     @attrs.setter
728     def attrs(self, value: Mapping[Hashable, Any]) -> None:
729         self._attrs = dict(value)
730 
731     @property
732     def encoding(self) -> Dict:
733         """Dictionary of global encoding attributes on this dataset"""
734         if self._encoding is None:
735             self._encoding = {}
736         return self._encoding
737 
738     @encoding.setter
739     def encoding(self, value: Mapping) -> None:
740         self._encoding = dict(value)
741 
742     @property
743     def dims(self) -> Mapping[Hashable, int]:
744         """Mapping from dimension names to lengths.
745 
746         Cannot be modified directly, but is updated when adding new variables.
747 
748         Note that type of this object differs from `DataArray.dims`.
749         See `Dataset.sizes` and `DataArray.sizes` for consistently named
750         properties.
751         """
752         return Frozen(SortedKeysDict(self._dims))
753 
754     @property
755     def sizes(self) -> Mapping[Hashable, int]:
756         """Mapping from dimension names to lengths.
757 
758         Cannot be modified directly, but is updated when adding new variables.
759 
760         This is an alias for `Dataset.dims` provided for the benefit of
761         consistency with `DataArray.sizes`.
762 
763         See also
764         --------
765         DataArray.sizes
766         """
767         return self.dims
768 
769     def load(self, **kwargs) -> "Dataset":
770         """Manually trigger loading and/or computation of this dataset's data
771         from disk or a remote source into memory and return this dataset.
772         Unlike compute, the original dataset is modified and returned.
773 
774         Normally, it should not be necessary to call this method in user code,
775         because all xarray functions should either work on deferred data or
776         load data automatically. However, this method can be necessary when
777         working with many file objects on disk.
778 
779         Parameters
780         ----------
781         **kwargs : dict
782             Additional keyword arguments passed on to ``dask.compute``.
783 
784         See Also
785         --------
786         dask.compute
787         """
788         # access .data to coerce everything to numpy or dask arrays
789         lazy_data = {
790             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
791         }
792         if lazy_data:
793             import dask.array as da
794 
795             # evaluate all the dask arrays simultaneously
796             evaluated_data = da.compute(*lazy_data.values(), **kwargs)
797 
798             for k, data in zip(lazy_data, evaluated_data):
799                 self.variables[k].data = data
800 
801         # load everything else sequentially
802         for k, v in self.variables.items():
803             if k not in lazy_data:
804                 v.load()
805 
806         return self
807 
808     def __dask_tokenize__(self):
809         from dask.base import normalize_token
810 
811         return normalize_token(
812             (type(self), self._variables, self._coord_names, self._attrs)
813         )
814 
815     def __dask_graph__(self):
816         graphs = {k: v.__dask_graph__() for k, v in self.variables.items()}
817         graphs = {k: v for k, v in graphs.items() if v is not None}
818         if not graphs:
819             return None
820         else:
821             try:
822                 from dask.highlevelgraph import HighLevelGraph
823 
824                 return HighLevelGraph.merge(*graphs.values())
825             except ImportError:
826                 from dask import sharedict
827 
828                 return sharedict.merge(*graphs.values())
829 
830     def __dask_keys__(self):
831         import dask
832 
833         return [
834             v.__dask_keys__()
835             for v in self.variables.values()
836             if dask.is_dask_collection(v)
837         ]
838 
839     def __dask_layers__(self):
840         import dask
841 
842         return sum(
843             [
844                 v.__dask_layers__()
845                 for v in self.variables.values()
846                 if dask.is_dask_collection(v)
847             ],
848             (),
849         )
850 
851     @property
852     def __dask_optimize__(self):
853         import dask.array as da
854 
855         return da.Array.__dask_optimize__
856 
857     @property
858     def __dask_scheduler__(self):
859         import dask.array as da
860 
861         return da.Array.__dask_scheduler__
862 
863     def __dask_postcompute__(self):
864         import dask
865 
866         info = [
867             (True, k, v.__dask_postcompute__())
868             if dask.is_dask_collection(v)
869             else (False, k, v)
870             for k, v in self._variables.items()
871         ]
872         args = (
873             info,
874             self._coord_names,
875             self._dims,
876             self._attrs,
877             self._indexes,
878             self._encoding,
879             self._file_obj,
880         )
881         return self._dask_postcompute, args
882 
883     def __dask_postpersist__(self):
884         import dask
885 
886         info = [
887             (True, k, v.__dask_postpersist__())
888             if dask.is_dask_collection(v)
889             else (False, k, v)
890             for k, v in self._variables.items()
891         ]
892         args = (
893             info,
894             self._coord_names,
895             self._dims,
896             self._attrs,
897             self._indexes,
898             self._encoding,
899             self._file_obj,
900         )
901         return self._dask_postpersist, args
902 
903     @staticmethod
904     def _dask_postcompute(results, info, *args):
905         variables = {}
906         results2 = list(results[::-1])
907         for is_dask, k, v in info:
908             if is_dask:
909                 func, args2 = v
910                 r = results2.pop()
911                 result = func(r, *args2)
912             else:
913                 result = v
914             variables[k] = result
915 
916         final = Dataset._construct_direct(variables, *args)
917         return final
918 
919     @staticmethod
920     def _dask_postpersist(dsk, info, *args):
921         variables = {}
922         # postpersist is called in both dask.optimize and dask.persist
923         # When persisting, we want to filter out unrelated keys for
924         # each Variable's task graph.
925         is_persist = len(dsk) == len(info)
926         for is_dask, k, v in info:
927             if is_dask:
928                 func, args2 = v
929                 if is_persist:
930                     name = args2[1][0]
931                     dsk2 = {k: v for k, v in dsk.items() if k[0] == name}
932                 else:
933                     dsk2 = dsk
934                 result = func(dsk2, *args2)
935             else:
936                 result = v
937             variables[k] = result
938 
939         return Dataset._construct_direct(variables, *args)
940 
941     def compute(self, **kwargs) -> "Dataset":
942         """Manually trigger loading and/or computation of this dataset's data
943         from disk or a remote source into memory and return a new dataset.
944         Unlike load, the original dataset is left unaltered.
945 
946         Normally, it should not be necessary to call this method in user code,
947         because all xarray functions should either work on deferred data or
948         load data automatically. However, this method can be necessary when
949         working with many file objects on disk.
950 
951         Parameters
952         ----------
953         **kwargs : dict
954             Additional keyword arguments passed on to ``dask.compute``.
955 
956         See Also
957         --------
958         dask.compute
959         """
960         new = self.copy(deep=False)
961         return new.load(**kwargs)
962 
963     def _persist_inplace(self, **kwargs) -> "Dataset":
964         """Persist all Dask arrays in memory"""
965         # access .data to coerce everything to numpy or dask arrays
966         lazy_data = {
967             k: v._data for k, v in self.variables.items() if is_duck_dask_array(v._data)
968         }
969         if lazy_data:
970             import dask
971 
972             # evaluate all the dask arrays simultaneously
973             evaluated_data = dask.persist(*lazy_data.values(), **kwargs)
974 
975             for k, data in zip(lazy_data, evaluated_data):
976                 self.variables[k].data = data
977 
978         return self
979 
980     def persist(self, **kwargs) -> "Dataset":
981         """Trigger computation, keeping data as dask arrays
982 
983         This operation can be used to trigger computation on underlying dask
984         arrays, similar to ``.compute()`` or ``.load()``.  However this
985         operation keeps the data as dask arrays. This is particularly useful
986         when using the dask.distributed scheduler and you want to load a large
987         amount of data into distributed memory.
988 
989         Parameters
990         ----------
991         **kwargs : dict
992             Additional keyword arguments passed on to ``dask.persist``.
993 
994         See Also
995         --------
996         dask.persist
997         """
998         new = self.copy(deep=False)
999         return new._persist_inplace(**kwargs)
1000 
1001     @classmethod
1002     def _construct_direct(
1003         cls,
1004         variables,
1005         coord_names,
1006         dims=None,
1007         attrs=None,
1008         indexes=None,
1009         encoding=None,
1010         file_obj=None,
1011     ):
1012         """Shortcut around __init__ for internal use when we want to skip
1013         costly validation
1014         """
1015         if dims is None:
1016             dims = calculate_dimensions(variables)
1017         obj = object.__new__(cls)
1018         obj._variables = variables
1019         obj._coord_names = coord_names
1020         obj._dims = dims
1021         obj._indexes = indexes
1022         obj._attrs = attrs
1023         obj._file_obj = file_obj
1024         obj._encoding = encoding
1025         return obj
1026 
1027     def _replace(
1028         self,
1029         variables: Dict[Hashable, Variable] = None,
1030         coord_names: Set[Hashable] = None,
1031         dims: Dict[Any, int] = None,
1032         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1033         indexes: Union[Dict[Any, pd.Index], None, Default] = _default,
1034         encoding: Union[dict, None, Default] = _default,
1035         inplace: bool = False,
1036     ) -> "Dataset":
1037         """Fastpath constructor for internal use.
1038 
1039         Returns an object with optionally with replaced attributes.
1040 
1041         Explicitly passed arguments are *not* copied when placed on the new
1042         dataset. It is up to the caller to ensure that they have the right type
1043         and are not used elsewhere.
1044         """
1045         if inplace:
1046             if variables is not None:
1047                 self._variables = variables
1048             if coord_names is not None:
1049                 self._coord_names = coord_names
1050             if dims is not None:
1051                 self._dims = dims
1052             if attrs is not _default:
1053                 self._attrs = attrs
1054             if indexes is not _default:
1055                 self._indexes = indexes
1056             if encoding is not _default:
1057                 self._encoding = encoding
1058             obj = self
1059         else:
1060             if variables is None:
1061                 variables = self._variables.copy()
1062             if coord_names is None:
1063                 coord_names = self._coord_names.copy()
1064             if dims is None:
1065                 dims = self._dims.copy()
1066             if attrs is _default:
1067                 attrs = copy.copy(self._attrs)
1068             if indexes is _default:
1069                 indexes = copy.copy(self._indexes)
1070             if encoding is _default:
1071                 encoding = copy.copy(self._encoding)
1072             obj = self._construct_direct(
1073                 variables, coord_names, dims, attrs, indexes, encoding
1074             )
1075         return obj
1076 
1077     def _replace_with_new_dims(
1078         self,
1079         variables: Dict[Hashable, Variable],
1080         coord_names: set = None,
1081         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1082         indexes: Union[Dict[Hashable, pd.Index], None, Default] = _default,
1083         inplace: bool = False,
1084     ) -> "Dataset":
1085         """Replace variables with recalculated dimensions."""
1086         dims = calculate_dimensions(variables)
1087         return self._replace(
1088             variables, coord_names, dims, attrs, indexes, inplace=inplace
1089         )
1090 
1091     def _replace_vars_and_dims(
1092         self,
1093         variables: Dict[Hashable, Variable],
1094         coord_names: set = None,
1095         dims: Dict[Hashable, int] = None,
1096         attrs: Union[Dict[Hashable, Any], None, Default] = _default,
1097         inplace: bool = False,
1098     ) -> "Dataset":
1099         """Deprecated version of _replace_with_new_dims().
1100 
1101         Unlike _replace_with_new_dims(), this method always recalculates
1102         indexes from variables.
1103         """
1104         if dims is None:
1105             dims = calculate_dimensions(variables)
1106         return self._replace(
1107             variables, coord_names, dims, attrs, indexes=None, inplace=inplace
1108         )
1109 
1110     def _overwrite_indexes(self, indexes: Mapping[Any, pd.Index]) -> "Dataset":
1111         if not indexes:
1112             return self
1113 
1114         variables = self._variables.copy()
1115         new_indexes = dict(self.indexes)
1116         for name, idx in indexes.items():
1117             variables[name] = IndexVariable(name, idx)
1118             new_indexes[name] = idx
1119         obj = self._replace(variables, indexes=new_indexes)
1120 
1121         # switch from dimension to level names, if necessary
1122         dim_names: Dict[Hashable, str] = {}
1123         for dim, idx in indexes.items():
1124             if not isinstance(idx, pd.MultiIndex) and idx.name != dim:
1125                 dim_names[dim] = idx.name
1126         if dim_names:
1127             obj = obj.rename(dim_names)
1128         return obj
1129 
1130     def copy(self, deep: bool = False, data: Mapping = None) -> "Dataset":
1131         """Returns a copy of this dataset.
1132 
1133         If `deep=True`, a deep copy is made of each of the component variables.
1134         Otherwise, a shallow copy of each of the component variable is made, so
1135         that the underlying memory region of the new dataset is the same as in
1136         the original dataset.
1137 
1138         Use `data` to create a new object with the same structure as
1139         original but entirely new data.
1140 
1141         Parameters
1142         ----------
1143         deep : bool, optional
1144             Whether each component variable is loaded into memory and copied onto
1145             the new object. Default is False.
1146         data : dict-like, optional
1147             Data to use in the new object. Each item in `data` must have same
1148             shape as corresponding data variable in original. When `data` is
1149             used, `deep` is ignored for the data variables and only used for
1150             coords.
1151 
1152         Returns
1153         -------
1154         object : Dataset
1155             New object with dimensions, attributes, coordinates, name, encoding,
1156             and optionally data copied from original.
1157 
1158         Examples
1159         --------
1160 
1161         Shallow copy versus deep copy
1162 
1163         >>> da = xr.DataArray(np.random.randn(2, 3))
1164         >>> ds = xr.Dataset(
1165         ...     {"foo": da, "bar": ("x", [-1, 2])},
1166         ...     coords={"x": ["one", "two"]},
1167         ... )
1168         >>> ds.copy()
1169         <xarray.Dataset>
1170         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1171         Coordinates:
1172           * x        (x) <U3 'one' 'two'
1173         Dimensions without coordinates: dim_0, dim_1
1174         Data variables:
1175             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
1176             bar      (x) int64 -1 2
1177 
1178         >>> ds_0 = ds.copy(deep=False)
1179         >>> ds_0["foo"][0, 0] = 7
1180         >>> ds_0
1181         <xarray.Dataset>
1182         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1183         Coordinates:
1184           * x        (x) <U3 'one' 'two'
1185         Dimensions without coordinates: dim_0, dim_1
1186         Data variables:
1187             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1188             bar      (x) int64 -1 2
1189 
1190         >>> ds
1191         <xarray.Dataset>
1192         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1193         Coordinates:
1194           * x        (x) <U3 'one' 'two'
1195         Dimensions without coordinates: dim_0, dim_1
1196         Data variables:
1197             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1198             bar      (x) int64 -1 2
1199 
1200         Changing the data using the ``data`` argument maintains the
1201         structure of the original object, but with the new data. Original
1202         object is unaffected.
1203 
1204         >>> ds.copy(data={"foo": np.arange(6).reshape(2, 3), "bar": ["a", "b"]})
1205         <xarray.Dataset>
1206         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1207         Coordinates:
1208           * x        (x) <U3 'one' 'two'
1209         Dimensions without coordinates: dim_0, dim_1
1210         Data variables:
1211             foo      (dim_0, dim_1) int64 0 1 2 3 4 5
1212             bar      (x) <U1 'a' 'b'
1213 
1214         >>> ds
1215         <xarray.Dataset>
1216         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
1217         Coordinates:
1218           * x        (x) <U3 'one' 'two'
1219         Dimensions without coordinates: dim_0, dim_1
1220         Data variables:
1221             foo      (dim_0, dim_1) float64 7.0 0.4002 0.9787 2.241 1.868 -0.9773
1222             bar      (x) int64 -1 2
1223 
1224         See Also
1225         --------
1226         pandas.DataFrame.copy
1227         """
1228         if data is None:
1229             variables = {k: v.copy(deep=deep) for k, v in self._variables.items()}
1230         elif not utils.is_dict_like(data):
1231             raise ValueError("Data must be dict-like")
1232         else:
1233             var_keys = set(self.data_vars.keys())
1234             data_keys = set(data.keys())
1235             keys_not_in_vars = data_keys - var_keys
1236             if keys_not_in_vars:
1237                 raise ValueError(
1238                     "Data must only contain variables in original "
1239                     "dataset. Extra variables: {}".format(keys_not_in_vars)
1240                 )
1241             keys_missing_from_data = var_keys - data_keys
1242             if keys_missing_from_data:
1243                 raise ValueError(
1244                     "Data must contain all variables in original "
1245                     "dataset. Data is missing {}".format(keys_missing_from_data)
1246                 )
1247             variables = {
1248                 k: v.copy(deep=deep, data=data.get(k))
1249                 for k, v in self._variables.items()
1250             }
1251 
1252         attrs = copy.deepcopy(self._attrs) if deep else copy.copy(self._attrs)
1253 
1254         return self._replace(variables, attrs=attrs)
1255 
1256     @property
1257     def _level_coords(self) -> Dict[str, Hashable]:
1258         """Return a mapping of all MultiIndex levels and their corresponding
1259         coordinate name.
1260         """
1261         level_coords: Dict[str, Hashable] = {}
1262         for name, index in self.indexes.items():
1263             if isinstance(index, pd.MultiIndex):
1264                 level_names = index.names
1265                 (dim,) = self.variables[name].dims
1266                 level_coords.update({lname: dim for lname in level_names})
1267         return level_coords
1268 
1269     def _copy_listed(self, names: Iterable[Hashable]) -> "Dataset":
1270         """Create a new Dataset with the listed variables from this dataset and
1271         the all relevant coordinates. Skips all validation.
1272         """
1273         variables: Dict[Hashable, Variable] = {}
1274         coord_names = set()
1275         indexes: Dict[Hashable, pd.Index] = {}
1276 
1277         for name in names:
1278             try:
1279                 variables[name] = self._variables[name]
1280             except KeyError:
1281                 ref_name, var_name, var = _get_virtual_variable(
1282                     self._variables, name, self._level_coords, self.dims
1283                 )
1284                 variables[var_name] = var
1285                 if ref_name in self._coord_names or ref_name in self.dims:
1286                     coord_names.add(var_name)
1287                 if (var_name,) == var.dims:
1288                     indexes[var_name] = var.to_index()
1289 
1290         needed_dims: Set[Hashable] = set()
1291         for v in variables.values():
1292             needed_dims.update(v.dims)
1293 
1294         dims = {k: self.dims[k] for k in needed_dims}
1295 
1296         # preserves ordering of coordinates
1297         for k in self._variables:
1298             if k not in self._coord_names:
1299                 continue
1300 
1301             if set(self.variables[k].dims) <= needed_dims:
1302                 variables[k] = self._variables[k]
1303                 coord_names.add(k)
1304                 if k in self.indexes:
1305                     indexes[k] = self.indexes[k]
1306 
1307         return self._replace(variables, coord_names, dims, indexes=indexes)
1308 
1309     def _construct_dataarray(self, name: Hashable) -> "DataArray":
1310         """Construct a DataArray by indexing this dataset"""
1311         from .dataarray import DataArray
1312 
1313         try:
1314             variable = self._variables[name]
1315         except KeyError:
1316             _, name, variable = _get_virtual_variable(
1317                 self._variables, name, self._level_coords, self.dims
1318             )
1319 
1320         needed_dims = set(variable.dims)
1321 
1322         coords: Dict[Hashable, Variable] = {}
1323         # preserve ordering
1324         for k in self._variables:
1325             if k in self._coord_names and set(self.variables[k].dims) <= needed_dims:
1326                 coords[k] = self.variables[k]
1327 
1328         if self._indexes is None:
1329             indexes = None
1330         else:
1331             indexes = {k: v for k, v in self._indexes.items() if k in coords}
1332 
1333         return DataArray(variable, coords, name=name, indexes=indexes, fastpath=True)
1334 
1335     def __copy__(self) -> "Dataset":
1336         return self.copy(deep=False)
1337 
1338     def __deepcopy__(self, memo=None) -> "Dataset":
1339         # memo does nothing but is required for compatibility with
1340         # copy.deepcopy
1341         return self.copy(deep=True)
1342 
1343     @property
1344     def _attr_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1345         """Places to look-up items for attribute-style access"""
1346         yield from self._item_sources
1347         yield self.attrs
1348 
1349     @property
1350     def _item_sources(self) -> Iterable[Mapping[Hashable, Any]]:
1351         """Places to look-up items for key-completion"""
1352         yield self.data_vars
1353         yield HybridMappingProxy(keys=self._coord_names, mapping=self.coords)
1354 
1355         # virtual coordinates
1356         yield HybridMappingProxy(keys=self.dims, mapping=self)
1357 
1358         # uses empty dict -- everything here can already be found in self.coords.
1359         yield HybridMappingProxy(keys=self._level_coords, mapping={})
1360 
1361     def __contains__(self, key: object) -> bool:
1362         """The 'in' operator will return true or false depending on whether
1363         'key' is an array in the dataset or not.
1364         """
1365         return key in self._variables
1366 
1367     def __len__(self) -> int:
1368         return len(self.data_vars)
1369 
1370     def __bool__(self) -> bool:
1371         return bool(self.data_vars)
1372 
1373     def __iter__(self) -> Iterator[Hashable]:
1374         return iter(self.data_vars)
1375 
1376     def __array__(self, dtype=None):
1377         raise TypeError(
1378             "cannot directly convert an xarray.Dataset into a "
1379             "numpy array. Instead, create an xarray.DataArray "
1380             "first, either with indexing on the Dataset or by "
1381             "invoking the `to_array()` method."
1382         )
1383 
1384     @property
1385     def nbytes(self) -> int:
1386         return sum(v.nbytes for v in self.variables.values())
1387 
1388     @property
1389     def loc(self) -> _LocIndexer:
1390         """Attribute for location based indexing. Only supports __getitem__,
1391         and only when the key is a dict of the form {dim: labels}.
1392         """
1393         return _LocIndexer(self)
1394 
1395     # FIXME https://github.com/python/mypy/issues/7328
1396     @overload
1397     def __getitem__(self, key: Mapping) -> "Dataset":  # type: ignore
1398         ...
1399 
1400     @overload
1401     def __getitem__(self, key: Hashable) -> "DataArray":  # type: ignore
1402         ...
1403 
1404     @overload
1405     def __getitem__(self, key: Any) -> "Dataset":
1406         ...
1407 
1408     def __getitem__(self, key):
1409         """Access variables or coordinates this dataset as a
1410         :py:class:`~xarray.DataArray`.
1411 
1412         Indexing with a list of names will return a new ``Dataset`` object.
1413         """
1414         if utils.is_dict_like(key):
1415             return self.isel(**cast(Mapping, key))
1416 
1417         if hashable(key):
1418             return self._construct_dataarray(key)
1419         else:
1420             return self._copy_listed(np.asarray(key))
1421 
1422     def __setitem__(self, key: Hashable, value) -> None:
1423         """Add an array to this dataset.
1424 
1425         If value is a `DataArray`, call its `select_vars()` method, rename it
1426         to `key` and merge the contents of the resulting dataset into this
1427         dataset.
1428 
1429         If value is an `Variable` object (or tuple of form
1430         ``(dims, data[, attrs])``), add it to this dataset as a new
1431         variable.
1432         """
1433         if utils.is_dict_like(key):
1434             raise NotImplementedError(
1435                 "cannot yet use a dictionary as a key to set Dataset values"
1436             )
1437 
1438         self.update({key: value})
1439 
1440     def __delitem__(self, key: Hashable) -> None:
1441         """Remove a variable from this dataset."""
1442         del self._variables[key]
1443         self._coord_names.discard(key)
1444         if key in self.indexes:
1445             assert self._indexes is not None
1446             del self._indexes[key]
1447         self._dims = calculate_dimensions(self._variables)
1448 
1449     # mutable objects should not be hashable
1450     # https://github.com/python/mypy/issues/4266
1451     __hash__ = None  # type: ignore
1452 
1453     def _all_compat(self, other: "Dataset", compat_str: str) -> bool:
1454         """Helper function for equals and identical"""
1455 
1456         # some stores (e.g., scipy) do not seem to preserve order, so don't
1457         # require matching order for equality
1458         def compat(x: Variable, y: Variable) -> bool:
1459             return getattr(x, compat_str)(y)
1460 
1461         return self._coord_names == other._coord_names and utils.dict_equiv(
1462             self._variables, other._variables, compat=compat
1463         )
1464 
1465     def broadcast_equals(self, other: "Dataset") -> bool:
1466         """Two Datasets are broadcast equal if they are equal after
1467         broadcasting all variables against each other.
1468 
1469         For example, variables that are scalar in one dataset but non-scalar in
1470         the other dataset can still be broadcast equal if the the non-scalar
1471         variable is a constant.
1472 
1473         See Also
1474         --------
1475         Dataset.equals
1476         Dataset.identical
1477         """
1478         try:
1479             return self._all_compat(other, "broadcast_equals")
1480         except (TypeError, AttributeError):
1481             return False
1482 
1483     def equals(self, other: "Dataset") -> bool:
1484         """Two Datasets are equal if they have matching variables and
1485         coordinates, all of which are equal.
1486 
1487         Datasets can still be equal (like pandas objects) if they have NaN
1488         values in the same locations.
1489 
1490         This method is necessary because `v1 == v2` for ``Dataset``
1491         does element-wise comparisons (like numpy.ndarrays).
1492 
1493         See Also
1494         --------
1495         Dataset.broadcast_equals
1496         Dataset.identical
1497         """
1498         try:
1499             return self._all_compat(other, "equals")
1500         except (TypeError, AttributeError):
1501             return False
1502 
1503     def identical(self, other: "Dataset") -> bool:
1504         """Like equals, but also checks all dataset attributes and the
1505         attributes on all variables and coordinates.
1506 
1507         See Also
1508         --------
1509         Dataset.broadcast_equals
1510         Dataset.equals
1511         """
1512         try:
1513             return utils.dict_equiv(self.attrs, other.attrs) and self._all_compat(
1514                 other, "identical"
1515             )
1516         except (TypeError, AttributeError):
1517             return False
1518 
1519     @property
1520     def indexes(self) -> Indexes:
1521         """Mapping of pandas.Index objects used for label based indexing"""
1522         if self._indexes is None:
1523             self._indexes = default_indexes(self._variables, self._dims)
1524         return Indexes(self._indexes)
1525 
1526     @property
1527     def coords(self) -> DatasetCoordinates:
1528         """Dictionary of xarray.DataArray objects corresponding to coordinate
1529         variables
1530         """
1531         return DatasetCoordinates(self)
1532 
1533     @property
1534     def data_vars(self) -> DataVariables:
1535         """Dictionary of DataArray objects corresponding to data variables"""
1536         return DataVariables(self)
1537 
1538     def set_coords(self, names: "Union[Hashable, Iterable[Hashable]]") -> "Dataset":
1539         """Given names of one or more variables, set them as coordinates
1540 
1541         Parameters
1542         ----------
1543         names : hashable or iterable of hashable
1544             Name(s) of variables in this dataset to convert into coordinates.
1545 
1546         Returns
1547         -------
1548         Dataset
1549 
1550         See also
1551         --------
1552         Dataset.swap_dims
1553         """
1554         # TODO: allow inserting new coordinates with this method, like
1555         # DataFrame.set_index?
1556         # nb. check in self._variables, not self.data_vars to insure that the
1557         # operation is idempotent
1558         if isinstance(names, str) or not isinstance(names, Iterable):
1559             names = [names]
1560         else:
1561             names = list(names)
1562         self._assert_all_in_dataset(names)
1563         obj = self.copy()
1564         obj._coord_names.update(names)
1565         return obj
1566 
1567     def reset_coords(
1568         self,
1569         names: "Union[Hashable, Iterable[Hashable], None]" = None,
1570         drop: bool = False,
1571     ) -> "Dataset":
1572         """Given names of coordinates, reset them to become variables
1573 
1574         Parameters
1575         ----------
1576         names : hashable or iterable of hashable, optional
1577             Name(s) of non-index coordinates in this dataset to reset into
1578             variables. By default, all non-index coordinates are reset.
1579         drop : bool, optional
1580             If True, remove coordinates instead of converting them into
1581             variables.
1582 
1583         Returns
1584         -------
1585         Dataset
1586         """
1587         if names is None:
1588             names = self._coord_names - set(self.dims)
1589         else:
1590             if isinstance(names, str) or not isinstance(names, Iterable):
1591                 names = [names]
1592             else:
1593                 names = list(names)
1594             self._assert_all_in_dataset(names)
1595             bad_coords = set(names) & set(self.dims)
1596             if bad_coords:
1597                 raise ValueError(
1598                     "cannot remove index coordinates with reset_coords: %s" % bad_coords
1599                 )
1600         obj = self.copy()
1601         obj._coord_names.difference_update(names)
1602         if drop:
1603             for name in names:
1604                 del obj._variables[name]
1605         return obj
1606 
1607     def dump_to_store(self, store: "AbstractDataStore", **kwargs) -> None:
1608         """Store dataset contents to a backends.*DataStore object."""
1609         from ..backends.api import dump_to_store
1610 
1611         # TODO: rename and/or cleanup this method to make it more consistent
1612         # with to_netcdf()
1613         dump_to_store(self, store, **kwargs)
1614 
1615     def to_netcdf(
1616         self,
1617         path=None,
1618         mode: str = "w",
1619         format: str = None,
1620         group: str = None,
1621         engine: str = None,
1622         encoding: Mapping = None,
1623         unlimited_dims: Iterable[Hashable] = None,
1624         compute: bool = True,
1625         invalid_netcdf: bool = False,
1626     ) -> Union[bytes, "Delayed", None]:
1627         """Write dataset contents to a netCDF file.
1628 
1629         Parameters
1630         ----------
1631         path : str, Path or file-like, optional
1632             Path to which to save this dataset. File-like objects are only
1633             supported by the scipy engine. If no path is provided, this
1634             function returns the resulting netCDF file as bytes; in this case,
1635             we need to use scipy, which does not support netCDF version 4 (the
1636             default format becomes NETCDF3_64BIT).
1637         mode : {"w", "a"}, default: "w"
1638             Write ('w') or append ('a') mode. If mode='w', any existing file at
1639             this location will be overwritten. If mode='a', existing variables
1640             will be overwritten.
1641         format : {"NETCDF4", "NETCDF4_CLASSIC", "NETCDF3_64BIT", \
1642                   "NETCDF3_CLASSIC"}, optional
1643             File format for the resulting netCDF file:
1644 
1645             * NETCDF4: Data is stored in an HDF5 file, using netCDF4 API
1646               features.
1647             * NETCDF4_CLASSIC: Data is stored in an HDF5 file, using only
1648               netCDF 3 compatible API features.
1649             * NETCDF3_64BIT: 64-bit offset version of the netCDF 3 file format,
1650               which fully supports 2+ GB files, but is only compatible with
1651               clients linked against netCDF version 3.6.0 or later.
1652             * NETCDF3_CLASSIC: The classic netCDF 3 file format. It does not
1653               handle 2+ GB files very well.
1654 
1655             All formats are supported by the netCDF4-python library.
1656             scipy.io.netcdf only supports the last two formats.
1657 
1658             The default format is NETCDF4 if you are saving a file to disk and
1659             have the netCDF4-python library available. Otherwise, xarray falls
1660             back to using scipy to write netCDF files and defaults to the
1661             NETCDF3_64BIT format (scipy does not support netCDF4).
1662         group : str, optional
1663             Path to the netCDF4 group in the given file to open (only works for
1664             format='NETCDF4'). The group(s) will be created if necessary.
1665         engine : {"netcdf4", "scipy", "h5netcdf"}, optional
1666             Engine to use when writing netCDF files. If not provided, the
1667             default engine is chosen based on available dependencies, with a
1668             preference for 'netcdf4' if writing to a file on disk.
1669         encoding : dict, optional
1670             Nested dictionary with variable names as keys and dictionaries of
1671             variable specific encodings as values, e.g.,
1672             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,
1673             "zlib": True}, ...}``
1674 
1675             The `h5netcdf` engine supports both the NetCDF4-style compression
1676             encoding parameters ``{"zlib": True, "complevel": 9}`` and the h5py
1677             ones ``{"compression": "gzip", "compression_opts": 9}``.
1678             This allows using any compression plugin installed in the HDF5
1679             library, e.g. LZF.
1680 
1681         unlimited_dims : iterable of hashable, optional
1682             Dimension(s) that should be serialized as unlimited dimensions.
1683             By default, no dimensions are treated as unlimited dimensions.
1684             Note that unlimited_dims may also be set via
1685             ``dataset.encoding["unlimited_dims"]``.
1686         compute: bool, default: True
1687             If true compute immediately, otherwise return a
1688             ``dask.delayed.Delayed`` object that can be computed later.
1689         invalid_netcdf: bool, default: False
1690             Only valid along with ``engine="h5netcdf"``. If True, allow writing
1691             hdf5 files which are invalid netcdf as described in
1692             https://github.com/shoyer/h5netcdf.
1693         """
1694         if encoding is None:
1695             encoding = {}
1696         from ..backends.api import to_netcdf
1697 
1698         return to_netcdf(
1699             self,
1700             path,
1701             mode,
1702             format=format,
1703             group=group,
1704             engine=engine,
1705             encoding=encoding,
1706             unlimited_dims=unlimited_dims,
1707             compute=compute,
1708             invalid_netcdf=invalid_netcdf,
1709         )
1710 
1711     def to_zarr(
1712         self,
1713         store: Union[MutableMapping, str, Path] = None,
1714         chunk_store: Union[MutableMapping, str, Path] = None,
1715         mode: str = None,
1716         synchronizer=None,
1717         group: str = None,
1718         encoding: Mapping = None,
1719         compute: bool = True,
1720         consolidated: bool = False,
1721         append_dim: Hashable = None,
1722         region: Mapping[str, slice] = None,
1723     ) -> "ZarrStore":
1724         """Write dataset contents to a zarr group.
1725 
1726         .. note:: Experimental
1727                   The Zarr backend is new and experimental. Please report any
1728                   unexpected behavior via github issues.
1729 
1730         Parameters
1731         ----------
1732         store : MutableMapping, str or Path, optional
1733             Store or path to directory in file system.
1734         chunk_store : MutableMapping, str or Path, optional
1735             Store or path to directory in file system only for Zarr array chunks.
1736             Requires zarr-python v2.4.0 or later.
1737         mode : {"w", "w-", "a", None}, optional
1738             Persistence mode: "w" means create (overwrite if exists);
1739             "w-" means create (fail if exists);
1740             "a" means override existing variables (create if does not exist).
1741             If ``append_dim`` is set, ``mode`` can be omitted as it is
1742             internally set to ``"a"``. Otherwise, ``mode`` will default to
1743             `w-` if not set.
1744         synchronizer : object, optional
1745             Zarr array synchronizer.
1746         group : str, optional
1747             Group path. (a.k.a. `path` in zarr terminology.)
1748         encoding : dict, optional
1749             Nested dictionary with variable names as keys and dictionaries of
1750             variable specific encodings as values, e.g.,
1751             ``{"my_variable": {"dtype": "int16", "scale_factor": 0.1,}, ...}``
1752         compute: bool, optional
1753             If True write array data immediately, otherwise return a
1754             ``dask.delayed.Delayed`` object that can be computed to write
1755             array data later. Metadata is always updated eagerly.
1756         consolidated: bool, optional
1757             If True, apply zarr's `consolidate_metadata` function to the store
1758             after writing metadata.
1759         append_dim: hashable, optional
1760             If set, the dimension along which the data will be appended. All
1761             other dimensions on overriden variables must remain the same size.
1762         region: dict, optional
1763             Optional mapping from dimension names to integer slices along
1764             dataset dimensions to indicate the region of existing zarr array(s)
1765             in which to write this dataset's data. For example,
1766             ``{'x': slice(0, 1000), 'y': slice(10000, 11000)}`` would indicate
1767             that values should be written to the region ``0:1000`` along ``x``
1768             and ``10000:11000`` along ``y``.
1769 
1770             Two restrictions apply to the use of ``region``:
1771 
1772             - If ``region`` is set, _all_ variables in a dataset must have at
1773               least one dimension in common with the region. Other variables
1774               should be written in a separate call to ``to_zarr()``.
1775             - Dimensions cannot be included in both ``region`` and
1776               ``append_dim`` at the same time. To create empty arrays to fill
1777               in with ``region``, use a separate call to ``to_zarr()`` with
1778               ``compute=False``. See "Appending to existing Zarr stores" in
1779               the reference documentation for full details.
1780 
1781         References
1782         ----------
1783         https://zarr.readthedocs.io/
1784 
1785         Notes
1786         -----
1787         Zarr chunking behavior:
1788             If chunks are found in the encoding argument or attribute
1789             corresponding to any DataArray, those chunks are used.
1790             If a DataArray is a dask array, it is written with those chunks.
1791             If not other chunks are found, Zarr uses its own heuristics to
1792             choose automatic chunk sizes.
1793         """
1794         from ..backends.api import to_zarr
1795 
1796         if encoding is None:
1797             encoding = {}
1798 
1799         return to_zarr(
1800             self,
1801             store=store,
1802             chunk_store=chunk_store,
1803             mode=mode,
1804             synchronizer=synchronizer,
1805             group=group,
1806             encoding=encoding,
1807             compute=compute,
1808             consolidated=consolidated,
1809             append_dim=append_dim,
1810             region=region,
1811         )
1812 
1813     def __repr__(self) -> str:
1814         return formatting.dataset_repr(self)
1815 
1816     def _repr_html_(self):
1817         if OPTIONS["display_style"] == "text":
1818             return f"<pre>{escape(repr(self))}</pre>"
1819         return formatting_html.dataset_repr(self)
1820 
1821     def info(self, buf=None) -> None:
1822         """
1823         Concise summary of a Dataset variables and attributes.
1824 
1825         Parameters
1826         ----------
1827         buf : file-like, default: sys.stdout
1828             writable buffer
1829 
1830         See Also
1831         --------
1832         pandas.DataFrame.assign
1833         ncdump: netCDF's ncdump
1834         """
1835         if buf is None:  # pragma: no cover
1836             buf = sys.stdout
1837 
1838         lines = []
1839         lines.append("xarray.Dataset {")
1840         lines.append("dimensions:")
1841         for name, size in self.dims.items():
1842             lines.append(f"\t{name} = {size} ;")
1843         lines.append("\nvariables:")
1844         for name, da in self.variables.items():
1845             dims = ", ".join(da.dims)
1846             lines.append(f"\t{da.dtype} {name}({dims}) ;")
1847             for k, v in da.attrs.items():
1848                 lines.append(f"\t\t{name}:{k} = {v} ;")
1849         lines.append("\n// global attributes:")
1850         for k, v in self.attrs.items():
1851             lines.append(f"\t:{k} = {v} ;")
1852         lines.append("}")
1853 
1854         buf.write("\n".join(lines))
1855 
1856     @property
1857     def chunks(self) -> Mapping[Hashable, Tuple[int, ...]]:
1858         """Block dimensions for this dataset's data or None if it's not a dask
1859         array.
1860         """
1861         chunks: Dict[Hashable, Tuple[int, ...]] = {}
1862         for v in self.variables.values():
1863             if v.chunks is not None:
1864                 for dim, c in zip(v.dims, v.chunks):
1865                     if dim in chunks and c != chunks[dim]:
1866                         raise ValueError(
1867                             f"Object has inconsistent chunks along dimension {dim}. "
1868                             "This can be fixed by calling unify_chunks()."
1869                         )
1870                     chunks[dim] = c
1871         return Frozen(SortedKeysDict(chunks))
1872 
1873     def chunk(
1874         self,
1875         chunks: Union[
1876             Number,
1877             str,
1878             Mapping[Hashable, Union[None, Number, str, Tuple[Number, ...]]],
1879         ] = {},  # {} even though it's technically unsafe, is being used intentionally here (#4667)
1880         name_prefix: str = "xarray-",
1881         token: str = None,
1882         lock: bool = False,
1883     ) -> "Dataset":
1884         """Coerce all arrays in this dataset into dask arrays with the given
1885         chunks.
1886 
1887         Non-dask arrays in this dataset will be converted to dask arrays. Dask
1888         arrays will be rechunked to the given chunk sizes.
1889 
1890         If neither chunks is not provided for one or more dimensions, chunk
1891         sizes along that dimension will not be updated; non-dask arrays will be
1892         converted into dask arrays with a single block.
1893 
1894         Parameters
1895         ----------
1896         chunks : int, 'auto' or mapping, optional
1897             Chunk sizes along each dimension, e.g., ``5`` or
1898             ``{"x": 5, "y": 5}``.
1899         name_prefix : str, optional
1900             Prefix for the name of any new dask arrays.
1901         token : str, optional
1902             Token uniquely identifying this dataset.
1903         lock : optional
1904             Passed on to :py:func:`dask.array.from_array`, if the array is not
1905             already as dask array.
1906 
1907         Returns
1908         -------
1909         chunked : xarray.Dataset
1910         """
1911         if chunks is None:
1912             warnings.warn(
1913                 "None value for 'chunks' is deprecated. "
1914                 "It will raise an error in the future. Use instead '{}'",
1915                 category=FutureWarning,
1916             )
1917             chunks = {}
1918 
1919         if isinstance(chunks, (Number, str)):
1920             chunks = dict.fromkeys(self.dims, chunks)
1921 
1922         bad_dims = chunks.keys() - self.dims.keys()
1923         if bad_dims:
1924             raise ValueError(
1925                 "some chunks keys are not dimensions on this " "object: %s" % bad_dims
1926             )
1927 
1928         variables = {
1929             k: _maybe_chunk(k, v, chunks, token, lock, name_prefix)
1930             for k, v in self.variables.items()
1931         }
1932         return self._replace(variables)
1933 
1934     def _validate_indexers(
1935         self, indexers: Mapping[Hashable, Any], missing_dims: str = "raise"
1936     ) -> Iterator[Tuple[Hashable, Union[int, slice, np.ndarray, Variable]]]:
1937         """Here we make sure
1938         + indexer has a valid keys
1939         + indexer is in a valid data type
1940         + string indexers are cast to the appropriate date type if the
1941           associated index is a DatetimeIndex or CFTimeIndex
1942         """
1943         from .dataarray import DataArray
1944 
1945         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
1946 
1947         # all indexers should be int, slice, np.ndarrays, or Variable
1948         for k, v in indexers.items():
1949             if isinstance(v, (int, slice, Variable)):
1950                 yield k, v
1951             elif isinstance(v, DataArray):
1952                 yield k, v.variable
1953             elif isinstance(v, tuple):
1954                 yield k, as_variable(v)
1955             elif isinstance(v, Dataset):
1956                 raise TypeError("cannot use a Dataset as an indexer")
1957             elif isinstance(v, Sequence) and len(v) == 0:
1958                 yield k, np.empty((0,), dtype="int64")
1959             else:
1960                 v = np.asarray(v)
1961 
1962                 if v.dtype.kind in "US":
1963                     index = self.indexes[k]
1964                     if isinstance(index, pd.DatetimeIndex):
1965                         v = v.astype("datetime64[ns]")
1966                     elif isinstance(index, xr.CFTimeIndex):
1967                         v = _parse_array_of_cftime_strings(v, index.date_type)
1968 
1969                 if v.ndim > 1:
1970                     raise IndexError(
1971                         "Unlabeled multi-dimensional array cannot be "
1972                         "used for indexing: {}".format(k)
1973                     )
1974                 yield k, v
1975 
1976     def _validate_interp_indexers(
1977         self, indexers: Mapping[Hashable, Any]
1978     ) -> Iterator[Tuple[Hashable, Variable]]:
1979         """Variant of _validate_indexers to be used for interpolation"""
1980         for k, v in self._validate_indexers(indexers):
1981             if isinstance(v, Variable):
1982                 if v.ndim == 1:
1983                     yield k, v.to_index_variable()
1984                 else:
1985                     yield k, v
1986             elif isinstance(v, int):
1987                 yield k, Variable((), v)
1988             elif isinstance(v, np.ndarray):
1989                 if v.ndim == 0:
1990                     yield k, Variable((), v)
1991                 elif v.ndim == 1:
1992                     yield k, IndexVariable((k,), v)
1993                 else:
1994                     raise AssertionError()  # Already tested by _validate_indexers
1995             else:
1996                 raise TypeError(type(v))
1997 
1998     def _get_indexers_coords_and_indexes(self, indexers):
1999         """Extract coordinates and indexes from indexers.
2000 
2001         Only coordinate with a name different from any of self.variables will
2002         be attached.
2003         """
2004         from .dataarray import DataArray
2005 
2006         coords_list = []
2007         for k, v in indexers.items():
2008             if isinstance(v, DataArray):
2009                 if v.dtype.kind == "b":
2010                     if v.ndim != 1:  # we only support 1-d boolean array
2011                         raise ValueError(
2012                             "{:d}d-boolean array is used for indexing along "
2013                             "dimension {!r}, but only 1d boolean arrays are "
2014                             "supported.".format(v.ndim, k)
2015                         )
2016                     # Make sure in case of boolean DataArray, its
2017                     # coordinate also should be indexed.
2018                     v_coords = v[v.values.nonzero()[0]].coords
2019                 else:
2020                     v_coords = v.coords
2021                 coords_list.append(v_coords)
2022 
2023         # we don't need to call align() explicitly or check indexes for
2024         # alignment, because merge_variables already checks for exact alignment
2025         # between dimension coordinates
2026         coords, indexes = merge_coordinates_without_align(coords_list)
2027         assert_coordinate_consistent(self, coords)
2028 
2029         # silently drop the conflicted variables.
2030         attached_coords = {k: v for k, v in coords.items() if k not in self._variables}
2031         attached_indexes = {
2032             k: v for k, v in indexes.items() if k not in self._variables
2033         }
2034         return attached_coords, attached_indexes
2035 
2036     def isel(
2037         self,
2038         indexers: Mapping[Hashable, Any] = None,
2039         drop: bool = False,
2040         missing_dims: str = "raise",
2041         **indexers_kwargs: Any,
2042     ) -> "Dataset":
2043         """Returns a new dataset with each array indexed along the specified
2044         dimension(s).
2045 
2046         This method selects values from each array using its `__getitem__`
2047         method, except this method does not require knowing the order of
2048         each array's dimensions.
2049 
2050         Parameters
2051         ----------
2052         indexers : dict, optional
2053             A dict with keys matching dimensions and values given
2054             by integers, slice objects or arrays.
2055             indexer can be a integer, slice, array-like or DataArray.
2056             If DataArrays are passed as indexers, xarray-style indexing will be
2057             carried out. See :ref:`indexing` for the details.
2058             One of indexers or indexers_kwargs must be provided.
2059         drop : bool, optional
2060             If ``drop=True``, drop coordinates variables indexed by integers
2061             instead of making them scalar.
2062         missing_dims : {"raise", "warn", "ignore"}, default: "raise"
2063             What to do if dimensions that should be selected from are not present in the
2064             Dataset:
2065             - "raise": raise an exception
2066             - "warning": raise a warning, and ignore the missing dimensions
2067             - "ignore": ignore the missing dimensions
2068         **indexers_kwargs : {dim: indexer, ...}, optional
2069             The keyword arguments form of ``indexers``.
2070             One of indexers or indexers_kwargs must be provided.
2071 
2072         Returns
2073         -------
2074         obj : Dataset
2075             A new Dataset with the same contents as this dataset, except each
2076             array and dimension is indexed by the appropriate indexers.
2077             If indexer DataArrays have coordinates that do not conflict with
2078             this object, then these coordinates will be attached.
2079             In general, each array's data will be a view of the array's data
2080             in this dataset, unless vectorized indexing was triggered by using
2081             an array indexer, in which case the data will be a copy.
2082 
2083         See Also
2084         --------
2085         Dataset.sel
2086         DataArray.isel
2087         """
2088         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "isel")
2089         if any(is_fancy_indexer(idx) for idx in indexers.values()):
2090             return self._isel_fancy(indexers, drop=drop, missing_dims=missing_dims)
2091 
2092         # Much faster algorithm for when all indexers are ints, slices, one-dimensional
2093         # lists, or zero or one-dimensional np.ndarray's
2094         indexers = drop_dims_from_indexers(indexers, self.dims, missing_dims)
2095 
2096         variables = {}
2097         dims: Dict[Hashable, Tuple[int, ...]] = {}
2098         coord_names = self._coord_names.copy()
2099         indexes = self._indexes.copy() if self._indexes is not None else None
2100 
2101         for var_name, var_value in self._variables.items():
2102             var_indexers = {k: v for k, v in indexers.items() if k in var_value.dims}
2103             if var_indexers:
2104                 var_value = var_value.isel(var_indexers)
2105                 if drop and var_value.ndim == 0 and var_name in coord_names:
2106                     coord_names.remove(var_name)
2107                     if indexes:
2108                         indexes.pop(var_name, None)
2109                     continue
2110                 if indexes and var_name in indexes:
2111                     if var_value.ndim == 1:
2112                         indexes[var_name] = var_value.to_index()
2113                     else:
2114                         del indexes[var_name]
2115             variables[var_name] = var_value
2116             dims.update(zip(var_value.dims, var_value.shape))
2117 
2118         return self._construct_direct(
2119             variables=variables,
2120             coord_names=coord_names,
2121             dims=dims,
2122             attrs=self._attrs,
2123             indexes=indexes,
2124             encoding=self._encoding,
2125             file_obj=self._file_obj,
2126         )
2127 
2128     def _isel_fancy(
2129         self,
2130         indexers: Mapping[Hashable, Any],
2131         *,
2132         drop: bool,
2133         missing_dims: str = "raise",
2134     ) -> "Dataset":
2135         # Note: we need to preserve the original indexers variable in order to merge the
2136         # coords below
2137         indexers_list = list(self._validate_indexers(indexers, missing_dims))
2138 
2139         variables: Dict[Hashable, Variable] = {}
2140         indexes: Dict[Hashable, pd.Index] = {}
2141 
2142         for name, var in self.variables.items():
2143             var_indexers = {k: v for k, v in indexers_list if k in var.dims}
2144             if drop and name in var_indexers:
2145                 continue  # drop this variable
2146 
2147             if name in self.indexes:
2148                 new_var, new_index = isel_variable_and_index(
2149                     name, var, self.indexes[name], var_indexers
2150                 )
2151                 if new_index is not None:
2152                     indexes[name] = new_index
2153             elif var_indexers:
2154                 new_var = var.isel(indexers=var_indexers)
2155             else:
2156                 new_var = var.copy(deep=False)
2157 
2158             variables[name] = new_var
2159 
2160         coord_names = self._coord_names & variables.keys()
2161         selected = self._replace_with_new_dims(variables, coord_names, indexes)
2162 
2163         # Extract coordinates from indexers
2164         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(indexers)
2165         variables.update(coord_vars)
2166         indexes.update(new_indexes)
2167         coord_names = self._coord_names & variables.keys() | coord_vars.keys()
2168         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2169 
2170     def sel(
2171         self,
2172         indexers: Mapping[Hashable, Any] = None,
2173         method: str = None,
2174         tolerance: Number = None,
2175         drop: bool = False,
2176         **indexers_kwargs: Any,
2177     ) -> "Dataset":
2178         """Returns a new dataset with each array indexed by tick labels
2179         along the specified dimension(s).
2180 
2181         In contrast to `Dataset.isel`, indexers for this method should use
2182         labels instead of integers.
2183 
2184         Under the hood, this method is powered by using pandas's powerful Index
2185         objects. This makes label based indexing essentially just as fast as
2186         using integer indexing.
2187 
2188         It also means this method uses pandas's (well documented) logic for
2189         indexing. This means you can use string shortcuts for datetime indexes
2190         (e.g., '2000-01' to select all values in January 2000). It also means
2191         that slices are treated as inclusive of both the start and stop values,
2192         unlike normal Python indexing.
2193 
2194         Parameters
2195         ----------
2196         indexers : dict, optional
2197             A dict with keys matching dimensions and values given
2198             by scalars, slices or arrays of tick labels. For dimensions with
2199             multi-index, the indexer may also be a dict-like object with keys
2200             matching index level names.
2201             If DataArrays are passed as indexers, xarray-style indexing will be
2202             carried out. See :ref:`indexing` for the details.
2203             One of indexers or indexers_kwargs must be provided.
2204         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2205             Method to use for inexact matches:
2206 
2207             * None (default): only exact matches
2208             * pad / ffill: propagate last valid index value forward
2209             * backfill / bfill: propagate next valid index value backward
2210             * nearest: use nearest valid index value
2211         tolerance : optional
2212             Maximum distance between original and new labels for inexact
2213             matches. The values of the index at the matching locations must
2214             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2215         drop : bool, optional
2216             If ``drop=True``, drop coordinates variables in `indexers` instead
2217             of making them scalar.
2218         **indexers_kwargs : {dim: indexer, ...}, optional
2219             The keyword arguments form of ``indexers``.
2220             One of indexers or indexers_kwargs must be provided.
2221 
2222         Returns
2223         -------
2224         obj : Dataset
2225             A new Dataset with the same contents as this dataset, except each
2226             variable and dimension is indexed by the appropriate indexers.
2227             If indexer DataArrays have coordinates that do not conflict with
2228             this object, then these coordinates will be attached.
2229             In general, each array's data will be a view of the array's data
2230             in this dataset, unless vectorized indexing was triggered by using
2231             an array indexer, in which case the data will be a copy.
2232 
2233 
2234         See Also
2235         --------
2236         Dataset.isel
2237         DataArray.sel
2238         """
2239         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "sel")
2240         pos_indexers, new_indexes = remap_label_indexers(
2241             self, indexers=indexers, method=method, tolerance=tolerance
2242         )
2243         result = self.isel(indexers=pos_indexers, drop=drop)
2244         return result._overwrite_indexes(new_indexes)
2245 
2246     def head(
2247         self,
2248         indexers: Union[Mapping[Hashable, int], int] = None,
2249         **indexers_kwargs: Any,
2250     ) -> "Dataset":
2251         """Returns a new dataset with the first `n` values of each array
2252         for the specified dimension(s).
2253 
2254         Parameters
2255         ----------
2256         indexers : dict or int, default: 5
2257             A dict with keys matching dimensions and integer values `n`
2258             or a single integer `n` applied over all dimensions.
2259             One of indexers or indexers_kwargs must be provided.
2260         **indexers_kwargs : {dim: n, ...}, optional
2261             The keyword arguments form of ``indexers``.
2262             One of indexers or indexers_kwargs must be provided.
2263 
2264 
2265         See Also
2266         --------
2267         Dataset.tail
2268         Dataset.thin
2269         DataArray.head
2270         """
2271         if not indexers_kwargs:
2272             if indexers is None:
2273                 indexers = 5
2274             if not isinstance(indexers, int) and not is_dict_like(indexers):
2275                 raise TypeError("indexers must be either dict-like or a single integer")
2276         if isinstance(indexers, int):
2277             indexers = {dim: indexers for dim in self.dims}
2278         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "head")
2279         for k, v in indexers.items():
2280             if not isinstance(v, int):
2281                 raise TypeError(
2282                     "expected integer type indexer for "
2283                     "dimension %r, found %r" % (k, type(v))
2284                 )
2285             elif v < 0:
2286                 raise ValueError(
2287                     "expected positive integer as indexer "
2288                     "for dimension %r, found %s" % (k, v)
2289                 )
2290         indexers_slices = {k: slice(val) for k, val in indexers.items()}
2291         return self.isel(indexers_slices)
2292 
2293     def tail(
2294         self,
2295         indexers: Union[Mapping[Hashable, int], int] = None,
2296         **indexers_kwargs: Any,
2297     ) -> "Dataset":
2298         """Returns a new dataset with the last `n` values of each array
2299         for the specified dimension(s).
2300 
2301         Parameters
2302         ----------
2303         indexers : dict or int, default: 5
2304             A dict with keys matching dimensions and integer values `n`
2305             or a single integer `n` applied over all dimensions.
2306             One of indexers or indexers_kwargs must be provided.
2307         **indexers_kwargs : {dim: n, ...}, optional
2308             The keyword arguments form of ``indexers``.
2309             One of indexers or indexers_kwargs must be provided.
2310 
2311 
2312         See Also
2313         --------
2314         Dataset.head
2315         Dataset.thin
2316         DataArray.tail
2317         """
2318         if not indexers_kwargs:
2319             if indexers is None:
2320                 indexers = 5
2321             if not isinstance(indexers, int) and not is_dict_like(indexers):
2322                 raise TypeError("indexers must be either dict-like or a single integer")
2323         if isinstance(indexers, int):
2324             indexers = {dim: indexers for dim in self.dims}
2325         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "tail")
2326         for k, v in indexers.items():
2327             if not isinstance(v, int):
2328                 raise TypeError(
2329                     "expected integer type indexer for "
2330                     "dimension %r, found %r" % (k, type(v))
2331                 )
2332             elif v < 0:
2333                 raise ValueError(
2334                     "expected positive integer as indexer "
2335                     "for dimension %r, found %s" % (k, v)
2336                 )
2337         indexers_slices = {
2338             k: slice(-val, None) if val != 0 else slice(val)
2339             for k, val in indexers.items()
2340         }
2341         return self.isel(indexers_slices)
2342 
2343     def thin(
2344         self,
2345         indexers: Union[Mapping[Hashable, int], int] = None,
2346         **indexers_kwargs: Any,
2347     ) -> "Dataset":
2348         """Returns a new dataset with each array indexed along every `n`-th
2349         value for the specified dimension(s)
2350 
2351         Parameters
2352         ----------
2353         indexers : dict or int
2354             A dict with keys matching dimensions and integer values `n`
2355             or a single integer `n` applied over all dimensions.
2356             One of indexers or indexers_kwargs must be provided.
2357         **indexers_kwargs : {dim: n, ...}, optional
2358             The keyword arguments form of ``indexers``.
2359             One of indexers or indexers_kwargs must be provided.
2360 
2361 
2362         See Also
2363         --------
2364         Dataset.head
2365         Dataset.tail
2366         DataArray.thin
2367         """
2368         if (
2369             not indexers_kwargs
2370             and not isinstance(indexers, int)
2371             and not is_dict_like(indexers)
2372         ):
2373             raise TypeError("indexers must be either dict-like or a single integer")
2374         if isinstance(indexers, int):
2375             indexers = {dim: indexers for dim in self.dims}
2376         indexers = either_dict_or_kwargs(indexers, indexers_kwargs, "thin")
2377         for k, v in indexers.items():
2378             if not isinstance(v, int):
2379                 raise TypeError(
2380                     "expected integer type indexer for "
2381                     "dimension %r, found %r" % (k, type(v))
2382                 )
2383             elif v < 0:
2384                 raise ValueError(
2385                     "expected positive integer as indexer "
2386                     "for dimension %r, found %s" % (k, v)
2387                 )
2388             elif v == 0:
2389                 raise ValueError("step cannot be zero")
2390         indexers_slices = {k: slice(None, None, val) for k, val in indexers.items()}
2391         return self.isel(indexers_slices)
2392 
2393     def broadcast_like(
2394         self, other: Union["Dataset", "DataArray"], exclude: Iterable[Hashable] = None
2395     ) -> "Dataset":
2396         """Broadcast this DataArray against another Dataset or DataArray.
2397         This is equivalent to xr.broadcast(other, self)[1]
2398 
2399         Parameters
2400         ----------
2401         other : Dataset or DataArray
2402             Object against which to broadcast this array.
2403         exclude : iterable of hashable, optional
2404             Dimensions that must not be broadcasted
2405 
2406         """
2407         if exclude is None:
2408             exclude = set()
2409         else:
2410             exclude = set(exclude)
2411         args = align(other, self, join="outer", copy=False, exclude=exclude)
2412 
2413         dims_map, common_coords = _get_broadcast_dims_map_common_coords(args, exclude)
2414 
2415         return _broadcast_helper(args[1], exclude, dims_map, common_coords)
2416 
2417     def reindex_like(
2418         self,
2419         other: Union["Dataset", "DataArray"],
2420         method: str = None,
2421         tolerance: Number = None,
2422         copy: bool = True,
2423         fill_value: Any = dtypes.NA,
2424     ) -> "Dataset":
2425         """Conform this object onto the indexes of another object, filling in
2426         missing values with ``fill_value``. The default fill value is NaN.
2427 
2428         Parameters
2429         ----------
2430         other : Dataset or DataArray
2431             Object with an 'indexes' attribute giving a mapping from dimension
2432             names to pandas.Index objects, which provides coordinates upon
2433             which to index the variables in this dataset. The indexes on this
2434             other object need not be the same as the indexes on this
2435             dataset. Any mis-matched index values will be filled in with
2436             NaN, and any mis-matched dimension names will simply be ignored.
2437         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2438             Method to use for filling index values from other not found in this
2439             dataset:
2440 
2441             * None (default): don't fill gaps
2442             * pad / ffill: propagate last valid index value forward
2443             * backfill / bfill: propagate next valid index value backward
2444             * nearest: use nearest valid index value
2445         tolerance : optional
2446             Maximum distance between original and new labels for inexact
2447             matches. The values of the index at the matching locations must
2448             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2449         copy : bool, optional
2450             If ``copy=True``, data in the return value is always copied. If
2451             ``copy=False`` and reindexing is unnecessary, or can be performed
2452             with only slice operations, then the output may share memory with
2453             the input. In either case, a new xarray object is always returned.
2454         fill_value : scalar or dict-like, optional
2455             Value to use for newly missing values. If a dict-like maps
2456             variable names to fill values.
2457 
2458         Returns
2459         -------
2460         reindexed : Dataset
2461             Another dataset, with this dataset's data but coordinates from the
2462             other object.
2463 
2464         See Also
2465         --------
2466         Dataset.reindex
2467         align
2468         """
2469         indexers = alignment.reindex_like_indexers(self, other)
2470         return self.reindex(
2471             indexers=indexers,
2472             method=method,
2473             copy=copy,
2474             fill_value=fill_value,
2475             tolerance=tolerance,
2476         )
2477 
2478     def reindex(
2479         self,
2480         indexers: Mapping[Hashable, Any] = None,
2481         method: str = None,
2482         tolerance: Number = None,
2483         copy: bool = True,
2484         fill_value: Any = dtypes.NA,
2485         **indexers_kwargs: Any,
2486     ) -> "Dataset":
2487         """Conform this object onto a new set of indexes, filling in
2488         missing values with ``fill_value``. The default fill value is NaN.
2489 
2490         Parameters
2491         ----------
2492         indexers : dict, optional
2493             Dictionary with keys given by dimension names and values given by
2494             arrays of coordinates tick labels. Any mis-matched coordinate
2495             values will be filled in with NaN, and any mis-matched dimension
2496             names will simply be ignored.
2497             One of indexers or indexers_kwargs must be provided.
2498         method : {None, "nearest", "pad", "ffill", "backfill", "bfill"}, optional
2499             Method to use for filling index values in ``indexers`` not found in
2500             this dataset:
2501 
2502             * None (default): don't fill gaps
2503             * pad / ffill: propagate last valid index value forward
2504             * backfill / bfill: propagate next valid index value backward
2505             * nearest: use nearest valid index value
2506         tolerance : optional
2507             Maximum distance between original and new labels for inexact
2508             matches. The values of the index at the matching locations must
2509             satisfy the equation ``abs(index[indexer] - target) <= tolerance``.
2510         copy : bool, optional
2511             If ``copy=True``, data in the return value is always copied. If
2512             ``copy=False`` and reindexing is unnecessary, or can be performed
2513             with only slice operations, then the output may share memory with
2514             the input. In either case, a new xarray object is always returned.
2515         fill_value : scalar or dict-like, optional
2516             Value to use for newly missing values. If a dict-like,
2517             maps variable names (including coordinates) to fill values.
2518         sparse : bool, default: False
2519             use sparse-array.
2520         **indexers_kwargs : {dim: indexer, ...}, optional
2521             Keyword arguments in the same form as ``indexers``.
2522             One of indexers or indexers_kwargs must be provided.
2523 
2524         Returns
2525         -------
2526         reindexed : Dataset
2527             Another dataset, with this dataset's data but replaced coordinates.
2528 
2529         See Also
2530         --------
2531         Dataset.reindex_like
2532         align
2533         pandas.Index.get_indexer
2534 
2535         Examples
2536         --------
2537 
2538         Create a dataset with some fictional data.
2539 
2540         >>> import xarray as xr
2541         >>> import pandas as pd
2542         >>> x = xr.Dataset(
2543         ...     {
2544         ...         "temperature": ("station", 20 * np.random.rand(4)),
2545         ...         "pressure": ("station", 500 * np.random.rand(4)),
2546         ...     },
2547         ...     coords={"station": ["boston", "nyc", "seattle", "denver"]},
2548         ... )
2549         >>> x
2550         <xarray.Dataset>
2551         Dimensions:      (station: 4)
2552         Coordinates:
2553           * station      (station) <U7 'boston' 'nyc' 'seattle' 'denver'
2554         Data variables:
2555             temperature  (station) float64 10.98 14.3 12.06 10.9
2556             pressure     (station) float64 211.8 322.9 218.8 445.9
2557         >>> x.indexes
2558         station: Index(['boston', 'nyc', 'seattle', 'denver'], dtype='object', name='station')
2559 
2560         Create a new index and reindex the dataset. By default values in the new index that
2561         do not have corresponding records in the dataset are assigned `NaN`.
2562 
2563         >>> new_index = ["boston", "austin", "seattle", "lincoln"]
2564         >>> x.reindex({"station": new_index})
2565         <xarray.Dataset>
2566         Dimensions:      (station: 4)
2567         Coordinates:
2568           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2569         Data variables:
2570             temperature  (station) float64 10.98 nan 12.06 nan
2571             pressure     (station) float64 211.8 nan 218.8 nan
2572 
2573         We can fill in the missing values by passing a value to the keyword `fill_value`.
2574 
2575         >>> x.reindex({"station": new_index}, fill_value=0)
2576         <xarray.Dataset>
2577         Dimensions:      (station: 4)
2578         Coordinates:
2579           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2580         Data variables:
2581             temperature  (station) float64 10.98 0.0 12.06 0.0
2582             pressure     (station) float64 211.8 0.0 218.8 0.0
2583 
2584         We can also use different fill values for each variable.
2585 
2586         >>> x.reindex(
2587         ...     {"station": new_index}, fill_value={"temperature": 0, "pressure": 100}
2588         ... )
2589         <xarray.Dataset>
2590         Dimensions:      (station: 4)
2591         Coordinates:
2592           * station      (station) <U7 'boston' 'austin' 'seattle' 'lincoln'
2593         Data variables:
2594             temperature  (station) float64 10.98 0.0 12.06 0.0
2595             pressure     (station) float64 211.8 100.0 218.8 100.0
2596 
2597         Because the index is not monotonically increasing or decreasing, we cannot use arguments
2598         to the keyword method to fill the `NaN` values.
2599 
2600         >>> x.reindex({"station": new_index}, method="nearest")
2601         Traceback (most recent call last):
2602         ...
2603             raise ValueError('index must be monotonic increasing or decreasing')
2604         ValueError: index must be monotonic increasing or decreasing
2605 
2606         To further illustrate the filling functionality in reindex, we will create a
2607         dataset with a monotonically increasing index (for example, a sequence of dates).
2608 
2609         >>> x2 = xr.Dataset(
2610         ...     {
2611         ...         "temperature": (
2612         ...             "time",
2613         ...             [15.57, 12.77, np.nan, 0.3081, 16.59, 15.12],
2614         ...         ),
2615         ...         "pressure": ("time", 500 * np.random.rand(6)),
2616         ...     },
2617         ...     coords={"time": pd.date_range("01/01/2019", periods=6, freq="D")},
2618         ... )
2619         >>> x2
2620         <xarray.Dataset>
2621         Dimensions:      (time: 6)
2622         Coordinates:
2623           * time         (time) datetime64[ns] 2019-01-01 2019-01-02 ... 2019-01-06
2624         Data variables:
2625             temperature  (time) float64 15.57 12.77 nan 0.3081 16.59 15.12
2626             pressure     (time) float64 481.8 191.7 395.9 264.4 284.0 462.8
2627 
2628         Suppose we decide to expand the dataset to cover a wider date range.
2629 
2630         >>> time_index2 = pd.date_range("12/29/2018", periods=10, freq="D")
2631         >>> x2.reindex({"time": time_index2})
2632         <xarray.Dataset>
2633         Dimensions:      (time: 10)
2634         Coordinates:
2635           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2636         Data variables:
2637             temperature  (time) float64 nan nan nan 15.57 ... 0.3081 16.59 15.12 nan
2638             pressure     (time) float64 nan nan nan 481.8 ... 264.4 284.0 462.8 nan
2639 
2640         The index entries that did not have a value in the original data frame (for example, `2018-12-29`)
2641         are by default filled with NaN. If desired, we can fill in the missing values using one of several options.
2642 
2643         For example, to back-propagate the last valid value to fill the `NaN` values,
2644         pass `bfill` as an argument to the `method` keyword.
2645 
2646         >>> x3 = x2.reindex({"time": time_index2}, method="bfill")
2647         >>> x3
2648         <xarray.Dataset>
2649         Dimensions:      (time: 10)
2650         Coordinates:
2651           * time         (time) datetime64[ns] 2018-12-29 2018-12-30 ... 2019-01-07
2652         Data variables:
2653             temperature  (time) float64 15.57 15.57 15.57 15.57 ... 16.59 15.12 nan
2654             pressure     (time) float64 481.8 481.8 481.8 481.8 ... 284.0 462.8 nan
2655 
2656         Please note that the `NaN` value present in the original dataset (at index value `2019-01-03`)
2657         will not be filled by any of the value propagation schemes.
2658 
2659         >>> x2.where(x2.temperature.isnull(), drop=True)
2660         <xarray.Dataset>
2661         Dimensions:      (time: 1)
2662         Coordinates:
2663           * time         (time) datetime64[ns] 2019-01-03
2664         Data variables:
2665             temperature  (time) float64 nan
2666             pressure     (time) float64 395.9
2667         >>> x3.where(x3.temperature.isnull(), drop=True)
2668         <xarray.Dataset>
2669         Dimensions:      (time: 2)
2670         Coordinates:
2671           * time         (time) datetime64[ns] 2019-01-03 2019-01-07
2672         Data variables:
2673             temperature  (time) float64 nan nan
2674             pressure     (time) float64 395.9 nan
2675 
2676         This is because filling while reindexing does not look at dataset values, but only compares
2677         the original and desired indexes. If you do want to fill in the `NaN` values present in the
2678         original dataset, use the :py:meth:`~Dataset.fillna()` method.
2679 
2680         """
2681         return self._reindex(
2682             indexers,
2683             method,
2684             tolerance,
2685             copy,
2686             fill_value,
2687             sparse=False,
2688             **indexers_kwargs,
2689         )
2690 
2691     def _reindex(
2692         self,
2693         indexers: Mapping[Hashable, Any] = None,
2694         method: str = None,
2695         tolerance: Number = None,
2696         copy: bool = True,
2697         fill_value: Any = dtypes.NA,
2698         sparse: bool = False,
2699         **indexers_kwargs: Any,
2700     ) -> "Dataset":
2701         """
2702         same to _reindex but support sparse option
2703         """
2704         indexers = utils.either_dict_or_kwargs(indexers, indexers_kwargs, "reindex")
2705 
2706         bad_dims = [d for d in indexers if d not in self.dims]
2707         if bad_dims:
2708             raise ValueError("invalid reindex dimensions: %s" % bad_dims)
2709 
2710         variables, indexes = alignment.reindex_variables(
2711             self.variables,
2712             self.sizes,
2713             self.indexes,
2714             indexers,
2715             method,
2716             tolerance,
2717             copy=copy,
2718             fill_value=fill_value,
2719             sparse=sparse,
2720         )
2721         coord_names = set(self._coord_names)
2722         coord_names.update(indexers)
2723         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2724 
2725     def interp(
2726         self,
2727         coords: Mapping[Hashable, Any] = None,
2728         method: str = "linear",
2729         assume_sorted: bool = False,
2730         kwargs: Mapping[str, Any] = None,
2731         **coords_kwargs: Any,
2732     ) -> "Dataset":
2733         """Multidimensional interpolation of Dataset.
2734 
2735         Parameters
2736         ----------
2737         coords : dict, optional
2738             Mapping from dimension names to the new coordinates.
2739             New coordinate can be a scalar, array-like or DataArray.
2740             If DataArrays are passed as new coordinates, their dimensions are
2741             used for the broadcasting. Missing values are skipped.
2742         method : str, optional
2743             {"linear", "nearest"} for multidimensional array,
2744             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2745             for 1-dimensional array. "linear" is used by default.
2746         assume_sorted : bool, optional
2747             If False, values of coordinates that are interpolated over can be
2748             in any order and they are sorted first. If True, interpolated
2749             coordinates are assumed to be an array of monotonically increasing
2750             values.
2751         kwargs: dict, optional
2752             Additional keyword arguments passed to scipy's interpolator. Valid
2753             options and their behavior depend on if 1-dimensional or
2754             multi-dimensional interpolation is used.
2755         **coords_kwargs : {dim: coordinate, ...}, optional
2756             The keyword arguments form of ``coords``.
2757             One of coords or coords_kwargs must be provided.
2758 
2759         Returns
2760         -------
2761         interpolated : Dataset
2762             New dataset on the new coordinates.
2763 
2764         Notes
2765         -----
2766         scipy is required.
2767 
2768         See Also
2769         --------
2770         scipy.interpolate.interp1d
2771         scipy.interpolate.interpn
2772 
2773         Examples
2774         --------
2775         >>> ds = xr.Dataset(
2776         ...     data_vars={
2777         ...         "a": ("x", [5, 7, 4]),
2778         ...         "b": (
2779         ...             ("x", "y"),
2780         ...             [[1, 4, 2, 9], [2, 7, 6, np.nan], [6, np.nan, 5, 8]],
2781         ...         ),
2782         ...     },
2783         ...     coords={"x": [0, 1, 2], "y": [10, 12, 14, 16]},
2784         ... )
2785         >>> ds
2786         <xarray.Dataset>
2787         Dimensions:  (x: 3, y: 4)
2788         Coordinates:
2789           * x        (x) int64 0 1 2
2790           * y        (y) int64 10 12 14 16
2791         Data variables:
2792             a        (x) int64 5 7 4
2793             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 6.0 nan 6.0 nan 5.0 8.0
2794 
2795         1D interpolation with the default method (linear):
2796 
2797         >>> ds.interp(x=[0, 0.75, 1.25, 1.75])
2798         <xarray.Dataset>
2799         Dimensions:  (x: 4, y: 4)
2800         Coordinates:
2801           * y        (y) int64 10 12 14 16
2802           * x        (x) float64 0.0 0.75 1.25 1.75
2803         Data variables:
2804             a        (x) float64 5.0 6.5 6.25 4.75
2805             b        (x, y) float64 1.0 4.0 2.0 nan 1.75 6.25 ... nan 5.0 nan 5.25 nan
2806 
2807         1D interpolation with a different method:
2808 
2809         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], method="nearest")
2810         <xarray.Dataset>
2811         Dimensions:  (x: 4, y: 4)
2812         Coordinates:
2813           * y        (y) int64 10 12 14 16
2814           * x        (x) float64 0.0 0.75 1.25 1.75
2815         Data variables:
2816             a        (x) float64 5.0 7.0 7.0 4.0
2817             b        (x, y) float64 1.0 4.0 2.0 9.0 2.0 7.0 ... 6.0 nan 6.0 nan 5.0 8.0
2818 
2819         1D extrapolation:
2820 
2821         >>> ds.interp(
2822         ...     x=[1, 1.5, 2.5, 3.5],
2823         ...     method="linear",
2824         ...     kwargs={"fill_value": "extrapolate"},
2825         ... )
2826         <xarray.Dataset>
2827         Dimensions:  (x: 4, y: 4)
2828         Coordinates:
2829           * y        (y) int64 10 12 14 16
2830           * x        (x) float64 1.0 1.5 2.5 3.5
2831         Data variables:
2832             a        (x) float64 7.0 5.5 2.5 -0.5
2833             b        (x, y) float64 2.0 7.0 6.0 nan 4.0 nan ... 4.5 nan 12.0 nan 3.5 nan
2834 
2835         2D interpolation:
2836 
2837         >>> ds.interp(x=[0, 0.75, 1.25, 1.75], y=[11, 13, 15], method="linear")
2838         <xarray.Dataset>
2839         Dimensions:  (x: 4, y: 3)
2840         Coordinates:
2841           * x        (x) float64 0.0 0.75 1.25 1.75
2842           * y        (y) int64 11 13 15
2843         Data variables:
2844             a        (x) float64 5.0 6.5 6.25 4.75
2845             b        (x, y) float64 2.5 3.0 nan 4.0 5.625 nan nan nan nan nan nan nan
2846         """
2847         from . import missing
2848 
2849         if kwargs is None:
2850             kwargs = {}
2851 
2852         coords = either_dict_or_kwargs(coords, coords_kwargs, "interp")
2853         indexers = dict(self._validate_interp_indexers(coords))
2854 
2855         if coords:
2856             # This avoids broadcasting over coordinates that are both in
2857             # the original array AND in the indexing array. It essentially
2858             # forces interpolation along the shared coordinates.
2859             sdims = (
2860                 set(self.dims)
2861                 .intersection(*[set(nx.dims) for nx in indexers.values()])
2862                 .difference(coords.keys())
2863             )
2864             indexers.update({d: self.variables[d] for d in sdims})
2865 
2866         obj = self if assume_sorted else self.sortby([k for k in coords])
2867 
2868         def maybe_variable(obj, k):
2869             # workaround to get variable for dimension without coordinate.
2870             try:
2871                 return obj._variables[k]
2872             except KeyError:
2873                 return as_variable((k, range(obj.dims[k])))
2874 
2875         def _validate_interp_indexer(x, new_x):
2876             # In the case of datetimes, the restrictions placed on indexers
2877             # used with interp are stronger than those which are placed on
2878             # isel, so we need an additional check after _validate_indexers.
2879             if _contains_datetime_like_objects(
2880                 x
2881             ) and not _contains_datetime_like_objects(new_x):
2882                 raise TypeError(
2883                     "When interpolating over a datetime-like "
2884                     "coordinate, the coordinates to "
2885                     "interpolate to must be either datetime "
2886                     "strings or datetimes. "
2887                     "Instead got\n{}".format(new_x)
2888                 )
2889             return x, new_x
2890 
2891         variables: Dict[Hashable, Variable] = {}
2892         for name, var in obj._variables.items():
2893             if name in indexers:
2894                 continue
2895 
2896             if var.dtype.kind in "uifc":
2897                 var_indexers = {
2898                     k: _validate_interp_indexer(maybe_variable(obj, k), v)
2899                     for k, v in indexers.items()
2900                     if k in var.dims
2901                 }
2902                 variables[name] = missing.interp(var, var_indexers, method, **kwargs)
2903             elif all(d not in indexers for d in var.dims):
2904                 # keep unrelated object array
2905                 variables[name] = var
2906 
2907         coord_names = obj._coord_names & variables.keys()
2908         indexes = {k: v for k, v in obj.indexes.items() if k not in indexers}
2909         selected = self._replace_with_new_dims(
2910             variables.copy(), coord_names, indexes=indexes
2911         )
2912 
2913         # attach indexer as coordinate
2914         variables.update(indexers)
2915         for k, v in indexers.items():
2916             assert isinstance(v, Variable)
2917             if v.dims == (k,):
2918                 indexes[k] = v.to_index()
2919 
2920         # Extract coordinates from indexers
2921         coord_vars, new_indexes = selected._get_indexers_coords_and_indexes(coords)
2922         variables.update(coord_vars)
2923         indexes.update(new_indexes)
2924 
2925         coord_names = obj._coord_names & variables.keys() | coord_vars.keys()
2926         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
2927 
2928     def interp_like(
2929         self,
2930         other: Union["Dataset", "DataArray"],
2931         method: str = "linear",
2932         assume_sorted: bool = False,
2933         kwargs: Mapping[str, Any] = None,
2934     ) -> "Dataset":
2935         """Interpolate this object onto the coordinates of another object,
2936         filling the out of range values with NaN.
2937 
2938         Parameters
2939         ----------
2940         other : Dataset or DataArray
2941             Object with an 'indexes' attribute giving a mapping from dimension
2942             names to an 1d array-like, which provides coordinates upon
2943             which to index the variables in this dataset. Missing values are skipped.
2944         method : str, optional
2945             {"linear", "nearest"} for multidimensional array,
2946             {"linear", "nearest", "zero", "slinear", "quadratic", "cubic"}
2947             for 1-dimensional array. 'linear' is used by default.
2948         assume_sorted : bool, optional
2949             If False, values of coordinates that are interpolated over can be
2950             in any order and they are sorted first. If True, interpolated
2951             coordinates are assumed to be an array of monotonically increasing
2952             values.
2953         kwargs: dict, optional
2954             Additional keyword passed to scipy's interpolator.
2955 
2956         Returns
2957         -------
2958         interpolated : Dataset
2959             Another dataset by interpolating this dataset's data along the
2960             coordinates of the other object.
2961 
2962         Notes
2963         -----
2964         scipy is required.
2965         If the dataset has object-type coordinates, reindex is used for these
2966         coordinates instead of the interpolation.
2967 
2968         See Also
2969         --------
2970         Dataset.interp
2971         Dataset.reindex_like
2972         """
2973         if kwargs is None:
2974             kwargs = {}
2975         coords = alignment.reindex_like_indexers(self, other)
2976 
2977         numeric_coords: Dict[Hashable, pd.Index] = {}
2978         object_coords: Dict[Hashable, pd.Index] = {}
2979         for k, v in coords.items():
2980             if v.dtype.kind in "uifcMm":
2981                 numeric_coords[k] = v
2982             else:
2983                 object_coords[k] = v
2984 
2985         ds = self
2986         if object_coords:
2987             # We do not support interpolation along object coordinate.
2988             # reindex instead.
2989             ds = self.reindex(object_coords)
2990         return ds.interp(numeric_coords, method, assume_sorted, kwargs)
2991 
2992     # Helper methods for rename()
2993     def _rename_vars(self, name_dict, dims_dict):
2994         variables = {}
2995         coord_names = set()
2996         for k, v in self.variables.items():
2997             var = v.copy(deep=False)
2998             var.dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
2999             name = name_dict.get(k, k)
3000             if name in variables:
3001                 raise ValueError(f"the new name {name!r} conflicts")
3002             variables[name] = var
3003             if k in self._coord_names:
3004                 coord_names.add(name)
3005         return variables, coord_names
3006 
3007     def _rename_dims(self, name_dict):
3008         return {name_dict.get(k, k): v for k, v in self.dims.items()}
3009 
3010     def _rename_indexes(self, name_dict, dims_set):
3011         if self._indexes is None:
3012             return None
3013         indexes = {}
3014         for k, v in self.indexes.items():
3015             new_name = name_dict.get(k, k)
3016             if new_name not in dims_set:
3017                 continue
3018             if isinstance(v, pd.MultiIndex):
3019                 new_names = [name_dict.get(k, k) for k in v.names]
3020                 index = v.rename(names=new_names)
3021             else:
3022                 index = v.rename(new_name)
3023             indexes[new_name] = index
3024         return indexes
3025 
3026     def _rename_all(self, name_dict, dims_dict):
3027         variables, coord_names = self._rename_vars(name_dict, dims_dict)
3028         dims = self._rename_dims(dims_dict)
3029         indexes = self._rename_indexes(name_dict, dims.keys())
3030         return variables, coord_names, dims, indexes
3031 
3032     def rename(
3033         self,
3034         name_dict: Mapping[Hashable, Hashable] = None,
3035         **names: Hashable,
3036     ) -> "Dataset":
3037         """Returns a new object with renamed variables and dimensions.
3038 
3039         Parameters
3040         ----------
3041         name_dict : dict-like, optional
3042             Dictionary whose keys are current variable or dimension names and
3043             whose values are the desired names.
3044         **names : optional
3045             Keyword form of ``name_dict``.
3046             One of name_dict or names must be provided.
3047 
3048         Returns
3049         -------
3050         renamed : Dataset
3051             Dataset with renamed variables and dimensions.
3052 
3053         See Also
3054         --------
3055         Dataset.swap_dims
3056         Dataset.rename_vars
3057         Dataset.rename_dims
3058         DataArray.rename
3059         """
3060         name_dict = either_dict_or_kwargs(name_dict, names, "rename")
3061         for k in name_dict.keys():
3062             if k not in self and k not in self.dims:
3063                 raise ValueError(
3064                     "cannot rename %r because it is not a "
3065                     "variable or dimension in this dataset" % k
3066                 )
3067 
3068         variables, coord_names, dims, indexes = self._rename_all(
3069             name_dict=name_dict, dims_dict=name_dict
3070         )
3071         assert_unique_multiindex_level_names(variables)
3072         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3073 
3074     def rename_dims(
3075         self, dims_dict: Mapping[Hashable, Hashable] = None, **dims: Hashable
3076     ) -> "Dataset":
3077         """Returns a new object with renamed dimensions only.
3078 
3079         Parameters
3080         ----------
3081         dims_dict : dict-like, optional
3082             Dictionary whose keys are current dimension names and
3083             whose values are the desired names. The desired names must
3084             not be the name of an existing dimension or Variable in the Dataset.
3085         **dims : optional
3086             Keyword form of ``dims_dict``.
3087             One of dims_dict or dims must be provided.
3088 
3089         Returns
3090         -------
3091         renamed : Dataset
3092             Dataset with renamed dimensions.
3093 
3094         See Also
3095         --------
3096         Dataset.swap_dims
3097         Dataset.rename
3098         Dataset.rename_vars
3099         DataArray.rename
3100         """
3101         dims_dict = either_dict_or_kwargs(dims_dict, dims, "rename_dims")
3102         for k, v in dims_dict.items():
3103             if k not in self.dims:
3104                 raise ValueError(
3105                     "cannot rename %r because it is not a "
3106                     "dimension in this dataset" % k
3107                 )
3108             if v in self.dims or v in self:
3109                 raise ValueError(
3110                     f"Cannot rename {k} to {v} because {v} already exists. "
3111                     "Try using swap_dims instead."
3112                 )
3113 
3114         variables, coord_names, sizes, indexes = self._rename_all(
3115             name_dict={}, dims_dict=dims_dict
3116         )
3117         return self._replace(variables, coord_names, dims=sizes, indexes=indexes)
3118 
3119     def rename_vars(
3120         self, name_dict: Mapping[Hashable, Hashable] = None, **names: Hashable
3121     ) -> "Dataset":
3122         """Returns a new object with renamed variables including coordinates
3123 
3124         Parameters
3125         ----------
3126         name_dict : dict-like, optional
3127             Dictionary whose keys are current variable or coordinate names and
3128             whose values are the desired names.
3129         **names : optional
3130             Keyword form of ``name_dict``.
3131             One of name_dict or names must be provided.
3132 
3133         Returns
3134         -------
3135         renamed : Dataset
3136             Dataset with renamed variables including coordinates
3137 
3138         See Also
3139         --------
3140         Dataset.swap_dims
3141         Dataset.rename
3142         Dataset.rename_dims
3143         DataArray.rename
3144         """
3145         name_dict = either_dict_or_kwargs(name_dict, names, "rename_vars")
3146         for k in name_dict:
3147             if k not in self:
3148                 raise ValueError(
3149                     "cannot rename %r because it is not a "
3150                     "variable or coordinate in this dataset" % k
3151                 )
3152         variables, coord_names, dims, indexes = self._rename_all(
3153             name_dict=name_dict, dims_dict={}
3154         )
3155         return self._replace(variables, coord_names, dims=dims, indexes=indexes)
3156 
3157     def swap_dims(self, dims_dict: Mapping[Hashable, Hashable]) -> "Dataset":
3158         """Returns a new object with swapped dimensions.
3159 
3160         Parameters
3161         ----------
3162         dims_dict : dict-like
3163             Dictionary whose keys are current dimension names and whose values
3164             are new names.
3165 
3166         Returns
3167         -------
3168         swapped : Dataset
3169             Dataset with swapped dimensions.
3170 
3171         Examples
3172         --------
3173         >>> ds = xr.Dataset(
3174         ...     data_vars={"a": ("x", [5, 7]), "b": ("x", [0.1, 2.4])},
3175         ...     coords={"x": ["a", "b"], "y": ("x", [0, 1])},
3176         ... )
3177         >>> ds
3178         <xarray.Dataset>
3179         Dimensions:  (x: 2)
3180         Coordinates:
3181           * x        (x) <U1 'a' 'b'
3182             y        (x) int64 0 1
3183         Data variables:
3184             a        (x) int64 5 7
3185             b        (x) float64 0.1 2.4
3186 
3187         >>> ds.swap_dims({"x": "y"})
3188         <xarray.Dataset>
3189         Dimensions:  (y: 2)
3190         Coordinates:
3191             x        (y) <U1 'a' 'b'
3192           * y        (y) int64 0 1
3193         Data variables:
3194             a        (y) int64 5 7
3195             b        (y) float64 0.1 2.4
3196 
3197         >>> ds.swap_dims({"x": "z"})
3198         <xarray.Dataset>
3199         Dimensions:  (z: 2)
3200         Coordinates:
3201             x        (z) <U1 'a' 'b'
3202             y        (z) int64 0 1
3203         Dimensions without coordinates: z
3204         Data variables:
3205             a        (z) int64 5 7
3206             b        (z) float64 0.1 2.4
3207 
3208         See Also
3209         --------
3210 
3211         Dataset.rename
3212         DataArray.swap_dims
3213         """
3214         # TODO: deprecate this method in favor of a (less confusing)
3215         # rename_dims() method that only renames dimensions.
3216         for k, v in dims_dict.items():
3217             if k not in self.dims:
3218                 raise ValueError(
3219                     "cannot swap from dimension %r because it is "
3220                     "not an existing dimension" % k
3221                 )
3222             if v in self.variables and self.variables[v].dims != (k,):
3223                 raise ValueError(
3224                     "replacement dimension %r is not a 1D "
3225                     "variable along the old dimension %r" % (v, k)
3226                 )
3227 
3228         result_dims = {dims_dict.get(dim, dim) for dim in self.dims}
3229 
3230         coord_names = self._coord_names.copy()
3231         coord_names.update({dim for dim in dims_dict.values() if dim in self.variables})
3232 
3233         variables: Dict[Hashable, Variable] = {}
3234         indexes: Dict[Hashable, pd.Index] = {}
3235         for k, v in self.variables.items():
3236             dims = tuple(dims_dict.get(dim, dim) for dim in v.dims)
3237             if k in result_dims:
3238                 var = v.to_index_variable()
3239                 if k in self.indexes:
3240                     indexes[k] = self.indexes[k]
3241                 else:
3242                     new_index = var.to_index()
3243                     if new_index.nlevels == 1:
3244                         # make sure index name matches dimension name
3245                         new_index = new_index.rename(k)
3246                     indexes[k] = new_index
3247             else:
3248                 var = v.to_base_variable()
3249             var.dims = dims
3250             variables[k] = var
3251 
3252         return self._replace_with_new_dims(variables, coord_names, indexes=indexes)
3253 
3254     def expand_dims(
3255         self,
3256         dim: Union[None, Hashable, Sequence[Hashable], Mapping[Hashable, Any]] = None,
3257         axis: Union[None, int, Sequence[int]] = None,
3258         **dim_kwargs: Any,
3259     ) -> "Dataset":
3260         """Return a new object with an additional axis (or axes) inserted at
3261         the corresponding position in the array shape.  The new object is a
3262         view into the underlying array, not a copy.
3263 
3264         If dim is already a scalar coordinate, it will be promoted to a 1D
3265         coordinate consisting of a single value.
3266 
3267         Parameters
3268         ----------
3269         dim : hashable, sequence of hashable, mapping, or None
3270             Dimensions to include on the new variable. If provided as hashable
3271             or sequence of hashable, then dimensions are inserted with length
3272             1. If provided as a mapping, then the keys are the new dimensions
3273             and the values are either integers (giving the length of the new
3274             dimensions) or array-like (giving the coordinates of the new
3275             dimensions).
3276         axis : int, sequence of int, or None
3277             Axis position(s) where new axis is to be inserted (position(s) on
3278             the result array). If a list (or tuple) of integers is passed,
3279             multiple axes are inserted. In this case, dim arguments should be
3280             same length list. If axis=None is passed, all the axes will be
3281             inserted to the start of the result array.
3282         **dim_kwargs : int or sequence or ndarray
3283             The keywords are arbitrary dimensions being inserted and the values
3284             are either the lengths of the new dims (if int is given), or their
3285             coordinates. Note, this is an alternative to passing a dict to the
3286             dim kwarg and will only be used if dim is None.
3287 
3288         Returns
3289         -------
3290         expanded : same type as caller
3291             This object, but with an additional dimension(s).
3292         """
3293         if dim is None:
3294             pass
3295         elif isinstance(dim, Mapping):
3296             # We're later going to modify dim in place; don't tamper with
3297             # the input
3298             dim = dict(dim)
3299         elif isinstance(dim, int):
3300             raise TypeError(
3301                 "dim should be hashable or sequence of hashables or mapping"
3302             )
3303         elif isinstance(dim, str) or not isinstance(dim, Sequence):
3304             dim = {dim: 1}
3305         elif isinstance(dim, Sequence):
3306             if len(dim) != len(set(dim)):
3307                 raise ValueError("dims should not contain duplicate values.")
3308             dim = {d: 1 for d in dim}
3309 
3310         dim = either_dict_or_kwargs(dim, dim_kwargs, "expand_dims")
3311         assert isinstance(dim, MutableMapping)
3312 
3313         if axis is None:
3314             axis = list(range(len(dim)))
3315         elif not isinstance(axis, Sequence):
3316             axis = [axis]
3317 
3318         if len(dim) != len(axis):
3319             raise ValueError("lengths of dim and axis should be identical.")
3320         for d in dim:
3321             if d in self.dims:
3322                 raise ValueError(f"Dimension {d} already exists.")
3323             if d in self._variables and not utils.is_scalar(self._variables[d]):
3324                 raise ValueError(
3325                     "{dim} already exists as coordinate or"
3326                     " variable name.".format(dim=d)
3327                 )
3328 
3329         variables: Dict[Hashable, Variable] = {}
3330         coord_names = self._coord_names.copy()
3331         # If dim is a dict, then ensure that the values are either integers
3332         # or iterables.
3333         for k, v in dim.items():
3334             if hasattr(v, "__iter__"):
3335                 # If the value for the new dimension is an iterable, then
3336                 # save the coordinates to the variables dict, and set the
3337                 # value within the dim dict to the length of the iterable
3338                 # for later use.
3339                 variables[k] = xr.IndexVariable((k,), v)
3340                 coord_names.add(k)
3341                 dim[k] = variables[k].size
3342             elif isinstance(v, int):
3343                 pass  # Do nothing if the dimensions value is just an int
3344             else:
3345                 raise TypeError(
3346                     "The value of new dimension {k} must be "
3347                     "an iterable or an int".format(k=k)
3348                 )
3349 
3350         for k, v in self._variables.items():
3351             if k not in dim:
3352                 if k in coord_names:  # Do not change coordinates
3353                     variables[k] = v
3354                 else:
3355                     result_ndim = len(v.dims) + len(axis)
3356                     for a in axis:
3357                         if a < -result_ndim or result_ndim - 1 < a:
3358                             raise IndexError(
3359                                 f"Axis {a} of variable {k} is out of bounds of the "
3360                                 f"expanded dimension size {result_ndim}"
3361                             )
3362 
3363                     axis_pos = [a if a >= 0 else result_ndim + a for a in axis]
3364                     if len(axis_pos) != len(set(axis_pos)):
3365                         raise ValueError("axis should not contain duplicate values")
3366                     # We need to sort them to make sure `axis` equals to the
3367                     # axis positions of the result array.
3368                     zip_axis_dim = sorted(zip(axis_pos, dim.items()))
3369 
3370                     all_dims = list(zip(v.dims, v.shape))
3371                     for d, c in zip_axis_dim:
3372                         all_dims.insert(d, c)
3373                     variables[k] = v.set_dims(dict(all_dims))
3374             else:
3375                 # If dims includes a label of a non-dimension coordinate,
3376                 # it will be promoted to a 1D coordinate with a single value.
3377                 variables[k] = v.set_dims(k).to_index_variable()
3378 
3379         new_dims = self._dims.copy()
3380         new_dims.update(dim)
3381 
3382         return self._replace_vars_and_dims(
3383             variables, dims=new_dims, coord_names=coord_names
3384         )
3385 
3386     def set_index(
3387         self,
3388         indexes: Mapping[Hashable, Union[Hashable, Sequence[Hashable]]] = None,
3389         append: bool = False,
3390         **indexes_kwargs: Union[Hashable, Sequence[Hashable]],
3391     ) -> "Dataset":
3392         """Set Dataset (multi-)indexes using one or more existing coordinates
3393         or variables.
3394 
3395         Parameters
3396         ----------
3397         indexes : {dim: index, ...}
3398             Mapping from names matching dimensions and values given
3399             by (lists of) the names of existing coordinates or variables to set
3400             as new (multi-)index.
3401         append : bool, optional
3402             If True, append the supplied index(es) to the existing index(es).
3403             Otherwise replace the existing index(es) (default).
3404         **indexes_kwargs : optional
3405             The keyword arguments form of ``indexes``.
3406             One of indexes or indexes_kwargs must be provided.
3407 
3408         Returns
3409         -------
3410         obj : Dataset
3411             Another dataset, with this dataset's data but replaced coordinates.
3412 
3413         Examples
3414         --------
3415         >>> arr = xr.DataArray(
3416         ...     data=np.ones((2, 3)),
3417         ...     dims=["x", "y"],
3418         ...     coords={"x": range(2), "y": range(3), "a": ("x", [3, 4])},
3419         ... )
3420         >>> ds = xr.Dataset({"v": arr})
3421         >>> ds
3422         <xarray.Dataset>
3423         Dimensions:  (x: 2, y: 3)
3424         Coordinates:
3425           * x        (x) int64 0 1
3426           * y        (y) int64 0 1 2
3427             a        (x) int64 3 4
3428         Data variables:
3429             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3430         >>> ds.set_index(x="a")
3431         <xarray.Dataset>
3432         Dimensions:  (x: 2, y: 3)
3433         Coordinates:
3434           * x        (x) int64 3 4
3435           * y        (y) int64 0 1 2
3436         Data variables:
3437             v        (x, y) float64 1.0 1.0 1.0 1.0 1.0 1.0
3438 
3439         See Also
3440         --------
3441         Dataset.reset_index
3442         Dataset.swap_dims
3443         """
3444         indexes = either_dict_or_kwargs(indexes, indexes_kwargs, "set_index")
3445         variables, coord_names = merge_indexes(
3446             indexes, self._variables, self._coord_names, append=append
3447         )
3448         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3449 
3450     def reset_index(
3451         self,
3452         dims_or_levels: Union[Hashable, Sequence[Hashable]],
3453         drop: bool = False,
3454     ) -> "Dataset":
3455         """Reset the specified index(es) or multi-index level(s).
3456 
3457         Parameters
3458         ----------
3459         dims_or_levels : str or list
3460             Name(s) of the dimension(s) and/or multi-index level(s) that will
3461             be reset.
3462         drop : bool, optional
3463             If True, remove the specified indexes and/or multi-index levels
3464             instead of extracting them as new coordinates (default: False).
3465 
3466         Returns
3467         -------
3468         obj : Dataset
3469             Another dataset, with this dataset's data but replaced coordinates.
3470 
3471         See Also
3472         --------
3473         Dataset.set_index
3474         """
3475         variables, coord_names = split_indexes(
3476             dims_or_levels,
3477             self._variables,
3478             self._coord_names,
3479             cast(Mapping[Hashable, Hashable], self._level_coords),
3480             drop=drop,
3481         )
3482         return self._replace_vars_and_dims(variables, coord_names=coord_names)
3483 
3484     def reorder_levels(
3485         self,
3486         dim_order: Mapping[Hashable, Sequence[int]] = None,
3487         **dim_order_kwargs: Sequence[int],
3488     ) -> "Dataset":
3489         """Rearrange index levels using input order.
3490 
3491         Parameters
3492         ----------
3493         dim_order : optional
3494             Mapping from names matching dimensions and values given
3495             by lists representing new level orders. Every given dimension
3496             must have a multi-index.
3497         **dim_order_kwargs : optional
3498             The keyword arguments form of ``dim_order``.
3499             One of dim_order or dim_order_kwargs must be provided.
3500 
3501         Returns
3502         -------
3503         obj : Dataset
3504             Another dataset, with this dataset's data but replaced
3505             coordinates.
3506         """
3507         dim_order = either_dict_or_kwargs(dim_order, dim_order_kwargs, "reorder_levels")
3508         variables = self._variables.copy()
3509         indexes = dict(self.indexes)
3510         for dim, order in dim_order.items():
3511             coord = self._variables[dim]
3512             index = self.indexes[dim]
3513             if not isinstance(index, pd.MultiIndex):
3514                 raise ValueError(f"coordinate {dim} has no MultiIndex")
3515             new_index = index.reorder_levels(order)
3516             variables[dim] = IndexVariable(coord.dims, new_index)
3517             indexes[dim] = new_index
3518 
3519         return self._replace(variables, indexes=indexes)
3520 
3521     def _stack_once(self, dims, new_dim):
3522         if ... in dims:
3523             dims = list(infix_dims(dims, self.dims))
3524         variables = {}
3525         for name, var in self.variables.items():
3526             if name not in dims:
3527                 if any(d in var.dims for d in dims):
3528                     add_dims = [d for d in dims if d not in var.dims]
3529                     vdims = list(var.dims) + add_dims
3530                     shape = [self.dims[d] for d in vdims]
3531                     exp_var = var.set_dims(vdims, shape)
3532                     stacked_var = exp_var.stack(**{new_dim: dims})
3533                     variables[name] = stacked_var
3534                 else:
3535                     variables[name] = var.copy(deep=False)
3536 
3537         # consider dropping levels that are unused?
3538         levels = [self.get_index(dim) for dim in dims]
3539         idx = utils.multiindex_from_product_levels(levels, names=dims)
3540         variables[new_dim] = IndexVariable(new_dim, idx)
3541 
3542         coord_names = set(self._coord_names) - set(dims) | {new_dim}
3543 
3544         indexes = {k: v for k, v in self.indexes.items() if k not in dims}
3545         indexes[new_dim] = idx
3546 
3547         return self._replace_with_new_dims(
3548             variables, coord_names=coord_names, indexes=indexes
3549         )
3550 
3551     def stack(
3552         self,
3553         dimensions: Mapping[Hashable, Sequence[Hashable]] = None,
3554         **dimensions_kwargs: Sequence[Hashable],
3555     ) -> "Dataset":
3556         """
3557         Stack any number of existing dimensions into a single new dimension.
3558 
3559         New dimensions will be added at the end, and the corresponding
3560         coordinate variables will be combined into a MultiIndex.
3561 
3562         Parameters
3563         ----------
3564         dimensions : mapping of hashable to sequence of hashable
3565             Mapping of the form `new_name=(dim1, dim2, ...)`. Names of new
3566             dimensions, and the existing dimensions that they replace. An
3567             ellipsis (`...`) will be replaced by all unlisted dimensions.
3568             Passing a list containing an ellipsis (`stacked_dim=[...]`) will stack over
3569             all dimensions.
3570         **dimensions_kwargs
3571             The keyword arguments form of ``dimensions``.
3572             One of dimensions or dimensions_kwargs must be provided.
3573 
3574         Returns
3575         -------
3576         stacked : Dataset
3577             Dataset with stacked data.
3578 
3579         See also
3580         --------
3581         Dataset.unstack
3582         """
3583         dimensions = either_dict_or_kwargs(dimensions, dimensions_kwargs, "stack")
3584         result = self
3585         for new_dim, dims in dimensions.items():
3586             result = result._stack_once(dims, new_dim)
3587         return result
3588 
3589     def to_stacked_array(
3590         self,
3591         new_dim: Hashable,
3592         sample_dims: Sequence[Hashable],
3593         variable_dim: str = "variable",
3594         name: Hashable = None,
3595     ) -> "DataArray":
3596         """Combine variables of differing dimensionality into a DataArray
3597         without broadcasting.
3598 
3599         This method is similar to Dataset.to_array but does not broadcast the
3600         variables.
3601 
3602         Parameters
3603         ----------
3604         new_dim : hashable
3605             Name of the new stacked coordinate
3606         sample_dims : sequence of hashable
3607             Dimensions that **will not** be stacked. Each array in the dataset
3608             must share these dimensions. For machine learning applications,
3609             these define the dimensions over which samples are drawn.
3610         variable_dim : str, optional
3611             Name of the level in the stacked coordinate which corresponds to
3612             the variables.
3613         name : str, optional
3614             Name of the new data array.
3615 
3616         Returns
3617         -------
3618         stacked : DataArray
3619             DataArray with the specified dimensions and data variables
3620             stacked together. The stacked coordinate is named ``new_dim``
3621             and represented by a MultiIndex object with a level containing the
3622             data variable names. The name of this level is controlled using
3623             the ``variable_dim`` argument.
3624 
3625         See Also
3626         --------
3627         Dataset.to_array
3628         Dataset.stack
3629         DataArray.to_unstacked_dataset
3630 
3631         Examples
3632         --------
3633         >>> data = xr.Dataset(
3634         ...     data_vars={
3635         ...         "a": (("x", "y"), [[0, 1, 2], [3, 4, 5]]),
3636         ...         "b": ("x", [6, 7]),
3637         ...     },
3638         ...     coords={"y": ["u", "v", "w"]},
3639         ... )
3640 
3641         >>> data
3642         <xarray.Dataset>
3643         Dimensions:  (x: 2, y: 3)
3644         Coordinates:
3645           * y        (y) <U1 'u' 'v' 'w'
3646         Dimensions without coordinates: x
3647         Data variables:
3648             a        (x, y) int64 0 1 2 3 4 5
3649             b        (x) int64 6 7
3650 
3651         >>> data.to_stacked_array("z", sample_dims=["x"])
3652         <xarray.DataArray 'a' (x: 2, z: 4)>
3653         array([[0, 1, 2, 6],
3654                [3, 4, 5, 7]])
3655         Coordinates:
3656           * z         (z) MultiIndex
3657           - variable  (z) object 'a' 'a' 'a' 'b'
3658           - y         (z) object 'u' 'v' 'w' nan
3659         Dimensions without coordinates: x
3660 
3661         """
3662         stacking_dims = tuple(dim for dim in self.dims if dim not in sample_dims)
3663 
3664         for variable in self:
3665             dims = self[variable].dims
3666             dims_include_sample_dims = set(sample_dims) <= set(dims)
3667             if not dims_include_sample_dims:
3668                 raise ValueError(
3669                     "All variables in the dataset must contain the "
3670                     "dimensions {}.".format(dims)
3671                 )
3672 
3673         def ensure_stackable(val):
3674             assign_coords = {variable_dim: val.name}
3675             for dim in stacking_dims:
3676                 if dim not in val.dims:
3677                     assign_coords[dim] = None
3678 
3679             expand_dims = set(stacking_dims).difference(set(val.dims))
3680             expand_dims.add(variable_dim)
3681             # must be list for .expand_dims
3682             expand_dims = list(expand_dims)
3683 
3684             return (
3685                 val.assign_coords(**assign_coords)
3686                 .expand_dims(expand_dims)
3687                 .stack({new_dim: (variable_dim,) + stacking_dims})
3688             )
3689 
3690         # concatenate the arrays
3691         stackable_vars = [ensure_stackable(self[key]) for key in self.data_vars]
3692         data_array = xr.concat(stackable_vars, dim=new_dim)
3693 
3694         # coerce the levels of the MultiIndex to have the same type as the
3695         # input dimensions. This code is messy, so it might be better to just
3696         # input a dummy value for the singleton dimension.
3697         idx = data_array.indexes[new_dim]
3698         levels = [idx.levels[0]] + [
3699             level.astype(self[level.name].dtype) for level in idx.levels[1:]
3700         ]
3701         new_idx = idx.set_levels(levels)
3702         data_array[new_dim] = IndexVariable(new_dim, new_idx)
3703 
3704         if name is not None:
3705             data_array.name = name
3706 
3707         return data_array
3708 
3709     def _unstack_once(self, dim: Hashable, fill_value, sparse) -> "Dataset":
3710         index = self.get_index(dim)
3711         index = remove_unused_levels_categories(index)
3712         full_idx = pd.MultiIndex.from_product(index.levels, names=index.names)
3713 
3714         # take a shortcut in case the MultiIndex was not modified.
3715         if index.equals(full_idx):
3716             obj = self
3717         else:
3718             obj = self._reindex(
3719                 {dim: full_idx}, copy=False, fill_value=fill_value, sparse=sparse
3720             )
3721 
3722         new_dim_names = index.names
3723         new_dim_sizes = [lev.size for lev in index.levels]
3724 
3725         variables: Dict[Hashable, Variable] = {}
3726         indexes = {k: v for k, v in self.indexes.items() if k != dim}
3727 
3728         for name, var in obj.variables.items():
3729             if name != dim:
3730                 if dim in var.dims:
3731                     new_dims = dict(zip(new_dim_names, new_dim_sizes))
3732                     variables[name] = var.unstack({dim: new_dims})
3733                 else:
3734                     variables[name] = var
3735 
3736         for name, lev in zip(new_dim_names, index.levels):
3737             variables[name] = IndexVariable(name, lev)
3738             indexes[name] = lev
3739 
3740         coord_names = set(self._coord_names) - {dim} | set(new_dim_names)
3741 
3742         return self._replace_with_new_dims(
3743             variables, coord_names=coord_names, indexes=indexes
3744         )
3745 
3746     def unstack(
3747         self,
3748         dim: Union[Hashable, Iterable[Hashable]] = None,
3749         fill_value: Any = dtypes.NA,
3750         sparse: bool = False,
3751     ) -> "Dataset":
3752         """
3753         Unstack existing dimensions corresponding to MultiIndexes into
3754         multiple new dimensions.
3755 
3756         New dimensions will be added at the end.
3757 
3758         Parameters
3759         ----------
3760         dim : hashable or iterable of hashable, optional
3761             Dimension(s) over which to unstack. By default unstacks all
3762             MultiIndexes.
3763         fill_value : scalar or dict-like, default: nan
3764             value to be filled. If a dict-like, maps variable names to
3765             fill values. If not provided or if the dict-like does not
3766             contain all variables, the dtype's NA value will be used.
3767         sparse : bool, default: False
3768             use sparse-array if True
3769 
3770         Returns
3771         -------
3772         unstacked : Dataset
3773             Dataset with unstacked data.
3774 
3775         See also
3776         --------
3777         Dataset.stack
3778         """
3779         if dim is None:
3780             dims = [
3781                 d for d in self.dims if isinstance(self.get_index(d), pd.MultiIndex)
3782             ]
3783         else:
3784             if isinstance(dim, str) or not isinstance(dim, Iterable):
3785                 dims = [dim]
3786             else:
3787                 dims = list(dim)
3788 
3789             missing_dims = [d for d in dims if d not in self.dims]
3790             if missing_dims:
3791                 raise ValueError(
3792                     "Dataset does not contain the dimensions: %s" % missing_dims
3793                 )
3794 
3795             non_multi_dims = [
3796                 d for d in dims if not isinstance(self.get_index(d), pd.MultiIndex)
3797             ]
3798             if non_multi_dims:
3799                 raise ValueError(
3800                     "cannot unstack dimensions that do not "
3801                     "have a MultiIndex: %s" % non_multi_dims
3802                 )
3803 
3804         result = self.copy(deep=False)
3805         for dim in dims:
3806             result = result._unstack_once(dim, fill_value, sparse)
3807         return result
3808 
3809     def update(self, other: "CoercibleMapping") -> "Dataset":
3810         """Update this dataset's variables with those from another dataset.
3811 
3812         Parameters
3813         ----------
3814         other : Dataset or mapping
3815             Variables with which to update this dataset. One of:
3816 
3817             - Dataset
3818             - mapping {var name: DataArray}
3819             - mapping {var name: Variable}
3820             - mapping {var name: (dimension name, array-like)}
3821             - mapping {var name: (tuple of dimension names, array-like)}
3822 
3823 
3824         Returns
3825         -------
3826         updated : Dataset
3827             Updated dataset.
3828 
3829         Raises
3830         ------
3831         ValueError
3832             If any dimensions would have inconsistent sizes in the updated
3833             dataset.
3834         """
3835         merge_result = dataset_update_method(self, other)
3836         return self._replace(inplace=True, **merge_result._asdict())
3837 
3838     def merge(
3839         self,
3840         other: Union["CoercibleMapping", "DataArray"],
3841         overwrite_vars: Union[Hashable, Iterable[Hashable]] = frozenset(),
3842         compat: str = "no_conflicts",
3843         join: str = "outer",
3844         fill_value: Any = dtypes.NA,
3845     ) -> "Dataset":
3846         """Merge the arrays of two datasets into a single dataset.
3847 
3848         This method generally does not allow for overriding data, with the
3849         exception of attributes, which are ignored on the second dataset.
3850         Variables with the same name are checked for conflicts via the equals
3851         or identical methods.
3852 
3853         Parameters
3854         ----------
3855         other : Dataset or mapping
3856             Dataset or variables to merge with this dataset.
3857         overwrite_vars : hashable or iterable of hashable, optional
3858             If provided, update variables of these name(s) without checking for
3859             conflicts in this dataset.
3860         compat : {"broadcast_equals", "equals", "identical", \
3861                   "no_conflicts"}, optional
3862             String indicating how to compare variables of the same name for
3863             potential conflicts:
3864 
3865             - 'broadcast_equals': all values must be equal when variables are
3866               broadcast against each other to ensure common dimensions.
3867             - 'equals': all values and dimensions must be the same.
3868             - 'identical': all values, dimensions and attributes must be the
3869               same.
3870             - 'no_conflicts': only values which are not null in both datasets
3871               must be equal. The returned dataset then contains the combination
3872               of all non-null values.
3873 
3874         join : {"outer", "inner", "left", "right", "exact"}, optional
3875             Method for joining ``self`` and ``other`` along shared dimensions:
3876 
3877             - 'outer': use the union of the indexes
3878             - 'inner': use the intersection of the indexes
3879             - 'left': use indexes from ``self``
3880             - 'right': use indexes from ``other``
3881             - 'exact': error instead of aligning non-equal indexes
3882         fill_value : scalar or dict-like, optional
3883             Value to use for newly missing values. If a dict-like, maps
3884             variable names (including coordinates) to fill values.
3885 
3886         Returns
3887         -------
3888         merged : Dataset
3889             Merged dataset.
3890 
3891         Raises
3892         ------
3893         MergeError
3894             If any variables conflict (see ``compat``).
3895         """
3896         other = other.to_dataset() if isinstance(other, xr.DataArray) else other
3897         merge_result = dataset_merge_method(
3898             self,
3899             other,
3900             overwrite_vars=overwrite_vars,
3901             compat=compat,
3902             join=join,
3903             fill_value=fill_value,
3904         )
3905         return self._replace(**merge_result._asdict())
3906 
3907     def _assert_all_in_dataset(
3908         self, names: Iterable[Hashable], virtual_okay: bool = False
3909     ) -> None:
3910         bad_names = set(names) - set(self._variables)
3911         if virtual_okay:
3912             bad_names -= self.virtual_variables
3913         if bad_names:
3914             raise ValueError(
3915                 "One or more of the specified variables "
3916                 "cannot be found in this dataset"
3917             )
3918 
3919     def drop_vars(
3920         self, names: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
3921     ) -> "Dataset":
3922         """Drop variables from this dataset.
3923 
3924         Parameters
3925         ----------
3926         names : hashable or iterable of hashable
3927             Name(s) of variables to drop.
3928         errors : {"raise", "ignore"}, optional
3929             If 'raise' (default), raises a ValueError error if any of the variable
3930             passed are not in the dataset. If 'ignore', any given names that are in the
3931             dataset are dropped and no error is raised.
3932 
3933         Returns
3934         -------
3935         dropped : Dataset
3936 
3937         """
3938         # the Iterable check is required for mypy
3939         if is_scalar(names) or not isinstance(names, Iterable):
3940             names = {names}
3941         else:
3942             names = set(names)
3943         if errors == "raise":
3944             self._assert_all_in_dataset(names)
3945 
3946         variables = {k: v for k, v in self._variables.items() if k not in names}
3947         coord_names = {k for k in self._coord_names if k in variables}
3948         indexes = {k: v for k, v in self.indexes.items() if k not in names}
3949         return self._replace_with_new_dims(
3950             variables, coord_names=coord_names, indexes=indexes
3951         )
3952 
3953     def drop(self, labels=None, dim=None, *, errors="raise", **labels_kwargs):
3954         """Backward compatible method based on `drop_vars` and `drop_sel`
3955 
3956         Using either `drop_vars` or `drop_sel` is encouraged
3957 
3958         See Also
3959         --------
3960         Dataset.drop_vars
3961         Dataset.drop_sel
3962         """
3963         if errors not in ["raise", "ignore"]:
3964             raise ValueError('errors must be either "raise" or "ignore"')
3965 
3966         if is_dict_like(labels) and not isinstance(labels, dict):
3967             warnings.warn(
3968                 "dropping coordinates using `drop` is be deprecated; use drop_vars.",
3969                 FutureWarning,
3970                 stacklevel=2,
3971             )
3972             return self.drop_vars(labels, errors=errors)
3973 
3974         if labels_kwargs or isinstance(labels, dict):
3975             if dim is not None:
3976                 raise ValueError("cannot specify dim and dict-like arguments.")
3977             labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
3978 
3979         if dim is None and (is_scalar(labels) or isinstance(labels, Iterable)):
3980             warnings.warn(
3981                 "dropping variables using `drop` will be deprecated; using drop_vars is encouraged.",
3982                 PendingDeprecationWarning,
3983                 stacklevel=2,
3984             )
3985             return self.drop_vars(labels, errors=errors)
3986         if dim is not None:
3987             warnings.warn(
3988                 "dropping labels using list-like labels is deprecated; using "
3989                 "dict-like arguments with `drop_sel`, e.g. `ds.drop_sel(dim=[labels]).",
3990                 DeprecationWarning,
3991                 stacklevel=2,
3992             )
3993             return self.drop_sel({dim: labels}, errors=errors, **labels_kwargs)
3994 
3995         warnings.warn(
3996             "dropping labels using `drop` will be deprecated; using drop_sel is encouraged.",
3997             PendingDeprecationWarning,
3998             stacklevel=2,
3999         )
4000         return self.drop_sel(labels, errors=errors)
4001 
4002     def drop_sel(self, labels=None, *, errors="raise", **labels_kwargs):
4003         """Drop index labels from this dataset.
4004 
4005         Parameters
4006         ----------
4007         labels : mapping of hashable to Any
4008             Index labels to drop
4009         errors : {"raise", "ignore"}, optional
4010             If 'raise' (default), raises a ValueError error if
4011             any of the index labels passed are not
4012             in the dataset. If 'ignore', any given labels that are in the
4013             dataset are dropped and no error is raised.
4014         **labels_kwargs : {dim: label, ...}, optional
4015             The keyword arguments form of ``dim`` and ``labels``
4016 
4017         Returns
4018         -------
4019         dropped : Dataset
4020 
4021         Examples
4022         --------
4023         >>> data = np.random.randn(2, 3)
4024         >>> labels = ["a", "b", "c"]
4025         >>> ds = xr.Dataset({"A": (["x", "y"], data), "y": labels})
4026         >>> ds.drop_sel(y=["a", "c"])
4027         <xarray.Dataset>
4028         Dimensions:  (x: 2, y: 1)
4029         Coordinates:
4030           * y        (y) <U1 'b'
4031         Dimensions without coordinates: x
4032         Data variables:
4033             A        (x, y) float64 0.4002 1.868
4034         >>> ds.drop_sel(y="b")
4035         <xarray.Dataset>
4036         Dimensions:  (x: 2, y: 2)
4037         Coordinates:
4038           * y        (y) <U1 'a' 'c'
4039         Dimensions without coordinates: x
4040         Data variables:
4041             A        (x, y) float64 1.764 0.9787 2.241 -0.9773
4042         """
4043         if errors not in ["raise", "ignore"]:
4044             raise ValueError('errors must be either "raise" or "ignore"')
4045 
4046         labels = either_dict_or_kwargs(labels, labels_kwargs, "drop")
4047 
4048         ds = self
4049         for dim, labels_for_dim in labels.items():
4050             # Don't cast to set, as it would harm performance when labels
4051             # is a large numpy array
4052             if utils.is_scalar(labels_for_dim):
4053                 labels_for_dim = [labels_for_dim]
4054             labels_for_dim = np.asarray(labels_for_dim)
4055             try:
4056                 index = self.indexes[dim]
4057             except KeyError:
4058                 raise ValueError("dimension %r does not have coordinate labels" % dim)
4059             new_index = index.drop(labels_for_dim, errors=errors)
4060             ds = ds.loc[{dim: new_index}]
4061         return ds
4062 
4063     def drop_dims(
4064         self, drop_dims: Union[Hashable, Iterable[Hashable]], *, errors: str = "raise"
4065     ) -> "Dataset":
4066         """Drop dimensions and associated variables from this dataset.
4067 
4068         Parameters
4069         ----------
4070         drop_dims : hashable or iterable of hashable
4071             Dimension or dimensions to drop.
4072         errors : {"raise", "ignore"}, optional
4073             If 'raise' (default), raises a ValueError error if any of the
4074             dimensions passed are not in the dataset. If 'ignore', any given
4075             labels that are in the dataset are dropped and no error is raised.
4076 
4077         Returns
4078         -------
4079         obj : Dataset
4080             The dataset without the given dimensions (or any variables
4081             containing those dimensions)
4082         errors : {"raise", "ignore"}, optional
4083             If 'raise' (default), raises a ValueError error if
4084             any of the dimensions passed are not
4085             in the dataset. If 'ignore', any given dimensions that are in the
4086             dataset are dropped and no error is raised.
4087         """
4088         if errors not in ["raise", "ignore"]:
4089             raise ValueError('errors must be either "raise" or "ignore"')
4090 
4091         if isinstance(drop_dims, str) or not isinstance(drop_dims, Iterable):
4092             drop_dims = {drop_dims}
4093         else:
4094             drop_dims = set(drop_dims)
4095 
4096         if errors == "raise":
4097             missing_dims = drop_dims - set(self.dims)
4098             if missing_dims:
4099                 raise ValueError(
4100                     "Dataset does not contain the dimensions: %s" % missing_dims
4101                 )
4102 
4103         drop_vars = {k for k, v in self._variables.items() if set(v.dims) & drop_dims}
4104         return self.drop_vars(drop_vars)
4105 
4106     def transpose(self, *dims: Hashable) -> "Dataset":
4107         """Return a new Dataset object with all array dimensions transposed.
4108 
4109         Although the order of dimensions on each array will change, the dataset
4110         dimensions themselves will remain in fixed (sorted) order.
4111 
4112         Parameters
4113         ----------
4114         *dims : hashable, optional
4115             By default, reverse the dimensions on each array. Otherwise,
4116             reorder the dimensions to this order.
4117 
4118         Returns
4119         -------
4120         transposed : Dataset
4121             Each array in the dataset (including) coordinates will be
4122             transposed to the given order.
4123 
4124         Notes
4125         -----
4126         This operation returns a view of each array's data. It is
4127         lazy for dask-backed DataArrays but not for numpy-backed DataArrays
4128         -- the data will be fully loaded into memory.
4129 
4130         See Also
4131         --------
4132         numpy.transpose
4133         DataArray.transpose
4134         """
4135         if dims:
4136             if set(dims) ^ set(self.dims) and ... not in dims:
4137                 raise ValueError(
4138                     "arguments to transpose (%s) must be "
4139                     "permuted dataset dimensions (%s)" % (dims, tuple(self.dims))
4140                 )
4141         ds = self.copy()
4142         for name, var in self._variables.items():
4143             var_dims = tuple(dim for dim in dims if dim in (var.dims + (...,)))
4144             ds._variables[name] = var.transpose(*var_dims)
4145         return ds
4146 
4147     def dropna(
4148         self,
4149         dim: Hashable,
4150         how: str = "any",
4151         thresh: int = None,
4152         subset: Iterable[Hashable] = None,
4153     ):
4154         """Returns a new dataset with dropped labels for missing values along
4155         the provided dimension.
4156 
4157         Parameters
4158         ----------
4159         dim : hashable
4160             Dimension along which to drop missing values. Dropping along
4161             multiple dimensions simultaneously is not yet supported.
4162         how : {"any", "all"}, default: "any"
4163             * any : if any NA values are present, drop that label
4164             * all : if all values are NA, drop that label
4165         thresh : int, default: None
4166             If supplied, require this many non-NA values.
4167         subset : iterable of hashable, optional
4168             Which variables to check for missing values. By default, all
4169             variables in the dataset are checked.
4170 
4171         Returns
4172         -------
4173         Dataset
4174         """
4175         # TODO: consider supporting multiple dimensions? Or not, given that
4176         # there are some ugly edge cases, e.g., pandas's dropna differs
4177         # depending on the order of the supplied axes.
4178 
4179         if dim not in self.dims:
4180             raise ValueError("%s must be a single dataset dimension" % dim)
4181 
4182         if subset is None:
4183             subset = iter(self.data_vars)
4184 
4185         count = np.zeros(self.dims[dim], dtype=np.int64)
4186         size = 0
4187 
4188         for k in subset:
4189             array = self._variables[k]
4190             if dim in array.dims:
4191                 dims = [d for d in array.dims if d != dim]
4192                 count += np.asarray(array.count(dims))  # type: ignore
4193                 size += np.prod([self.dims[d] for d in dims])
4194 
4195         if thresh is not None:
4196             mask = count >= thresh
4197         elif how == "any":
4198             mask = count == size
4199         elif how == "all":
4200             mask = count > 0
4201         elif how is not None:
4202             raise ValueError("invalid how option: %s" % how)
4203         else:
4204             raise TypeError("must specify how or thresh")
4205 
4206         return self.isel({dim: mask})
4207 
4208     def fillna(self, value: Any) -> "Dataset":
4209         """Fill missing values in this object.
4210 
4211         This operation follows the normal broadcasting and alignment rules that
4212         xarray uses for binary arithmetic, except the result is aligned to this
4213         object (``join='left'``) instead of aligned to the intersection of
4214         index coordinates (``join='inner'``).
4215 
4216         Parameters
4217         ----------
4218         value : scalar, ndarray, DataArray, dict or Dataset
4219             Used to fill all matching missing values in this dataset's data
4220             variables. Scalars, ndarrays or DataArrays arguments are used to
4221             fill all data with aligned coordinates (for DataArrays).
4222             Dictionaries or datasets match data variables and then align
4223             coordinates if necessary.
4224 
4225         Returns
4226         -------
4227         Dataset
4228 
4229         Examples
4230         --------
4231 
4232         >>> import numpy as np
4233         >>> import xarray as xr
4234         >>> ds = xr.Dataset(
4235         ...     {
4236         ...         "A": ("x", [np.nan, 2, np.nan, 0]),
4237         ...         "B": ("x", [3, 4, np.nan, 1]),
4238         ...         "C": ("x", [np.nan, np.nan, np.nan, 5]),
4239         ...         "D": ("x", [np.nan, 3, np.nan, 4]),
4240         ...     },
4241         ...     coords={"x": [0, 1, 2, 3]},
4242         ... )
4243         >>> ds
4244         <xarray.Dataset>
4245         Dimensions:  (x: 4)
4246         Coordinates:
4247           * x        (x) int64 0 1 2 3
4248         Data variables:
4249             A        (x) float64 nan 2.0 nan 0.0
4250             B        (x) float64 3.0 4.0 nan 1.0
4251             C        (x) float64 nan nan nan 5.0
4252             D        (x) float64 nan 3.0 nan 4.0
4253 
4254         Replace all `NaN` values with 0s.
4255 
4256         >>> ds.fillna(0)
4257         <xarray.Dataset>
4258         Dimensions:  (x: 4)
4259         Coordinates:
4260           * x        (x) int64 0 1 2 3
4261         Data variables:
4262             A        (x) float64 0.0 2.0 0.0 0.0
4263             B        (x) float64 3.0 4.0 0.0 1.0
4264             C        (x) float64 0.0 0.0 0.0 5.0
4265             D        (x) float64 0.0 3.0 0.0 4.0
4266 
4267         Replace all `NaN` elements in column A, B, C, and D, with 0, 1, 2, and 3 respectively.
4268 
4269         >>> values = {"A": 0, "B": 1, "C": 2, "D": 3}
4270         >>> ds.fillna(value=values)
4271         <xarray.Dataset>
4272         Dimensions:  (x: 4)
4273         Coordinates:
4274           * x        (x) int64 0 1 2 3
4275         Data variables:
4276             A        (x) float64 0.0 2.0 0.0 0.0
4277             B        (x) float64 3.0 4.0 1.0 1.0
4278             C        (x) float64 2.0 2.0 2.0 5.0
4279             D        (x) float64 3.0 3.0 3.0 4.0
4280         """
4281         if utils.is_dict_like(value):
4282             value_keys = getattr(value, "data_vars", value).keys()
4283             if not set(value_keys) <= set(self.data_vars.keys()):
4284                 raise ValueError(
4285                     "all variables in the argument to `fillna` "
4286                     "must be contained in the original dataset"
4287                 )
4288         out = ops.fillna(self, value)
4289         return out
4290 
4291     def interpolate_na(
4292         self,
4293         dim: Hashable = None,
4294         method: str = "linear",
4295         limit: int = None,
4296         use_coordinate: Union[bool, Hashable] = True,
4297         max_gap: Union[
4298             int, float, str, pd.Timedelta, np.timedelta64, datetime.timedelta
4299         ] = None,
4300         **kwargs: Any,
4301     ) -> "Dataset":
4302         """Fill in NaNs by interpolating according to different methods.
4303 
4304         Parameters
4305         ----------
4306         dim : str
4307             Specifies the dimension along which to interpolate.
4308 
4309         method : str, optional
4310             String indicating which method to use for interpolation:
4311 
4312             - 'linear': linear interpolation (Default). Additional keyword
4313               arguments are passed to :py:func:`numpy.interp`
4314             - 'nearest', 'zero', 'slinear', 'quadratic', 'cubic', 'polynomial':
4315               are passed to :py:func:`scipy.interpolate.interp1d`. If
4316               ``method='polynomial'``, the ``order`` keyword argument must also be
4317               provided.
4318             - 'barycentric', 'krog', 'pchip', 'spline', 'akima': use their
4319               respective :py:class:`scipy.interpolate` classes.
4320 
4321         use_coordinate : bool, str, default: True
4322             Specifies which index to use as the x values in the interpolation
4323             formulated as `y = f(x)`. If False, values are treated as if
4324             eqaully-spaced along ``dim``. If True, the IndexVariable `dim` is
4325             used. If ``use_coordinate`` is a string, it specifies the name of a
4326             coordinate variariable to use as the index.
4327         limit : int, default: None
4328             Maximum number of consecutive NaNs to fill. Must be greater than 0
4329             or None for no limit. This filling is done regardless of the size of
4330             the gap in the data. To only interpolate over gaps less than a given length,
4331             see ``max_gap``.
4332         max_gap : int, float, str, pandas.Timedelta, numpy.timedelta64, datetime.timedelta, default: None
4333             Maximum size of gap, a continuous sequence of NaNs, that will be filled.
4334             Use None for no limit. When interpolating along a datetime64 dimension
4335             and ``use_coordinate=True``, ``max_gap`` can be one of the following:
4336 
4337             - a string that is valid input for pandas.to_timedelta
4338             - a :py:class:`numpy.timedelta64` object
4339             - a :py:class:`pandas.Timedelta` object
4340             - a :py:class:`datetime.timedelta` object
4341 
4342             Otherwise, ``max_gap`` must be an int or a float. Use of ``max_gap`` with unlabeled
4343             dimensions has not been implemented yet. Gap length is defined as the difference
4344             between coordinate values at the first data point after a gap and the last value
4345             before a gap. For gaps at the beginning (end), gap length is defined as the difference
4346             between coordinate values at the first (last) valid data point and the first (last) NaN.
4347             For example, consider::
4348 
4349                 <xarray.DataArray (x: 9)>
4350                 array([nan, nan, nan,  1., nan, nan,  4., nan, nan])
4351                 Coordinates:
4352                   * x        (x) int64 0 1 2 3 4 5 6 7 8
4353 
4354             The gap lengths are 3-0 = 3; 6-3 = 3; and 8-6 = 2 respectively
4355         kwargs : dict, optional
4356             parameters passed verbatim to the underlying interpolation function
4357 
4358         Returns
4359         -------
4360         interpolated: Dataset
4361             Filled in Dataset.
4362 
4363         See also
4364         --------
4365         numpy.interp
4366         scipy.interpolate
4367 
4368         Examples
4369         --------
4370         >>> ds = xr.Dataset(
4371         ...     {
4372         ...         "A": ("x", [np.nan, 2, 3, np.nan, 0]),
4373         ...         "B": ("x", [3, 4, np.nan, 1, 7]),
4374         ...         "C": ("x", [np.nan, np.nan, np.nan, 5, 0]),
4375         ...         "D": ("x", [np.nan, 3, np.nan, -1, 4]),
4376         ...     },
4377         ...     coords={"x": [0, 1, 2, 3, 4]},
4378         ... )
4379         >>> ds
4380         <xarray.Dataset>
4381         Dimensions:  (x: 5)
4382         Coordinates:
4383           * x        (x) int64 0 1 2 3 4
4384         Data variables:
4385             A        (x) float64 nan 2.0 3.0 nan 0.0
4386             B        (x) float64 3.0 4.0 nan 1.0 7.0
4387             C        (x) float64 nan nan nan 5.0 0.0
4388             D        (x) float64 nan 3.0 nan -1.0 4.0
4389 
4390         >>> ds.interpolate_na(dim="x", method="linear")
4391         <xarray.Dataset>
4392         Dimensions:  (x: 5)
4393         Coordinates:
4394           * x        (x) int64 0 1 2 3 4
4395         Data variables:
4396             A        (x) float64 nan 2.0 3.0 1.5 0.0
4397             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4398             C        (x) float64 nan nan nan 5.0 0.0
4399             D        (x) float64 nan 3.0 1.0 -1.0 4.0
4400 
4401         >>> ds.interpolate_na(dim="x", method="linear", fill_value="extrapolate")
4402         <xarray.Dataset>
4403         Dimensions:  (x: 5)
4404         Coordinates:
4405           * x        (x) int64 0 1 2 3 4
4406         Data variables:
4407             A        (x) float64 1.0 2.0 3.0 1.5 0.0
4408             B        (x) float64 3.0 4.0 2.5 1.0 7.0
4409             C        (x) float64 20.0 15.0 10.0 5.0 0.0
4410             D        (x) float64 5.0 3.0 1.0 -1.0 4.0
4411         """
4412         from .missing import _apply_over_vars_with_dim, interp_na
4413 
4414         new = _apply_over_vars_with_dim(
4415             interp_na,
4416             self,
4417             dim=dim,
4418             method=method,
4419             limit=limit,
4420             use_coordinate=use_coordinate,
4421             max_gap=max_gap,
4422             **kwargs,
4423         )
4424         return new
4425 
4426     def ffill(self, dim: Hashable, limit: int = None) -> "Dataset":
4427         """Fill NaN values by propogating values forward
4428 
4429         *Requires bottleneck.*
4430 
4431         Parameters
4432         ----------
4433         dim : Hashable
4434             Specifies the dimension along which to propagate values when
4435             filling.
4436         limit : int, default: None
4437             The maximum number of consecutive NaN values to forward fill. In
4438             other words, if there is a gap with more than this number of
4439             consecutive NaNs, it will only be partially filled. Must be greater
4440             than 0 or None for no limit.
4441 
4442         Returns
4443         -------
4444         Dataset
4445         """
4446         from .missing import _apply_over_vars_with_dim, ffill
4447 
4448         new = _apply_over_vars_with_dim(ffill, self, dim=dim, limit=limit)
4449         return new
4450 
4451     def bfill(self, dim: Hashable, limit: int = None) -> "Dataset":
4452         """Fill NaN values by propogating values backward
4453 
4454         *Requires bottleneck.*
4455 
4456         Parameters
4457         ----------
4458         dim : str
4459             Specifies the dimension along which to propagate values when
4460             filling.
4461         limit : int, default: None
4462             The maximum number of consecutive NaN values to backward fill. In
4463             other words, if there is a gap with more than this number of
4464             consecutive NaNs, it will only be partially filled. Must be greater
4465             than 0 or None for no limit.
4466 
4467         Returns
4468         -------
4469         Dataset
4470         """
4471         from .missing import _apply_over_vars_with_dim, bfill
4472 
4473         new = _apply_over_vars_with_dim(bfill, self, dim=dim, limit=limit)
4474         return new
4475 
4476     def combine_first(self, other: "Dataset") -> "Dataset":
4477         """Combine two Datasets, default to data_vars of self.
4478 
4479         The new coordinates follow the normal broadcasting and alignment rules
4480         of ``join='outer'``.  Vacant cells in the expanded coordinates are
4481         filled with np.nan.
4482 
4483         Parameters
4484         ----------
4485         other : Dataset
4486             Used to fill all matching missing values in this array.
4487 
4488         Returns
4489         -------
4490         Dataset
4491         """
4492         out = ops.fillna(self, other, join="outer", dataset_join="outer")
4493         return out
4494 
4495     def reduce(
4496         self,
4497         func: Callable,
4498         dim: Union[Hashable, Iterable[Hashable]] = None,
4499         keep_attrs: bool = None,
4500         keepdims: bool = False,
4501         numeric_only: bool = False,
4502         **kwargs: Any,
4503     ) -> "Dataset":
4504         """Reduce this dataset by applying `func` along some dimension(s).
4505 
4506         Parameters
4507         ----------
4508         func : callable
4509             Function which can be called in the form
4510             `f(x, axis=axis, **kwargs)` to return the result of reducing an
4511             np.ndarray over an integer valued axis.
4512         dim : str or sequence of str, optional
4513             Dimension(s) over which to apply `func`.  By default `func` is
4514             applied over all dimensions.
4515         keep_attrs : bool, optional
4516             If True, the dataset's attributes (`attrs`) will be copied from
4517             the original object to the new one.  If False (default), the new
4518             object will be returned without attributes.
4519         keepdims : bool, default: False
4520             If True, the dimensions which are reduced are left in the result
4521             as dimensions of size one. Coordinates that use these dimensions
4522             are removed.
4523         numeric_only : bool, optional
4524             If True, only apply ``func`` to variables with a numeric dtype.
4525         **kwargs : Any
4526             Additional keyword arguments passed on to ``func``.
4527 
4528         Returns
4529         -------
4530         reduced : Dataset
4531             Dataset with this object's DataArrays replaced with new DataArrays
4532             of summarized data and the indicated dimension(s) removed.
4533         """
4534         if dim is None or dim is ...:
4535             dims = set(self.dims)
4536         elif isinstance(dim, str) or not isinstance(dim, Iterable):
4537             dims = {dim}
4538         else:
4539             dims = set(dim)
4540 
4541         missing_dimensions = [d for d in dims if d not in self.dims]
4542         if missing_dimensions:
4543             raise ValueError(
4544                 "Dataset does not contain the dimensions: %s" % missing_dimensions
4545             )
4546 
4547         if keep_attrs is None:
4548             keep_attrs = _get_keep_attrs(default=False)
4549 
4550         variables: Dict[Hashable, Variable] = {}
4551         for name, var in self._variables.items():
4552             reduce_dims = [d for d in var.dims if d in dims]
4553             if name in self.coords:
4554                 if not reduce_dims:
4555                     variables[name] = var
4556             else:
4557                 if (
4558                     not numeric_only
4559                     or np.issubdtype(var.dtype, np.number)
4560                     or (var.dtype == np.bool_)
4561                 ):
4562                     if len(reduce_dims) == 1:
4563                         # unpack dimensions for the benefit of functions
4564                         # like np.argmin which can't handle tuple arguments
4565                         (reduce_dims,) = reduce_dims
4566                     elif len(reduce_dims) == var.ndim:
4567                         # prefer to aggregate over axis=None rather than
4568                         # axis=(0, 1) if they will be equivalent, because
4569                         # the former is often more efficient
4570                         reduce_dims = None  # type: ignore
4571                     variables[name] = var.reduce(
4572                         func,
4573                         dim=reduce_dims,
4574                         keep_attrs=keep_attrs,
4575                         keepdims=keepdims,
4576                         **kwargs,
4577                     )
4578 
4579         coord_names = {k for k in self.coords if k in variables}
4580         indexes = {k: v for k, v in self.indexes.items() if k in variables}
4581         attrs = self.attrs if keep_attrs else None
4582         return self._replace_with_new_dims(
4583             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
4584         )
4585 
4586     def map(
4587         self,
4588         func: Callable,
4589         keep_attrs: bool = None,
4590         args: Iterable[Any] = (),
4591         **kwargs: Any,
4592     ) -> "Dataset":
4593         """Apply a function to each variable in this dataset
4594 
4595         Parameters
4596         ----------
4597         func : callable
4598             Function which can be called in the form `func(x, *args, **kwargs)`
4599             to transform each DataArray `x` in this dataset into another
4600             DataArray.
4601         keep_attrs : bool, optional
4602             If True, the dataset's attributes (`attrs`) will be copied from
4603             the original object to the new one. If False, the new object will
4604             be returned without attributes.
4605         args : tuple, optional
4606             Positional arguments passed on to `func`.
4607         **kwargs : Any
4608             Keyword arguments passed on to `func`.
4609 
4610         Returns
4611         -------
4612         applied : Dataset
4613             Resulting dataset from applying ``func`` to each data variable.
4614 
4615         Examples
4616         --------
4617         >>> da = xr.DataArray(np.random.randn(2, 3))
4618         >>> ds = xr.Dataset({"foo": da, "bar": ("x", [-1, 2])})
4619         >>> ds
4620         <xarray.Dataset>
4621         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4622         Dimensions without coordinates: dim_0, dim_1, x
4623         Data variables:
4624             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 -0.9773
4625             bar      (x) int64 -1 2
4626         >>> ds.map(np.fabs)
4627         <xarray.Dataset>
4628         Dimensions:  (dim_0: 2, dim_1: 3, x: 2)
4629         Dimensions without coordinates: dim_0, dim_1, x
4630         Data variables:
4631             foo      (dim_0, dim_1) float64 1.764 0.4002 0.9787 2.241 1.868 0.9773
4632             bar      (x) float64 1.0 2.0
4633         """
4634         if keep_attrs is None:
4635             keep_attrs = _get_keep_attrs(default=False)
4636         variables = {
4637             k: maybe_wrap_array(v, func(v, *args, **kwargs))
4638             for k, v in self.data_vars.items()
4639         }
4640         if keep_attrs:
4641             for k, v in variables.items():
4642                 v._copy_attrs_from(self.data_vars[k])
4643         attrs = self.attrs if keep_attrs else None
4644         return type(self)(variables, attrs=attrs)
4645 
4646     def apply(
4647         self,
4648         func: Callable,
4649         keep_attrs: bool = None,
4650         args: Iterable[Any] = (),
4651         **kwargs: Any,
4652     ) -> "Dataset":
4653         """
4654         Backward compatible implementation of ``map``
4655 
4656         See Also
4657         --------
4658         Dataset.map
4659         """
4660         warnings.warn(
4661             "Dataset.apply may be deprecated in the future. Using Dataset.map is encouraged",
4662             PendingDeprecationWarning,
4663             stacklevel=2,
4664         )
4665         return self.map(func, keep_attrs, args, **kwargs)
4666 
4667     def assign(
4668         self, variables: Mapping[Hashable, Any] = None, **variables_kwargs: Hashable
4669     ) -> "Dataset":
4670         """Assign new data variables to a Dataset, returning a new object
4671         with all the original variables in addition to the new ones.
4672 
4673         Parameters
4674         ----------
4675         variables : mapping of hashable to Any
4676             Mapping from variables names to the new values. If the new values
4677             are callable, they are computed on the Dataset and assigned to new
4678             data variables. If the values are not callable, (e.g. a DataArray,
4679             scalar, or array), they are simply assigned.
4680         **variables_kwargs
4681             The keyword arguments form of ``variables``.
4682             One of variables or variables_kwargs must be provided.
4683 
4684         Returns
4685         -------
4686         ds : Dataset
4687             A new Dataset with the new variables in addition to all the
4688             existing variables.
4689 
4690         Notes
4691         -----
4692         Since ``kwargs`` is a dictionary, the order of your arguments may not
4693         be preserved, and so the order of the new variables is not well
4694         defined. Assigning multiple variables within the same ``assign`` is
4695         possible, but you cannot reference other variables created within the
4696         same ``assign`` call.
4697 
4698         See Also
4699         --------
4700         pandas.DataFrame.assign
4701 
4702         Examples
4703         --------
4704         >>> x = xr.Dataset(
4705         ...     {
4706         ...         "temperature_c": (
4707         ...             ("lat", "lon"),
4708         ...             20 * np.random.rand(4).reshape(2, 2),
4709         ...         ),
4710         ...         "precipitation": (("lat", "lon"), np.random.rand(4).reshape(2, 2)),
4711         ...     },
4712         ...     coords={"lat": [10, 20], "lon": [150, 160]},
4713         ... )
4714         >>> x
4715         <xarray.Dataset>
4716         Dimensions:        (lat: 2, lon: 2)
4717         Coordinates:
4718           * lat            (lat) int64 10 20
4719           * lon            (lon) int64 150 160
4720         Data variables:
4721             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4722             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4723 
4724         Where the value is a callable, evaluated on dataset:
4725 
4726         >>> x.assign(temperature_f=lambda x: x.temperature_c * 9 / 5 + 32)
4727         <xarray.Dataset>
4728         Dimensions:        (lat: 2, lon: 2)
4729         Coordinates:
4730           * lat            (lat) int64 10 20
4731           * lon            (lon) int64 150 160
4732         Data variables:
4733             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4734             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4735             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4736 
4737         Alternatively, the same behavior can be achieved by directly referencing an existing dataarray:
4738 
4739         >>> x.assign(temperature_f=x["temperature_c"] * 9 / 5 + 32)
4740         <xarray.Dataset>
4741         Dimensions:        (lat: 2, lon: 2)
4742         Coordinates:
4743           * lat            (lat) int64 10 20
4744           * lon            (lon) int64 150 160
4745         Data variables:
4746             temperature_c  (lat, lon) float64 10.98 14.3 12.06 10.9
4747             precipitation  (lat, lon) float64 0.4237 0.6459 0.4376 0.8918
4748             temperature_f  (lat, lon) float64 51.76 57.75 53.7 51.62
4749 
4750         """
4751         variables = either_dict_or_kwargs(variables, variables_kwargs, "assign")
4752         data = self.copy()
4753         # do all calculations first...
4754         results = data._calc_assign_results(variables)
4755         # ... and then assign
4756         data.update(results)
4757         return data
4758 
4759     def to_array(self, dim="variable", name=None):
4760         """Convert this dataset into an xarray.DataArray
4761 
4762         The data variables of this dataset will be broadcast against each other
4763         and stacked along the first axis of the new array. All coordinates of
4764         this dataset will remain coordinates.
4765 
4766         Parameters
4767         ----------
4768         dim : str, optional
4769             Name of the new dimension.
4770         name : str, optional
4771             Name of the new data array.
4772 
4773         Returns
4774         -------
4775         array : xarray.DataArray
4776         """
4777         from .dataarray import DataArray
4778 
4779         data_vars = [self.variables[k] for k in self.data_vars]
4780         broadcast_vars = broadcast_variables(*data_vars)
4781         data = duck_array_ops.stack([b.data for b in broadcast_vars], axis=0)
4782 
4783         coords = dict(self.coords)
4784         coords[dim] = list(self.data_vars)
4785         indexes = propagate_indexes(self._indexes)
4786 
4787         dims = (dim,) + broadcast_vars[0].dims
4788 
4789         return DataArray(
4790             data, coords, dims, attrs=self.attrs, name=name, indexes=indexes
4791         )
4792 
4793     def _normalize_dim_order(
4794         self, dim_order: List[Hashable] = None
4795     ) -> Dict[Hashable, int]:
4796         """
4797         Check the validity of the provided dimensions if any and return the mapping
4798         between dimension name and their size.
4799 
4800         Parameters
4801         ----------
4802         dim_order
4803             Dimension order to validate (default to the alphabetical order if None).
4804 
4805         Returns
4806         -------
4807         result
4808             Validated dimensions mapping.
4809 
4810         """
4811         if dim_order is None:
4812             dim_order = list(self.dims)
4813         elif set(dim_order) != set(self.dims):
4814             raise ValueError(
4815                 "dim_order {} does not match the set of dimensions of this "
4816                 "Dataset: {}".format(dim_order, list(self.dims))
4817             )
4818 
4819         ordered_dims = {k: self.dims[k] for k in dim_order}
4820 
4821         return ordered_dims
4822 
4823     def _to_dataframe(self, ordered_dims: Mapping[Hashable, int]):
4824         columns = [k for k in self.variables if k not in self.dims]
4825         data = [
4826             self._variables[k].set_dims(ordered_dims).values.reshape(-1)
4827             for k in columns
4828         ]
4829         index = self.coords.to_index([*ordered_dims])
4830         return pd.DataFrame(dict(zip(columns, data)), index=index)
4831 
4832     def to_dataframe(self, dim_order: List[Hashable] = None) -> pd.DataFrame:
4833         """Convert this dataset into a pandas.DataFrame.
4834 
4835         Non-index variables in this dataset form the columns of the
4836         DataFrame. The DataFrame is indexed by the Cartesian product of
4837         this dataset's indices.
4838 
4839         Parameters
4840         ----------
4841         dim_order
4842             Hierarchical dimension order for the resulting dataframe. All
4843             arrays are transposed to this order and then written out as flat
4844             vectors in contiguous order, so the last dimension in this list
4845             will be contiguous in the resulting DataFrame. This has a major
4846             influence on which operations are efficient on the resulting
4847             dataframe.
4848 
4849             If provided, must include all dimensions of this dataset. By
4850             default, dimensions are sorted alphabetically.
4851 
4852         Returns
4853         -------
4854         result
4855             Dataset as a pandas DataFrame.
4856 
4857         """
4858 
4859         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
4860 
4861         return self._to_dataframe(ordered_dims=ordered_dims)
4862 
4863     def _set_sparse_data_from_dataframe(
4864         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
4865     ) -> None:
4866         from sparse import COO
4867 
4868         if isinstance(idx, pd.MultiIndex):
4869             coords = np.stack([np.asarray(code) for code in idx.codes], axis=0)
4870             is_sorted = idx.is_lexsorted()
4871             shape = tuple(lev.size for lev in idx.levels)
4872         else:
4873             coords = np.arange(idx.size).reshape(1, -1)
4874             is_sorted = True
4875             shape = (idx.size,)
4876 
4877         for name, values in arrays:
4878             # In virtually all real use cases, the sparse array will now have
4879             # missing values and needs a fill_value. For consistency, don't
4880             # special case the rare exceptions (e.g., dtype=int without a
4881             # MultiIndex).
4882             dtype, fill_value = dtypes.maybe_promote(values.dtype)
4883             values = np.asarray(values, dtype=dtype)
4884 
4885             data = COO(
4886                 coords,
4887                 values,
4888                 shape,
4889                 has_duplicates=False,
4890                 sorted=is_sorted,
4891                 fill_value=fill_value,
4892             )
4893             self[name] = (dims, data)
4894 
4895     def _set_numpy_data_from_dataframe(
4896         self, idx: pd.Index, arrays: List[Tuple[Hashable, np.ndarray]], dims: tuple
4897     ) -> None:
4898         if not isinstance(idx, pd.MultiIndex):
4899             for name, values in arrays:
4900                 self[name] = (dims, values)
4901             return
4902 
4903         shape = tuple(lev.size for lev in idx.levels)
4904         indexer = tuple(idx.codes)
4905 
4906         # We already verified that the MultiIndex has all unique values, so
4907         # there are missing values if and only if the size of output arrays is
4908         # larger that the index.
4909         missing_values = np.prod(shape) > idx.shape[0]
4910 
4911         for name, values in arrays:
4912             # NumPy indexing is much faster than using DataFrame.reindex() to
4913             # fill in missing values:
4914             # https://stackoverflow.com/a/35049899/809705
4915             if missing_values:
4916                 dtype, fill_value = dtypes.maybe_promote(values.dtype)
4917                 data = np.full(shape, fill_value, dtype)
4918             else:
4919                 # If there are no missing values, keep the existing dtype
4920                 # instead of promoting to support NA, e.g., keep integer
4921                 # columns as integers.
4922                 # TODO: consider removing this special case, which doesn't
4923                 # exist for sparse=True.
4924                 data = np.zeros(shape, values.dtype)
4925             data[indexer] = values
4926             self[name] = (dims, data)
4927 
4928     @classmethod
4929     def from_dataframe(cls, dataframe: pd.DataFrame, sparse: bool = False) -> "Dataset":
4930         """Convert a pandas.DataFrame into an xarray.Dataset
4931 
4932         Each column will be converted into an independent variable in the
4933         Dataset. If the dataframe's index is a MultiIndex, it will be expanded
4934         into a tensor product of one-dimensional indices (filling in missing
4935         values with NaN). This method will produce a Dataset very similar to
4936         that on which the 'to_dataframe' method was called, except with
4937         possibly redundant dimensions (since all dataset variables will have
4938         the same dimensionality)
4939 
4940         Parameters
4941         ----------
4942         dataframe : DataFrame
4943             DataFrame from which to copy data and indices.
4944         sparse : bool, default: False
4945             If true, create a sparse arrays instead of dense numpy arrays. This
4946             can potentially save a large amount of memory if the DataFrame has
4947             a MultiIndex. Requires the sparse package (sparse.pydata.org).
4948 
4949         Returns
4950         -------
4951         New Dataset.
4952 
4953         See also
4954         --------
4955         xarray.DataArray.from_series
4956         pandas.DataFrame.to_xarray
4957         """
4958         # TODO: Add an option to remove dimensions along which the variables
4959         # are constant, to enable consistent serialization to/from a dataframe,
4960         # even if some variables have different dimensionality.
4961 
4962         if not dataframe.columns.is_unique:
4963             raise ValueError("cannot convert DataFrame with non-unique columns")
4964 
4965         idx = remove_unused_levels_categories(dataframe.index)
4966 
4967         if isinstance(idx, pd.MultiIndex) and not idx.is_unique:
4968             raise ValueError(
4969                 "cannot convert a DataFrame with a non-unique MultiIndex into xarray"
4970             )
4971 
4972         # Cast to a NumPy array first, in case the Series is a pandas Extension
4973         # array (which doesn't have a valid NumPy dtype)
4974         # TODO: allow users to control how this casting happens, e.g., by
4975         # forwarding arguments to pandas.Series.to_numpy?
4976         arrays = [(k, np.asarray(v)) for k, v in dataframe.items()]
4977 
4978         obj = cls()
4979 
4980         if isinstance(idx, pd.MultiIndex):
4981             dims = tuple(
4982                 name if name is not None else "level_%i" % n
4983                 for n, name in enumerate(idx.names)
4984             )
4985             for dim, lev in zip(dims, idx.levels):
4986                 obj[dim] = (dim, lev)
4987         else:
4988             index_name = idx.name if idx.name is not None else "index"
4989             dims = (index_name,)
4990             obj[index_name] = (dims, idx)
4991 
4992         if sparse:
4993             obj._set_sparse_data_from_dataframe(idx, arrays, dims)
4994         else:
4995             obj._set_numpy_data_from_dataframe(idx, arrays, dims)
4996         return obj
4997 
4998     def to_dask_dataframe(self, dim_order=None, set_index=False):
4999         """
5000         Convert this dataset into a dask.dataframe.DataFrame.
5001 
5002         The dimensions, coordinates and data variables in this dataset form
5003         the columns of the DataFrame.
5004 
5005         Parameters
5006         ----------
5007         dim_order : list, optional
5008             Hierarchical dimension order for the resulting dataframe. All
5009             arrays are transposed to this order and then written out as flat
5010             vectors in contiguous order, so the last dimension in this list
5011             will be contiguous in the resulting DataFrame. This has a major
5012             influence on which operations are efficient on the resulting dask
5013             dataframe.
5014 
5015             If provided, must include all dimensions of this dataset. By
5016             default, dimensions are sorted alphabetically.
5017         set_index : bool, optional
5018             If set_index=True, the dask DataFrame is indexed by this dataset's
5019             coordinate. Since dask DataFrames do not support multi-indexes,
5020             set_index only works if the dataset only contains one dimension.
5021 
5022         Returns
5023         -------
5024         dask.dataframe.DataFrame
5025         """
5026 
5027         import dask.array as da
5028         import dask.dataframe as dd
5029 
5030         ordered_dims = self._normalize_dim_order(dim_order=dim_order)
5031 
5032         columns = list(ordered_dims)
5033         columns.extend(k for k in self.coords if k not in self.dims)
5034         columns.extend(self.data_vars)
5035 
5036         series_list = []
5037         for name in columns:
5038             try:
5039                 var = self.variables[name]
5040             except KeyError:
5041                 # dimension without a matching coordinate
5042                 size = self.dims[name]
5043                 data = da.arange(size, chunks=size, dtype=np.int64)
5044                 var = Variable((name,), data)
5045 
5046             # IndexVariable objects have a dummy .chunk() method
5047             if isinstance(var, IndexVariable):
5048                 var = var.to_base_variable()
5049 
5050             dask_array = var.set_dims(ordered_dims).chunk(self.chunks).data
5051             series = dd.from_array(dask_array.reshape(-1), columns=[name])
5052             series_list.append(series)
5053 
5054         df = dd.concat(series_list, axis=1)
5055 
5056         if set_index:
5057             dim_order = [*ordered_dims]
5058 
5059             if len(dim_order) == 1:
5060                 (dim,) = dim_order
5061                 df = df.set_index(dim)
5062             else:
5063                 # triggers an error about multi-indexes, even if only one
5064                 # dimension is passed
5065                 df = df.set_index(dim_order)
5066 
5067         return df
5068 
5069     def to_dict(self, data=True):
5070         """
5071         Convert this dataset to a dictionary following xarray naming
5072         conventions.
5073 
5074         Converts all variables and attributes to native Python objects
5075         Useful for converting to json. To avoid datetime incompatibility
5076         use decode_times=False kwarg in xarrray.open_dataset.
5077 
5078         Parameters
5079         ----------
5080         data : bool, optional
5081             Whether to include the actual data in the dictionary. When set to
5082             False, returns just the schema.
5083 
5084         See also
5085         --------
5086         Dataset.from_dict
5087         """
5088         d = {
5089             "coords": {},
5090             "attrs": decode_numpy_dict_values(self.attrs),
5091             "dims": dict(self.dims),
5092             "data_vars": {},
5093         }
5094         for k in self.coords:
5095             d["coords"].update({k: self[k].variable.to_dict(data=data)})
5096         for k in self.data_vars:
5097             d["data_vars"].update({k: self[k].variable.to_dict(data=data)})
5098         return d
5099 
5100     @classmethod
5101     def from_dict(cls, d):
5102         """
5103         Convert a dictionary into an xarray.Dataset.
5104 
5105         Input dict can take several forms:
5106 
5107         .. code:: python
5108 
5109             d = {
5110                 "t": {"dims": ("t"), "data": t},
5111                 "a": {"dims": ("t"), "data": x},
5112                 "b": {"dims": ("t"), "data": y},
5113             }
5114 
5115             d = {
5116                 "coords": {"t": {"dims": "t", "data": t, "attrs": {"units": "s"}}},
5117                 "attrs": {"title": "air temperature"},
5118                 "dims": "t",
5119                 "data_vars": {
5120                     "a": {"dims": "t", "data": x},
5121                     "b": {"dims": "t", "data": y},
5122                 },
5123             }
5124 
5125         where "t" is the name of the dimesion, "a" and "b" are names of data
5126         variables and t, x, and y are lists, numpy.arrays or pandas objects.
5127 
5128         Parameters
5129         ----------
5130         d : dict-like
5131             Mapping with a minimum structure of
5132                 ``{"var_0": {"dims": [..], "data": [..]}, \
5133                             ...}``
5134 
5135         Returns
5136         -------
5137         obj : xarray.Dataset
5138 
5139         See also
5140         --------
5141         Dataset.to_dict
5142         DataArray.from_dict
5143         """
5144 
5145         if not {"coords", "data_vars"}.issubset(set(d)):
5146             variables = d.items()
5147         else:
5148             import itertools
5149 
5150             variables = itertools.chain(
5151                 d.get("coords", {}).items(), d.get("data_vars", {}).items()
5152             )
5153         try:
5154             variable_dict = {
5155                 k: (v["dims"], v["data"], v.get("attrs")) for k, v in variables
5156             }
5157         except KeyError as e:
5158             raise ValueError(
5159                 "cannot convert dict without the key "
5160                 "'{dims_data}'".format(dims_data=str(e.args[0]))
5161             )
5162         obj = cls(variable_dict)
5163 
5164         # what if coords aren't dims?
5165         coords = set(d.get("coords", {})) - set(d.get("dims", {}))
5166         obj = obj.set_coords(coords)
5167 
5168         obj.attrs.update(d.get("attrs", {}))
5169 
5170         return obj
5171 
5172     @staticmethod
5173     def _unary_op(f):
5174         @functools.wraps(f)
5175         def func(self, *args, **kwargs):
5176             variables = {}
5177             keep_attrs = kwargs.pop("keep_attrs", None)
5178             if keep_attrs is None:
5179                 keep_attrs = _get_keep_attrs(default=True)
5180             for k, v in self._variables.items():
5181                 if k in self._coord_names:
5182                     variables[k] = v
5183                 else:
5184                     variables[k] = f(v, *args, **kwargs)
5185                     if keep_attrs:
5186                         variables[k].attrs = v._attrs
5187             attrs = self._attrs if keep_attrs else None
5188             return self._replace_with_new_dims(variables, attrs=attrs)
5189 
5190         return func
5191 
5192     @staticmethod
5193     def _binary_op(f, reflexive=False, join=None):
5194         @functools.wraps(f)
5195         def func(self, other):
5196             from .dataarray import DataArray
5197 
5198             if isinstance(other, groupby.GroupBy):
5199                 return NotImplemented
5200             align_type = OPTIONS["arithmetic_join"] if join is None else join
5201             if isinstance(other, (DataArray, Dataset)):
5202                 self, other = align(self, other, join=align_type, copy=False)
5203             g = f if not reflexive else lambda x, y: f(y, x)
5204             ds = self._calculate_binary_op(g, other, join=align_type)
5205             return ds
5206 
5207         return func
5208 
5209     @staticmethod
5210     def _inplace_binary_op(f):
5211         @functools.wraps(f)
5212         def func(self, other):
5213             from .dataarray import DataArray
5214 
5215             if isinstance(other, groupby.GroupBy):
5216                 raise TypeError(
5217                     "in-place operations between a Dataset and "
5218                     "a grouped object are not permitted"
5219                 )
5220             # we don't actually modify arrays in-place with in-place Dataset
5221             # arithmetic -- this lets us automatically align things
5222             if isinstance(other, (DataArray, Dataset)):
5223                 other = other.reindex_like(self, copy=False)
5224             g = ops.inplace_to_noninplace_op(f)
5225             ds = self._calculate_binary_op(g, other, inplace=True)
5226             self._replace_with_new_dims(
5227                 ds._variables,
5228                 ds._coord_names,
5229                 attrs=ds._attrs,
5230                 indexes=ds._indexes,
5231                 inplace=True,
5232             )
5233             return self
5234 
5235         return func
5236 
5237     def _calculate_binary_op(self, f, other, join="inner", inplace=False):
5238         def apply_over_both(lhs_data_vars, rhs_data_vars, lhs_vars, rhs_vars):
5239             if inplace and set(lhs_data_vars) != set(rhs_data_vars):
5240                 raise ValueError(
5241                     "datasets must have the same data variables "
5242                     "for in-place arithmetic operations: %s, %s"
5243                     % (list(lhs_data_vars), list(rhs_data_vars))
5244                 )
5245 
5246             dest_vars = {}
5247 
5248             for k in lhs_data_vars:
5249                 if k in rhs_data_vars:
5250                     dest_vars[k] = f(lhs_vars[k], rhs_vars[k])
5251                 elif join in ["left", "outer"]:
5252                     dest_vars[k] = f(lhs_vars[k], np.nan)
5253             for k in rhs_data_vars:
5254                 if k not in dest_vars and join in ["right", "outer"]:
5255                     dest_vars[k] = f(rhs_vars[k], np.nan)
5256             return dest_vars
5257 
5258         if utils.is_dict_like(other) and not isinstance(other, Dataset):
5259             # can't use our shortcut of doing the binary operation with
5260             # Variable objects, so apply over our data vars instead.
5261             new_data_vars = apply_over_both(
5262                 self.data_vars, other, self.data_vars, other
5263             )
5264             return Dataset(new_data_vars)
5265 
5266         other_coords = getattr(other, "coords", None)
5267         ds = self.coords.merge(other_coords)
5268 
5269         if isinstance(other, Dataset):
5270             new_vars = apply_over_both(
5271                 self.data_vars, other.data_vars, self.variables, other.variables
5272             )
5273         else:
5274             other_variable = getattr(other, "variable", other)
5275             new_vars = {k: f(self.variables[k], other_variable) for k in self.data_vars}
5276         ds._variables.update(new_vars)
5277         ds._dims = calculate_dimensions(ds._variables)
5278         return ds
5279 
5280     def _copy_attrs_from(self, other):
5281         self.attrs = other.attrs
5282         for v in other.variables:
5283             if v in self.variables:
5284                 self.variables[v].attrs = other.variables[v].attrs
5285 
5286     def diff(self, dim, n=1, label="upper"):
5287         """Calculate the n-th order discrete difference along given axis.
5288 
5289         Parameters
5290         ----------
5291         dim : str
5292             Dimension over which to calculate the finite difference.
5293         n : int, optional
5294             The number of times values are differenced.
5295         label : str, optional
5296             The new coordinate in dimension ``dim`` will have the
5297             values of either the minuend's or subtrahend's coordinate
5298             for values 'upper' and 'lower', respectively.  Other
5299             values are not supported.
5300 
5301         Returns
5302         -------
5303         difference : same type as caller
5304             The n-th order finite difference of this object.
5305 
5306         .. note::
5307 
5308             `n` matches numpy's behavior and is different from pandas' first
5309             argument named `periods`.
5310 
5311         Examples
5312         --------
5313         >>> ds = xr.Dataset({"foo": ("x", [5, 5, 6, 6])})
5314         >>> ds.diff("x")
5315         <xarray.Dataset>
5316         Dimensions:  (x: 3)
5317         Dimensions without coordinates: x
5318         Data variables:
5319             foo      (x) int64 0 1 0
5320         >>> ds.diff("x", 2)
5321         <xarray.Dataset>
5322         Dimensions:  (x: 2)
5323         Dimensions without coordinates: x
5324         Data variables:
5325             foo      (x) int64 1 -1
5326 
5327         See Also
5328         --------
5329         Dataset.differentiate
5330         """
5331         if n == 0:
5332             return self
5333         if n < 0:
5334             raise ValueError(f"order `n` must be non-negative but got {n}")
5335 
5336         # prepare slices
5337         kwargs_start = {dim: slice(None, -1)}
5338         kwargs_end = {dim: slice(1, None)}
5339 
5340         # prepare new coordinate
5341         if label == "upper":
5342             kwargs_new = kwargs_end
5343         elif label == "lower":
5344             kwargs_new = kwargs_start
5345         else:
5346             raise ValueError("The 'label' argument has to be either 'upper' or 'lower'")
5347 
5348         variables = {}
5349 
5350         for name, var in self.variables.items():
5351             if dim in var.dims:
5352                 if name in self.data_vars:
5353                     variables[name] = var.isel(**kwargs_end) - var.isel(**kwargs_start)
5354                 else:
5355                     variables[name] = var.isel(**kwargs_new)
5356             else:
5357                 variables[name] = var
5358 
5359         indexes = dict(self.indexes)
5360         if dim in indexes:
5361             indexes[dim] = indexes[dim][kwargs_new[dim]]
5362 
5363         difference = self._replace_with_new_dims(variables, indexes=indexes)
5364 
5365         if n > 1:
5366             return difference.diff(dim, n - 1)
5367         else:
5368             return difference
5369 
5370     def shift(self, shifts=None, fill_value=dtypes.NA, **shifts_kwargs):
5371         """Shift this dataset by an offset along one or more dimensions.
5372 
5373         Only data variables are moved; coordinates stay in place. This is
5374         consistent with the behavior of ``shift`` in pandas.
5375 
5376         Parameters
5377         ----------
5378         shifts : mapping of hashable to int
5379             Integer offset to shift along each of the given dimensions.
5380             Positive offsets shift to the right; negative offsets shift to the
5381             left.
5382         fill_value : scalar or dict-like, optional
5383             Value to use for newly missing values. If a dict-like, maps
5384             variable names (including coordinates) to fill values.
5385         **shifts_kwargs
5386             The keyword arguments form of ``shifts``.
5387             One of shifts or shifts_kwargs must be provided.
5388 
5389         Returns
5390         -------
5391         shifted : Dataset
5392             Dataset with the same coordinates and attributes but shifted data
5393             variables.
5394 
5395         See also
5396         --------
5397         roll
5398 
5399         Examples
5400         --------
5401 
5402         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5403         >>> ds.shift(x=2)
5404         <xarray.Dataset>
5405         Dimensions:  (x: 5)
5406         Dimensions without coordinates: x
5407         Data variables:
5408             foo      (x) object nan nan 'a' 'b' 'c'
5409         """
5410         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "shift")
5411         invalid = [k for k in shifts if k not in self.dims]
5412         if invalid:
5413             raise ValueError("dimensions %r do not exist" % invalid)
5414 
5415         variables = {}
5416         for name, var in self.variables.items():
5417             if name in self.data_vars:
5418                 fill_value_ = (
5419                     fill_value.get(name, dtypes.NA)
5420                     if isinstance(fill_value, dict)
5421                     else fill_value
5422                 )
5423 
5424                 var_shifts = {k: v for k, v in shifts.items() if k in var.dims}
5425                 variables[name] = var.shift(fill_value=fill_value_, shifts=var_shifts)
5426             else:
5427                 variables[name] = var
5428 
5429         return self._replace(variables)
5430 
5431     def roll(self, shifts=None, roll_coords=None, **shifts_kwargs):
5432         """Roll this dataset by an offset along one or more dimensions.
5433 
5434         Unlike shift, roll may rotate all variables, including coordinates
5435         if specified. The direction of rotation is consistent with
5436         :py:func:`numpy.roll`.
5437 
5438         Parameters
5439         ----------
5440 
5441         shifts : dict, optional
5442             A dict with keys matching dimensions and values given
5443             by integers to rotate each of the given dimensions. Positive
5444             offsets roll to the right; negative offsets roll to the left.
5445         roll_coords : bool
5446             Indicates whether to  roll the coordinates by the offset
5447             The current default of roll_coords (None, equivalent to True) is
5448             deprecated and will change to False in a future version.
5449             Explicitly pass roll_coords to silence the warning.
5450         **shifts_kwargs : {dim: offset, ...}, optional
5451             The keyword arguments form of ``shifts``.
5452             One of shifts or shifts_kwargs must be provided.
5453         Returns
5454         -------
5455         rolled : Dataset
5456             Dataset with the same coordinates and attributes but rolled
5457             variables.
5458 
5459         See also
5460         --------
5461         shift
5462 
5463         Examples
5464         --------
5465 
5466         >>> ds = xr.Dataset({"foo": ("x", list("abcde"))})
5467         >>> ds.roll(x=2)
5468         <xarray.Dataset>
5469         Dimensions:  (x: 5)
5470         Dimensions without coordinates: x
5471         Data variables:
5472             foo      (x) <U1 'd' 'e' 'a' 'b' 'c'
5473         """
5474         shifts = either_dict_or_kwargs(shifts, shifts_kwargs, "roll")
5475         invalid = [k for k in shifts if k not in self.dims]
5476         if invalid:
5477             raise ValueError("dimensions %r do not exist" % invalid)
5478 
5479         if roll_coords is None:
5480             warnings.warn(
5481                 "roll_coords will be set to False in the future."
5482                 " Explicitly set roll_coords to silence warning.",
5483                 FutureWarning,
5484                 stacklevel=2,
5485             )
5486             roll_coords = True
5487 
5488         unrolled_vars = () if roll_coords else self.coords
5489 
5490         variables = {}
5491         for k, v in self.variables.items():
5492             if k not in unrolled_vars:
5493                 variables[k] = v.roll(
5494                     **{k: s for k, s in shifts.items() if k in v.dims}
5495                 )
5496             else:
5497                 variables[k] = v
5498 
5499         if roll_coords:
5500             indexes = {}
5501             for k, v in self.indexes.items():
5502                 (dim,) = self.variables[k].dims
5503                 if dim in shifts:
5504                     indexes[k] = roll_index(v, shifts[dim])
5505                 else:
5506                     indexes[k] = v
5507         else:
5508             indexes = dict(self.indexes)
5509 
5510         return self._replace(variables, indexes=indexes)
5511 
5512     def sortby(self, variables, ascending=True):
5513         """
5514         Sort object by labels or values (along an axis).
5515 
5516         Sorts the dataset, either along specified dimensions,
5517         or according to values of 1-D dataarrays that share dimension
5518         with calling object.
5519 
5520         If the input variables are dataarrays, then the dataarrays are aligned
5521         (via left-join) to the calling object prior to sorting by cell values.
5522         NaNs are sorted to the end, following Numpy convention.
5523 
5524         If multiple sorts along the same dimension is
5525         given, numpy's lexsort is performed along that dimension:
5526         https://docs.scipy.org/doc/numpy/reference/generated/numpy.lexsort.html
5527         and the FIRST key in the sequence is used as the primary sort key,
5528         followed by the 2nd key, etc.
5529 
5530         Parameters
5531         ----------
5532         variables: str, DataArray, or list of str or DataArray
5533             1D DataArray objects or name(s) of 1D variable(s) in
5534             coords/data_vars whose values are used to sort the dataset.
5535         ascending: bool, optional
5536             Whether to sort by ascending or descending order.
5537 
5538         Returns
5539         -------
5540         sorted : Dataset
5541             A new dataset where all the specified dims are sorted by dim
5542             labels.
5543         """
5544         from .dataarray import DataArray
5545 
5546         if not isinstance(variables, list):
5547             variables = [variables]
5548         else:
5549             variables = variables
5550         variables = [v if isinstance(v, DataArray) else self[v] for v in variables]
5551         aligned_vars = align(self, *variables, join="left")
5552         aligned_self = aligned_vars[0]
5553         aligned_other_vars = aligned_vars[1:]
5554         vars_by_dim = defaultdict(list)
5555         for data_array in aligned_other_vars:
5556             if data_array.ndim != 1:
5557                 raise ValueError("Input DataArray is not 1-D.")
5558             (key,) = data_array.dims
5559             vars_by_dim[key].append(data_array)
5560 
5561         indices = {}
5562         for key, arrays in vars_by_dim.items():
5563             order = np.lexsort(tuple(reversed(arrays)))
5564             indices[key] = order if ascending else order[::-1]
5565         return aligned_self.isel(**indices)
5566 
5567     def quantile(
5568         self,
5569         q,
5570         dim=None,
5571         interpolation="linear",
5572         numeric_only=False,
5573         keep_attrs=None,
5574         skipna=True,
5575     ):
5576         """Compute the qth quantile of the data along the specified dimension.
5577 
5578         Returns the qth quantiles(s) of the array elements for each variable
5579         in the Dataset.
5580 
5581         Parameters
5582         ----------
5583         q : float or array-like of float
5584             Quantile to compute, which must be between 0 and 1 inclusive.
5585         dim : str or sequence of str, optional
5586             Dimension(s) over which to apply quantile.
5587         interpolation : {"linear", "lower", "higher", "midpoint", "nearest"}, default: "linear"
5588             This optional parameter specifies the interpolation method to
5589             use when the desired quantile lies between two data points
5590             ``i < j``:
5591 
5592                 * linear: ``i + (j - i) * fraction``, where ``fraction`` is
5593                   the fractional part of the index surrounded by ``i`` and
5594                   ``j``.
5595                 * lower: ``i``.
5596                 * higher: ``j``.
5597                 * nearest: ``i`` or ``j``, whichever is nearest.
5598                 * midpoint: ``(i + j) / 2``.
5599         keep_attrs : bool, optional
5600             If True, the dataset's attributes (`attrs`) will be copied from
5601             the original object to the new one.  If False (default), the new
5602             object will be returned without attributes.
5603         numeric_only : bool, optional
5604             If True, only apply ``func`` to variables with a numeric dtype.
5605         skipna : bool, optional
5606             Whether to skip missing values when aggregating.
5607 
5608         Returns
5609         -------
5610         quantiles : Dataset
5611             If `q` is a single quantile, then the result is a scalar for each
5612             variable in data_vars. If multiple percentiles are given, first
5613             axis of the result corresponds to the quantile and a quantile
5614             dimension is added to the return Dataset. The other dimensions are
5615             the dimensions that remain after the reduction of the array.
5616 
5617         See Also
5618         --------
5619         numpy.nanquantile, numpy.quantile, pandas.Series.quantile, DataArray.quantile
5620 
5621         Examples
5622         --------
5623 
5624         >>> ds = xr.Dataset(
5625         ...     {"a": (("x", "y"), [[0.7, 4.2, 9.4, 1.5], [6.5, 7.3, 2.6, 1.9]])},
5626         ...     coords={"x": [7, 9], "y": [1, 1.5, 2, 2.5]},
5627         ... )
5628         >>> ds.quantile(0)  # or ds.quantile(0, dim=...)
5629         <xarray.Dataset>
5630         Dimensions:   ()
5631         Coordinates:
5632             quantile  float64 0.0
5633         Data variables:
5634             a         float64 0.7
5635         >>> ds.quantile(0, dim="x")
5636         <xarray.Dataset>
5637         Dimensions:   (y: 4)
5638         Coordinates:
5639           * y         (y) float64 1.0 1.5 2.0 2.5
5640             quantile  float64 0.0
5641         Data variables:
5642             a         (y) float64 0.7 4.2 2.6 1.5
5643         >>> ds.quantile([0, 0.5, 1])
5644         <xarray.Dataset>
5645         Dimensions:   (quantile: 3)
5646         Coordinates:
5647           * quantile  (quantile) float64 0.0 0.5 1.0
5648         Data variables:
5649             a         (quantile) float64 0.7 3.4 9.4
5650         >>> ds.quantile([0, 0.5, 1], dim="x")
5651         <xarray.Dataset>
5652         Dimensions:   (quantile: 3, y: 4)
5653         Coordinates:
5654           * y         (y) float64 1.0 1.5 2.0 2.5
5655           * quantile  (quantile) float64 0.0 0.5 1.0
5656         Data variables:
5657             a         (quantile, y) float64 0.7 4.2 2.6 1.5 3.6 ... 1.7 6.5 7.3 9.4 1.9
5658         """
5659 
5660         if isinstance(dim, str):
5661             dims = {dim}
5662         elif dim in [None, ...]:
5663             dims = set(self.dims)
5664         else:
5665             dims = set(dim)
5666 
5667         _assert_empty(
5668             [d for d in dims if d not in self.dims],
5669             "Dataset does not contain the dimensions: %s",
5670         )
5671 
5672         q = np.asarray(q, dtype=np.float64)
5673 
5674         variables = {}
5675         for name, var in self.variables.items():
5676             reduce_dims = [d for d in var.dims if d in dims]
5677             if reduce_dims or not var.dims:
5678                 if name not in self.coords:
5679                     if (
5680                         not numeric_only
5681                         or np.issubdtype(var.dtype, np.number)
5682                         or var.dtype == np.bool_
5683                     ):
5684                         if len(reduce_dims) == var.ndim:
5685                             # prefer to aggregate over axis=None rather than
5686                             # axis=(0, 1) if they will be equivalent, because
5687                             # the former is often more efficient
5688                             reduce_dims = None
5689                         variables[name] = var.quantile(
5690                             q,
5691                             dim=reduce_dims,
5692                             interpolation=interpolation,
5693                             keep_attrs=keep_attrs,
5694                             skipna=skipna,
5695                         )
5696 
5697             else:
5698                 variables[name] = var
5699 
5700         # construct the new dataset
5701         coord_names = {k for k in self.coords if k in variables}
5702         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5703         if keep_attrs is None:
5704             keep_attrs = _get_keep_attrs(default=False)
5705         attrs = self.attrs if keep_attrs else None
5706         new = self._replace_with_new_dims(
5707             variables, coord_names=coord_names, attrs=attrs, indexes=indexes
5708         )
5709         return new.assign_coords(quantile=q)
5710 
5711     def rank(self, dim, pct=False, keep_attrs=None):
5712         """Ranks the data.
5713 
5714         Equal values are assigned a rank that is the average of the ranks that
5715         would have been otherwise assigned to all of the values within
5716         that set.
5717         Ranks begin at 1, not 0. If pct is True, computes percentage ranks.
5718 
5719         NaNs in the input array are returned as NaNs.
5720 
5721         The `bottleneck` library is required.
5722 
5723         Parameters
5724         ----------
5725         dim : str
5726             Dimension over which to compute rank.
5727         pct : bool, optional
5728             If True, compute percentage ranks, otherwise compute integer ranks.
5729         keep_attrs : bool, optional
5730             If True, the dataset's attributes (`attrs`) will be copied from
5731             the original object to the new one.  If False (default), the new
5732             object will be returned without attributes.
5733 
5734         Returns
5735         -------
5736         ranked : Dataset
5737             Variables that do not depend on `dim` are dropped.
5738         """
5739         if dim not in self.dims:
5740             raise ValueError("Dataset does not contain the dimension: %s" % dim)
5741 
5742         variables = {}
5743         for name, var in self.variables.items():
5744             if name in self.data_vars:
5745                 if dim in var.dims:
5746                     variables[name] = var.rank(dim, pct=pct)
5747             else:
5748                 variables[name] = var
5749 
5750         coord_names = set(self.coords)
5751         if keep_attrs is None:
5752             keep_attrs = _get_keep_attrs(default=False)
5753         attrs = self.attrs if keep_attrs else None
5754         return self._replace(variables, coord_names, attrs=attrs)
5755 
5756     def differentiate(self, coord, edge_order=1, datetime_unit=None):
5757         """ Differentiate with the second order accurate central
5758         differences.
5759 
5760         .. note::
5761             This feature is limited to simple cartesian geometry, i.e. coord
5762             must be one dimensional.
5763 
5764         Parameters
5765         ----------
5766         coord : str
5767             The coordinate to be used to compute the gradient.
5768         edge_order : {1, 2}, default: 1
5769             N-th order accurate differences at the boundaries.
5770         datetime_unit : None or {"Y", "M", "W", "D", "h", "m", "s", "ms", \
5771             "us", "ns", "ps", "fs", "as"}, default: None
5772             Unit to compute gradient. Only valid for datetime coordinate.
5773 
5774         Returns
5775         -------
5776         differentiated: Dataset
5777 
5778         See also
5779         --------
5780         numpy.gradient: corresponding numpy function
5781         """
5782         from .variable import Variable
5783 
5784         if coord not in self.variables and coord not in self.dims:
5785             raise ValueError(f"Coordinate {coord} does not exist.")
5786 
5787         coord_var = self[coord].variable
5788         if coord_var.ndim != 1:
5789             raise ValueError(
5790                 "Coordinate {} must be 1 dimensional but is {}"
5791                 " dimensional".format(coord, coord_var.ndim)
5792             )
5793 
5794         dim = coord_var.dims[0]
5795         if _contains_datetime_like_objects(coord_var):
5796             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5797                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5798             elif datetime_unit is None:
5799                 datetime_unit = "s"  # Default to seconds for cftime objects
5800             coord_var = coord_var._to_numeric(datetime_unit=datetime_unit)
5801 
5802         variables = {}
5803         for k, v in self.variables.items():
5804             if k in self.data_vars and dim in v.dims and k not in self.coords:
5805                 if _contains_datetime_like_objects(v):
5806                     v = v._to_numeric(datetime_unit=datetime_unit)
5807                 grad = duck_array_ops.gradient(
5808                     v.data, coord_var, edge_order=edge_order, axis=v.get_axis_num(dim)
5809                 )
5810                 variables[k] = Variable(v.dims, grad)
5811             else:
5812                 variables[k] = v
5813         return self._replace(variables)
5814 
5815     def integrate(self, coord, datetime_unit=None):
5816         """ integrate the array with the trapezoidal rule.
5817 
5818         .. note::
5819             This feature is limited to simple cartesian geometry, i.e. coord
5820             must be one dimensional.
5821 
5822         Parameters
5823         ----------
5824         coord: str, or sequence of str
5825             Coordinate(s) used for the integration.
5826         datetime_unit : {"Y", "M", "W", "D", "h", "m", "s", "ms", "us", "ns", \
5827                          "ps", "fs", "as"}, optional
5828             Can be specify the unit if datetime coordinate is used.
5829 
5830         Returns
5831         -------
5832         integrated : Dataset
5833 
5834         See also
5835         --------
5836         DataArray.integrate
5837         numpy.trapz: corresponding numpy function
5838 
5839         Examples
5840         --------
5841         >>> ds = xr.Dataset(
5842         ...     data_vars={"a": ("x", [5, 5, 6, 6]), "b": ("x", [1, 2, 1, 0])},
5843         ...     coords={"x": [0, 1, 2, 3], "y": ("x", [1, 7, 3, 5])},
5844         ... )
5845         >>> ds
5846         <xarray.Dataset>
5847         Dimensions:  (x: 4)
5848         Coordinates:
5849           * x        (x) int64 0 1 2 3
5850             y        (x) int64 1 7 3 5
5851         Data variables:
5852             a        (x) int64 5 5 6 6
5853             b        (x) int64 1 2 1 0
5854         >>> ds.integrate("x")
5855         <xarray.Dataset>
5856         Dimensions:  ()
5857         Data variables:
5858             a        float64 16.5
5859             b        float64 3.5
5860         >>> ds.integrate("y")
5861         <xarray.Dataset>
5862         Dimensions:  ()
5863         Data variables:
5864             a        float64 20.0
5865             b        float64 4.0
5866         """
5867         if not isinstance(coord, (list, tuple)):
5868             coord = (coord,)
5869         result = self
5870         for c in coord:
5871             result = result._integrate_one(c, datetime_unit=datetime_unit)
5872         return result
5873 
5874     def _integrate_one(self, coord, datetime_unit=None):
5875         from .variable import Variable
5876 
5877         if coord not in self.variables and coord not in self.dims:
5878             raise ValueError(f"Coordinate {coord} does not exist.")
5879 
5880         coord_var = self[coord].variable
5881         if coord_var.ndim != 1:
5882             raise ValueError(
5883                 "Coordinate {} must be 1 dimensional but is {}"
5884                 " dimensional".format(coord, coord_var.ndim)
5885             )
5886 
5887         dim = coord_var.dims[0]
5888         if _contains_datetime_like_objects(coord_var):
5889             if coord_var.dtype.kind in "mM" and datetime_unit is None:
5890                 datetime_unit, _ = np.datetime_data(coord_var.dtype)
5891             elif datetime_unit is None:
5892                 datetime_unit = "s"  # Default to seconds for cftime objects
5893             coord_var = coord_var._replace(
5894                 data=datetime_to_numeric(coord_var.data, datetime_unit=datetime_unit)
5895             )
5896 
5897         variables = {}
5898         coord_names = set()
5899         for k, v in self.variables.items():
5900             if k in self.coords:
5901                 if dim not in v.dims:
5902                     variables[k] = v
5903                     coord_names.add(k)
5904             else:
5905                 if k in self.data_vars and dim in v.dims:
5906                     if _contains_datetime_like_objects(v):
5907                         v = datetime_to_numeric(v, datetime_unit=datetime_unit)
5908                     integ = duck_array_ops.trapz(
5909                         v.data, coord_var.data, axis=v.get_axis_num(dim)
5910                     )
5911                     v_dims = list(v.dims)
5912                     v_dims.remove(dim)
5913                     variables[k] = Variable(v_dims, integ)
5914                 else:
5915                     variables[k] = v
5916         indexes = {k: v for k, v in self.indexes.items() if k in variables}
5917         return self._replace_with_new_dims(
5918             variables, coord_names=coord_names, indexes=indexes
5919         )
5920 
5921     @property
5922     def real(self):
5923         return self.map(lambda x: x.real, keep_attrs=True)
5924 
5925     @property
5926     def imag(self):
5927         return self.map(lambda x: x.imag, keep_attrs=True)
5928 
5929     plot = utils.UncachedAccessor(_Dataset_PlotMethods)
5930 
5931     def filter_by_attrs(self, **kwargs):
5932         """Returns a ``Dataset`` with variables that match specific conditions.
5933 
5934         Can pass in ``key=value`` or ``key=callable``.  A Dataset is returned
5935         containing only the variables for which all the filter tests pass.
5936         These tests are either ``key=value`` for which the attribute ``key``
5937         has the exact value ``value`` or the callable passed into
5938         ``key=callable`` returns True. The callable will be passed a single
5939         value, either the value of the attribute ``key`` or ``None`` if the
5940         DataArray does not have an attribute with the name ``key``.
5941 
5942         Parameters
5943         ----------
5944         **kwargs
5945             key : str
5946                 Attribute name.
5947             value : callable or obj
5948                 If value is a callable, it should return a boolean in the form
5949                 of bool = func(attr) where attr is da.attrs[key].
5950                 Otherwise, value will be compared to the each
5951                 DataArray's attrs[key].
5952 
5953         Returns
5954         -------
5955         new : Dataset
5956             New dataset with variables filtered by attribute.
5957 
5958         Examples
5959         --------
5960         >>> # Create an example dataset:
5961         >>> temp = 15 + 8 * np.random.randn(2, 2, 3)
5962         >>> precip = 10 * np.random.rand(2, 2, 3)
5963         >>> lon = [[-99.83, -99.32], [-99.79, -99.23]]
5964         >>> lat = [[42.25, 42.21], [42.63, 42.59]]
5965         >>> dims = ["x", "y", "time"]
5966         >>> temp_attr = dict(standard_name="air_potential_temperature")
5967         >>> precip_attr = dict(standard_name="convective_precipitation_flux")
5968         >>> ds = xr.Dataset(
5969         ...     {
5970         ...         "temperature": (dims, temp, temp_attr),
5971         ...         "precipitation": (dims, precip, precip_attr),
5972         ...     },
5973         ...     coords={
5974         ...         "lon": (["x", "y"], lon),
5975         ...         "lat": (["x", "y"], lat),
5976         ...         "time": pd.date_range("2014-09-06", periods=3),
5977         ...         "reference_time": pd.Timestamp("2014-09-05"),
5978         ...     },
5979         ... )
5980         >>> # Get variables matching a specific standard_name.
5981         >>> ds.filter_by_attrs(standard_name="convective_precipitation_flux")
5982         <xarray.Dataset>
5983         Dimensions:         (time: 3, x: 2, y: 2)
5984         Coordinates:
5985             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5986             lat             (x, y) float64 42.25 42.21 42.63 42.59
5987           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
5988             reference_time  datetime64[ns] 2014-09-05
5989         Dimensions without coordinates: x, y
5990         Data variables:
5991             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
5992         >>> # Get all variables that have a standard_name attribute.
5993         >>> standard_name = lambda v: v is not None
5994         >>> ds.filter_by_attrs(standard_name=standard_name)
5995         <xarray.Dataset>
5996         Dimensions:         (time: 3, x: 2, y: 2)
5997         Coordinates:
5998             lon             (x, y) float64 -99.83 -99.32 -99.79 -99.23
5999             lat             (x, y) float64 42.25 42.21 42.63 42.59
6000           * time            (time) datetime64[ns] 2014-09-06 2014-09-07 2014-09-08
6001             reference_time  datetime64[ns] 2014-09-05
6002         Dimensions without coordinates: x, y
6003         Data variables:
6004             temperature     (x, y, time) float64 29.11 18.2 22.83 ... 18.28 16.15 26.63
6005             precipitation   (x, y, time) float64 5.68 9.256 0.7104 ... 7.992 4.615 7.805
6006 
6007         """
6008         selection = []
6009         for var_name, variable in self.variables.items():
6010             has_value_flag = False
6011             for attr_name, pattern in kwargs.items():
6012                 attr_value = variable.attrs.get(attr_name)
6013                 if (callable(pattern) and pattern(attr_value)) or attr_value == pattern:
6014                     has_value_flag = True
6015                 else:
6016                     has_value_flag = False
6017                     break
6018             if has_value_flag is True:
6019                 selection.append(var_name)
6020         return self[selection]
6021 
6022     def unify_chunks(self) -> "Dataset":
6023         """Unify chunk size along all chunked dimensions of this Dataset.
6024 
6025         Returns
6026         -------
6027 
6028         Dataset with consistent chunk sizes for all dask-array variables
6029 
6030         See Also
6031         --------
6032 
6033         dask.array.core.unify_chunks
6034         """
6035 
6036         try:
6037             self.chunks
6038         except ValueError:  # "inconsistent chunks"
6039             pass
6040         else:
6041             # No variables with dask backend, or all chunks are already aligned
6042             return self.copy()
6043 
6044         # import dask is placed after the quick exit test above to allow
6045         # running this method if dask isn't installed and there are no chunks
6046         import dask.array
6047 
6048         ds = self.copy()
6049 
6050         dims_pos_map = {dim: index for index, dim in enumerate(ds.dims)}
6051 
6052         dask_array_names = []
6053         dask_unify_args = []
6054         for name, variable in ds.variables.items():
6055             if isinstance(variable.data, dask.array.Array):
6056                 dims_tuple = [dims_pos_map[dim] for dim in variable.dims]
6057                 dask_array_names.append(name)
6058                 dask_unify_args.append(variable.data)
6059                 dask_unify_args.append(dims_tuple)
6060 
6061         _, rechunked_arrays = dask.array.core.unify_chunks(*dask_unify_args)
6062 
6063         for name, new_array in zip(dask_array_names, rechunked_arrays):
6064             ds.variables[name]._data = new_array
6065 
6066         return ds
6067 
6068     def map_blocks(
6069         self,
6070         func: "Callable[..., T_DSorDA]",
6071         args: Sequence[Any] = (),
6072         kwargs: Mapping[str, Any] = None,
6073         template: Union["DataArray", "Dataset"] = None,
6074     ) -> "T_DSorDA":
6075         """
6076         Apply a function to each block of this Dataset.
6077 
6078         .. warning::
6079             This method is experimental and its signature may change.
6080 
6081         Parameters
6082         ----------
6083         func : callable
6084             User-provided function that accepts a Dataset as its first
6085             parameter. The function will receive a subset or 'block' of this Dataset (see below),
6086             corresponding to one chunk along each chunked dimension. ``func`` will be
6087             executed as ``func(subset_dataset, *subset_args, **kwargs)``.
6088 
6089             This function must return either a single DataArray or a single Dataset.
6090 
6091             This function cannot add a new chunked dimension.
6092         args : sequence
6093             Passed to func after unpacking and subsetting any xarray objects by blocks.
6094             xarray objects in args must be aligned with obj, otherwise an error is raised.
6095         kwargs : mapping
6096             Passed verbatim to func after unpacking. xarray objects, if any, will not be
6097             subset to blocks. Passing dask collections in kwargs is not allowed.
6098         template : DataArray or Dataset, optional
6099             xarray object representing the final result after compute is called. If not provided,
6100             the function will be first run on mocked-up data, that looks like this object but
6101             has sizes 0, to determine properties of the returned object such as dtype,
6102             variable names, attributes, new dimensions and new indexes (if any).
6103             ``template`` must be provided if the function changes the size of existing dimensions.
6104             When provided, ``attrs`` on variables in `template` are copied over to the result. Any
6105             ``attrs`` set by ``func`` will be ignored.
6106 
6107 
6108         Returns
6109         -------
6110         A single DataArray or Dataset with dask backend, reassembled from the outputs of the
6111         function.
6112 
6113         Notes
6114         -----
6115         This function is designed for when ``func`` needs to manipulate a whole xarray object
6116         subset to each block. In the more common case where ``func`` can work on numpy arrays, it is
6117         recommended to use ``apply_ufunc``.
6118 
6119         If none of the variables in this object is backed by dask arrays, calling this function is
6120         equivalent to calling ``func(obj, *args, **kwargs)``.
6121 
6122         See Also
6123         --------
6124         dask.array.map_blocks, xarray.apply_ufunc, xarray.Dataset.map_blocks,
6125         xarray.DataArray.map_blocks
6126 
6127         Examples
6128         --------
6129 
6130         Calculate an anomaly from climatology using ``.groupby()``. Using
6131         ``xr.map_blocks()`` allows for parallel operations with knowledge of ``xarray``,
6132         its indices, and its methods like ``.groupby()``.
6133 
6134         >>> def calculate_anomaly(da, groupby_type="time.month"):
6135         ...     gb = da.groupby(groupby_type)
6136         ...     clim = gb.mean(dim="time")
6137         ...     return gb - clim
6138         ...
6139         >>> time = xr.cftime_range("1990-01", "1992-01", freq="M")
6140         >>> month = xr.DataArray(time.month, coords={"time": time}, dims=["time"])
6141         >>> np.random.seed(123)
6142         >>> array = xr.DataArray(
6143         ...     np.random.rand(len(time)),
6144         ...     dims=["time"],
6145         ...     coords={"time": time, "month": month},
6146         ... ).chunk()
6147         >>> ds = xr.Dataset({"a": array})
6148         >>> ds.map_blocks(calculate_anomaly, template=ds).compute()
6149         <xarray.Dataset>
6150         Dimensions:  (time: 24)
6151         Coordinates:
6152           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6153             month    (time) int64 1 2 3 4 5 6 7 8 9 10 11 12 1 2 3 4 5 6 7 8 9 10 11 12
6154         Data variables:
6155             a        (time) float64 0.1289 0.1132 -0.0856 ... 0.2287 0.1906 -0.05901
6156 
6157         Note that one must explicitly use ``args=[]`` and ``kwargs={}`` to pass arguments
6158         to the function being applied in ``xr.map_blocks()``:
6159 
6160         >>> ds.map_blocks(
6161         ...     calculate_anomaly,
6162         ...     kwargs={"groupby_type": "time.year"},
6163         ...     template=ds,
6164         ... )
6165         <xarray.Dataset>
6166         Dimensions:  (time: 24)
6167         Coordinates:
6168           * time     (time) object 1990-01-31 00:00:00 ... 1991-12-31 00:00:00
6169             month    (time) int64 dask.array<chunksize=(24,), meta=np.ndarray>
6170         Data variables:
6171             a        (time) float64 dask.array<chunksize=(24,), meta=np.ndarray>
6172         """
6173         from .parallel import map_blocks
6174 
6175         return map_blocks(func, self, args, kwargs, template)
6176 
6177     def polyfit(
6178         self,
6179         dim: Hashable,
6180         deg: int,
6181         skipna: bool = None,
6182         rcond: float = None,
6183         w: Union[Hashable, Any] = None,
6184         full: bool = False,
6185         cov: Union[bool, str] = False,
6186     ):
6187         """
6188         Least squares polynomial fit.
6189 
6190         This replicates the behaviour of `numpy.polyfit` but differs by skipping
6191         invalid values when `skipna = True`.
6192 
6193         Parameters
6194         ----------
6195         dim : hashable
6196             Coordinate along which to fit the polynomials.
6197         deg : int
6198             Degree of the fitting polynomial.
6199         skipna : bool, optional
6200             If True, removes all invalid values before fitting each 1D slices of the array.
6201             Default is True if data is stored in a dask.array or if there is any
6202             invalid values, False otherwise.
6203         rcond : float, optional
6204             Relative condition number to the fit.
6205         w : hashable or Any, optional
6206             Weights to apply to the y-coordinate of the sample points.
6207             Can be an array-like object or the name of a coordinate in the dataset.
6208         full : bool, optional
6209             Whether to return the residuals, matrix rank and singular values in addition
6210             to the coefficients.
6211         cov : bool or str, optional
6212             Whether to return to the covariance matrix in addition to the coefficients.
6213             The matrix is not scaled if `cov='unscaled'`.
6214 
6215 
6216         Returns
6217         -------
6218         polyfit_results : Dataset
6219             A single dataset which contains (for each "var" in the input dataset):
6220 
6221             [var]_polyfit_coefficients
6222                 The coefficients of the best fit for each variable in this dataset.
6223             [var]_polyfit_residuals
6224                 The residuals of the least-square computation for each variable (only included if `full=True`)
6225                 When the matrix rank is deficient, np.nan is returned.
6226             [dim]_matrix_rank
6227                 The effective rank of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6228                 The rank is computed ignoring the NaN values that might be skipped.
6229             [dim]_singular_values
6230                 The singular values of the scaled Vandermonde coefficient matrix (only included if `full=True`)
6231             [var]_polyfit_covariance
6232                 The covariance matrix of the polynomial coefficient estimates (only included if `full=False` and `cov=True`)
6233 
6234         Warns
6235         -----
6236         RankWarning
6237             The rank of the coefficient matrix in the least-squares fit is deficient.
6238             The warning is not raised with in-memory (not dask) data and `full=True`.
6239 
6240         See also
6241         --------
6242         numpy.polyfit
6243         """
6244         variables = {}
6245         skipna_da = skipna
6246 
6247         x = get_clean_interp_index(self, dim, strict=False)
6248         xname = "{}_".format(self[dim].name)
6249         order = int(deg) + 1
6250         lhs = np.vander(x, order)
6251 
6252         if rcond is None:
6253             rcond = x.shape[0] * np.core.finfo(x.dtype).eps
6254 
6255         # Weights:
6256         if w is not None:
6257             if isinstance(w, Hashable):
6258                 w = self.coords[w]
6259             w = np.asarray(w)
6260             if w.ndim != 1:
6261                 raise TypeError("Expected a 1-d array for weights.")
6262             if w.shape[0] != lhs.shape[0]:
6263                 raise TypeError("Expected w and {} to have the same length".format(dim))
6264             lhs *= w[:, np.newaxis]
6265 
6266         # Scaling
6267         scale = np.sqrt((lhs * lhs).sum(axis=0))
6268         lhs /= scale
6269 
6270         degree_dim = utils.get_temp_dimname(self.dims, "degree")
6271 
6272         rank = np.linalg.matrix_rank(lhs)
6273 
6274         if full:
6275             rank = xr.DataArray(rank, name=xname + "matrix_rank")
6276             variables[rank.name] = rank
6277             sing = np.linalg.svd(lhs, compute_uv=False)
6278             sing = xr.DataArray(
6279                 sing,
6280                 dims=(degree_dim,),
6281                 coords={degree_dim: np.arange(rank - 1, -1, -1)},
6282                 name=xname + "singular_values",
6283             )
6284             variables[sing.name] = sing
6285 
6286         for name, da in self.data_vars.items():
6287             if dim not in da.dims:
6288                 continue
6289 
6290             if is_duck_dask_array(da.data) and (
6291                 rank != order or full or skipna is None
6292             ):
6293                 # Current algorithm with dask and skipna=False neither supports
6294                 # deficient ranks nor does it output the "full" info (issue dask/dask#6516)
6295                 skipna_da = True
6296             elif skipna is None:
6297                 skipna_da = np.any(da.isnull())
6298 
6299             dims_to_stack = [dimname for dimname in da.dims if dimname != dim]
6300             stacked_coords: Dict[Hashable, DataArray] = {}
6301             if dims_to_stack:
6302                 stacked_dim = utils.get_temp_dimname(dims_to_stack, "stacked")
6303                 rhs = da.transpose(dim, *dims_to_stack).stack(
6304                     {stacked_dim: dims_to_stack}
6305                 )
6306                 stacked_coords = {stacked_dim: rhs[stacked_dim]}
6307                 scale_da = scale[:, np.newaxis]
6308             else:
6309                 rhs = da
6310                 scale_da = scale
6311 
6312             if w is not None:
6313                 rhs *= w[:, np.newaxis]
6314 
6315             with warnings.catch_warnings():
6316                 if full:  # Copy np.polyfit behavior
6317                     warnings.simplefilter("ignore", np.RankWarning)
6318                 else:  # Raise only once per variable
6319                     warnings.simplefilter("once", np.RankWarning)
6320 
6321                 coeffs, residuals = duck_array_ops.least_squares(
6322                     lhs, rhs.data, rcond=rcond, skipna=skipna_da
6323                 )
6324 
6325             if isinstance(name, str):
6326                 name = "{}_".format(name)
6327             else:
6328                 # Thus a ReprObject => polyfit was called on a DataArray
6329                 name = ""
6330 
6331             coeffs = xr.DataArray(
6332                 coeffs / scale_da,
6333                 dims=[degree_dim] + list(stacked_coords.keys()),
6334                 coords={degree_dim: np.arange(order)[::-1], **stacked_coords},
6335                 name=name + "polyfit_coefficients",
6336             )
6337             if dims_to_stack:
6338                 coeffs = coeffs.unstack(stacked_dim)
6339             variables[coeffs.name] = coeffs
6340 
6341             if full or (cov is True):
6342                 residuals = xr.DataArray(
6343                     residuals if dims_to_stack else residuals.squeeze(),
6344                     dims=list(stacked_coords.keys()),
6345                     coords=stacked_coords,
6346                     name=name + "polyfit_residuals",
6347                 )
6348                 if dims_to_stack:
6349                     residuals = residuals.unstack(stacked_dim)
6350                 variables[residuals.name] = residuals
6351 
6352             if cov:
6353                 Vbase = np.linalg.inv(np.dot(lhs.T, lhs))
6354                 Vbase /= np.outer(scale, scale)
6355                 if cov == "unscaled":
6356                     fac = 1
6357                 else:
6358                     if x.shape[0] <= order:
6359                         raise ValueError(
6360                             "The number of data points must exceed order to scale the covariance matrix."
6361                         )
6362                     fac = residuals / (x.shape[0] - order)
6363                 covariance = xr.DataArray(Vbase, dims=("cov_i", "cov_j")) * fac
6364                 variables[name + "polyfit_covariance"] = covariance
6365 
6366         return Dataset(data_vars=variables, attrs=self.attrs.copy())
6367 
6368     def pad(
6369         self,
6370         pad_width: Mapping[Hashable, Union[int, Tuple[int, int]]] = None,
6371         mode: str = "constant",
6372         stat_length: Union[
6373             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6374         ] = None,
6375         constant_values: Union[
6376             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6377         ] = None,
6378         end_values: Union[
6379             int, Tuple[int, int], Mapping[Hashable, Tuple[int, int]]
6380         ] = None,
6381         reflect_type: str = None,
6382         **pad_width_kwargs: Any,
6383     ) -> "Dataset":
6384         """Pad this dataset along one or more dimensions.
6385 
6386         .. warning::
6387             This function is experimental and its behaviour is likely to change
6388             especially regarding padding of dimension coordinates (or IndexVariables).
6389 
6390         When using one of the modes ("edge", "reflect", "symmetric", "wrap"),
6391         coordinates will be padded with the same mode, otherwise coordinates
6392         are padded using the "constant" mode with fill_value dtypes.NA.
6393 
6394         Parameters
6395         ----------
6396         pad_width : mapping of hashable to tuple of int
6397             Mapping with the form of {dim: (pad_before, pad_after)}
6398             describing the number of values padded along each dimension.
6399             {dim: pad} is a shortcut for pad_before = pad_after = pad
6400         mode : str, default: "constant"
6401             One of the following string values (taken from numpy docs).
6402 
6403             'constant' (default)
6404                 Pads with a constant value.
6405             'edge'
6406                 Pads with the edge values of array.
6407             'linear_ramp'
6408                 Pads with the linear ramp between end_value and the
6409                 array edge value.
6410             'maximum'
6411                 Pads with the maximum value of all or part of the
6412                 vector along each axis.
6413             'mean'
6414                 Pads with the mean value of all or part of the
6415                 vector along each axis.
6416             'median'
6417                 Pads with the median value of all or part of the
6418                 vector along each axis.
6419             'minimum'
6420                 Pads with the minimum value of all or part of the
6421                 vector along each axis.
6422             'reflect'
6423                 Pads with the reflection of the vector mirrored on
6424                 the first and last values of the vector along each
6425                 axis.
6426             'symmetric'
6427                 Pads with the reflection of the vector mirrored
6428                 along the edge of the array.
6429             'wrap'
6430                 Pads with the wrap of the vector along the axis.
6431                 The first values are used to pad the end and the
6432                 end values are used to pad the beginning.
6433         stat_length : int, tuple or mapping of hashable to tuple, default: None
6434             Used in 'maximum', 'mean', 'median', and 'minimum'.  Number of
6435             values at edge of each axis used to calculate the statistic value.
6436             {dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)} unique
6437             statistic lengths along each dimension.
6438             ((before, after),) yields same before and after statistic lengths
6439             for each dimension.
6440             (stat_length,) or int is a shortcut for before = after = statistic
6441             length for all axes.
6442             Default is ``None``, to use the entire axis.
6443         constant_values : scalar, tuple or mapping of hashable to tuple, default: 0
6444             Used in 'constant'.  The values to set the padded values for each
6445             axis.
6446             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6447             pad constants along each dimension.
6448             ``((before, after),)`` yields same before and after constants for each
6449             dimension.
6450             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6451             all dimensions.
6452             Default is 0.
6453         end_values : scalar, tuple or mapping of hashable to tuple, default: 0
6454             Used in 'linear_ramp'.  The values used for the ending value of the
6455             linear_ramp and that will form the edge of the padded array.
6456             ``{dim_1: (before_1, after_1), ... dim_N: (before_N, after_N)}`` unique
6457             end values along each dimension.
6458             ``((before, after),)`` yields same before and after end values for each
6459             axis.
6460             ``(constant,)`` or ``constant`` is a shortcut for ``before = after = constant`` for
6461             all axes.
6462             Default is 0.
6463         reflect_type : {"even", "odd"}, optional
6464             Used in "reflect", and "symmetric".  The "even" style is the
6465             default with an unaltered reflection around the edge value.  For
6466             the "odd" style, the extended part of the array is created by
6467             subtracting the reflected values from two times the edge value.
6468         **pad_width_kwargs
6469             The keyword arguments form of ``pad_width``.
6470             One of ``pad_width`` or ``pad_width_kwargs`` must be provided.
6471 
6472         Returns
6473         -------
6474         padded : Dataset
6475             Dataset with the padded coordinates and data.
6476 
6477         See also
6478         --------
6479         Dataset.shift, Dataset.roll, Dataset.bfill, Dataset.ffill, numpy.pad, dask.array.pad
6480 
6481         Notes
6482         -----
6483         By default when ``mode="constant"`` and ``constant_values=None``, integer types will be
6484         promoted to ``float`` and padded with ``np.nan``. To avoid type promotion
6485         specify ``constant_values=np.nan``
6486 
6487         Examples
6488         --------
6489 
6490         >>> ds = xr.Dataset({"foo": ("x", range(5))})
6491         >>> ds.pad(x=(1, 2))
6492         <xarray.Dataset>
6493         Dimensions:  (x: 8)
6494         Dimensions without coordinates: x
6495         Data variables:
6496             foo      (x) float64 nan 0.0 1.0 2.0 3.0 4.0 nan nan
6497         """
6498         pad_width = either_dict_or_kwargs(pad_width, pad_width_kwargs, "pad")
6499 
6500         if mode in ("edge", "reflect", "symmetric", "wrap"):
6501             coord_pad_mode = mode
6502             coord_pad_options = {
6503                 "stat_length": stat_length,
6504                 "constant_values": constant_values,
6505                 "end_values": end_values,
6506                 "reflect_type": reflect_type,
6507             }
6508         else:
6509             coord_pad_mode = "constant"
6510             coord_pad_options = {}
6511 
6512         variables = {}
6513         for name, var in self.variables.items():
6514             var_pad_width = {k: v for k, v in pad_width.items() if k in var.dims}
6515             if not var_pad_width:
6516                 variables[name] = var
6517             elif name in self.data_vars:
6518                 variables[name] = var.pad(
6519                     pad_width=var_pad_width,
6520                     mode=mode,
6521                     stat_length=stat_length,
6522                     constant_values=constant_values,
6523                     end_values=end_values,
6524                     reflect_type=reflect_type,
6525                 )
6526             else:
6527                 variables[name] = var.pad(
6528                     pad_width=var_pad_width,
6529                     mode=coord_pad_mode,
6530                     **coord_pad_options,  # type: ignore
6531                 )
6532 
6533         return self._replace_vars_and_dims(variables)
6534 
6535     def idxmin(
6536         self,
6537         dim: Hashable = None,
6538         skipna: bool = None,
6539         fill_value: Any = dtypes.NA,
6540         keep_attrs: bool = None,
6541     ) -> "Dataset":
6542         """Return the coordinate label of the minimum value along a dimension.
6543 
6544         Returns a new `Dataset` named after the dimension with the values of
6545         the coordinate labels along that dimension corresponding to minimum
6546         values along that dimension.
6547 
6548         In comparison to :py:meth:`~Dataset.argmin`, this returns the
6549         coordinate label while :py:meth:`~Dataset.argmin` returns the index.
6550 
6551         Parameters
6552         ----------
6553         dim : str, optional
6554             Dimension over which to apply `idxmin`.  This is optional for 1D
6555             variables, but required for variables with 2 or more dimensions.
6556         skipna : bool or None, default: None
6557             If True, skip missing values (as marked by NaN). By default, only
6558             skips missing values for ``float``, ``complex``, and ``object``
6559             dtypes; other dtypes either do not have a sentinel missing value
6560             (``int``) or ``skipna=True`` has not been implemented
6561             (``datetime64`` or ``timedelta64``).
6562         fill_value : Any, default: NaN
6563             Value to be filled in case all of the values along a dimension are
6564             null.  By default this is NaN.  The fill value and result are
6565             automatically converted to a compatible dtype if possible.
6566             Ignored if ``skipna`` is False.
6567         keep_attrs : bool, default: False
6568             If True, the attributes (``attrs``) will be copied from the
6569             original object to the new one.  If False (default), the new object
6570             will be returned without attributes.
6571 
6572         Returns
6573         -------
6574         reduced : Dataset
6575             New `Dataset` object with `idxmin` applied to its data and the
6576             indicated dimension removed.
6577 
6578         See also
6579         --------
6580         DataArray.idxmin, Dataset.idxmax, Dataset.min, Dataset.argmin
6581 
6582         Examples
6583         --------
6584 
6585         >>> array1 = xr.DataArray(
6586         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6587         ... )
6588         >>> array2 = xr.DataArray(
6589         ...     [
6590         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6591         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6592         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6593         ...     ],
6594         ...     dims=["y", "x"],
6595         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6596         ... )
6597         >>> ds = xr.Dataset({"int": array1, "float": array2})
6598         >>> ds.min(dim="x")
6599         <xarray.Dataset>
6600         Dimensions:  (y: 3)
6601         Coordinates:
6602           * y        (y) int64 -1 0 1
6603         Data variables:
6604             int      int64 -2
6605             float    (y) float64 -2.0 -4.0 1.0
6606         >>> ds.argmin(dim="x")
6607         <xarray.Dataset>
6608         Dimensions:  (y: 3)
6609         Coordinates:
6610           * y        (y) int64 -1 0 1
6611         Data variables:
6612             int      int64 4
6613             float    (y) int64 4 0 2
6614         >>> ds.idxmin(dim="x")
6615         <xarray.Dataset>
6616         Dimensions:  (y: 3)
6617         Coordinates:
6618           * y        (y) int64 -1 0 1
6619         Data variables:
6620             int      <U1 'e'
6621             float    (y) object 'e' 'a' 'c'
6622         """
6623         return self.map(
6624             methodcaller(
6625                 "idxmin",
6626                 dim=dim,
6627                 skipna=skipna,
6628                 fill_value=fill_value,
6629                 keep_attrs=keep_attrs,
6630             )
6631         )
6632 
6633     def idxmax(
6634         self,
6635         dim: Hashable = None,
6636         skipna: bool = None,
6637         fill_value: Any = dtypes.NA,
6638         keep_attrs: bool = None,
6639     ) -> "Dataset":
6640         """Return the coordinate label of the maximum value along a dimension.
6641 
6642         Returns a new `Dataset` named after the dimension with the values of
6643         the coordinate labels along that dimension corresponding to maximum
6644         values along that dimension.
6645 
6646         In comparison to :py:meth:`~Dataset.argmax`, this returns the
6647         coordinate label while :py:meth:`~Dataset.argmax` returns the index.
6648 
6649         Parameters
6650         ----------
6651         dim : str, optional
6652             Dimension over which to apply `idxmax`.  This is optional for 1D
6653             variables, but required for variables with 2 or more dimensions.
6654         skipna : bool or None, default: None
6655             If True, skip missing values (as marked by NaN). By default, only
6656             skips missing values for ``float``, ``complex``, and ``object``
6657             dtypes; other dtypes either do not have a sentinel missing value
6658             (``int``) or ``skipna=True`` has not been implemented
6659             (``datetime64`` or ``timedelta64``).
6660         fill_value : Any, default: NaN
6661             Value to be filled in case all of the values along a dimension are
6662             null.  By default this is NaN.  The fill value and result are
6663             automatically converted to a compatible dtype if possible.
6664             Ignored if ``skipna`` is False.
6665         keep_attrs : bool, default: False
6666             If True, the attributes (``attrs``) will be copied from the
6667             original object to the new one.  If False (default), the new object
6668             will be returned without attributes.
6669 
6670         Returns
6671         -------
6672         reduced : Dataset
6673             New `Dataset` object with `idxmax` applied to its data and the
6674             indicated dimension removed.
6675 
6676         See also
6677         --------
6678         DataArray.idxmax, Dataset.idxmin, Dataset.max, Dataset.argmax
6679 
6680         Examples
6681         --------
6682 
6683         >>> array1 = xr.DataArray(
6684         ...     [0, 2, 1, 0, -2], dims="x", coords={"x": ["a", "b", "c", "d", "e"]}
6685         ... )
6686         >>> array2 = xr.DataArray(
6687         ...     [
6688         ...         [2.0, 1.0, 2.0, 0.0, -2.0],
6689         ...         [-4.0, np.NaN, 2.0, np.NaN, -2.0],
6690         ...         [np.NaN, np.NaN, 1.0, np.NaN, np.NaN],
6691         ...     ],
6692         ...     dims=["y", "x"],
6693         ...     coords={"y": [-1, 0, 1], "x": ["a", "b", "c", "d", "e"]},
6694         ... )
6695         >>> ds = xr.Dataset({"int": array1, "float": array2})
6696         >>> ds.max(dim="x")
6697         <xarray.Dataset>
6698         Dimensions:  (y: 3)
6699         Coordinates:
6700           * y        (y) int64 -1 0 1
6701         Data variables:
6702             int      int64 2
6703             float    (y) float64 2.0 2.0 1.0
6704         >>> ds.argmax(dim="x")
6705         <xarray.Dataset>
6706         Dimensions:  (y: 3)
6707         Coordinates:
6708           * y        (y) int64 -1 0 1
6709         Data variables:
6710             int      int64 1
6711             float    (y) int64 0 2 2
6712         >>> ds.idxmax(dim="x")
6713         <xarray.Dataset>
6714         Dimensions:  (y: 3)
6715         Coordinates:
6716           * y        (y) int64 -1 0 1
6717         Data variables:
6718             int      <U1 'b'
6719             float    (y) object 'a' 'c' 'c'
6720         """
6721         return self.map(
6722             methodcaller(
6723                 "idxmax",
6724                 dim=dim,
6725                 skipna=skipna,
6726                 fill_value=fill_value,
6727                 keep_attrs=keep_attrs,
6728             )
6729         )
6730 
6731     def argmin(self, dim=None, axis=None, **kwargs):
6732         """Indices of the minima of the member variables.
6733 
6734         If there are multiple minima, the indices of the first one found will be
6735         returned.
6736 
6737         Parameters
6738         ----------
6739         dim : str, optional
6740             The dimension over which to find the minimum. By default, finds minimum over
6741             all dimensions - for now returning an int for backward compatibility, but
6742             this is deprecated, in future will be an error, since DataArray.argmin will
6743             return a dict with indices for all dimensions, which does not make sense for
6744             a Dataset.
6745         axis : int, optional
6746             Axis over which to apply `argmin`. Only one of the 'dim' and 'axis' arguments
6747             can be supplied.
6748         keep_attrs : bool, optional
6749             If True, the attributes (`attrs`) will be copied from the original
6750             object to the new one.  If False (default), the new object will be
6751             returned without attributes.
6752         skipna : bool, optional
6753             If True, skip missing values (as marked by NaN). By default, only
6754             skips missing values for float dtypes; other dtypes either do not
6755             have a sentinel missing value (int) or skipna=True has not been
6756             implemented (object, datetime64 or timedelta64).
6757 
6758         Returns
6759         -------
6760         result : Dataset
6761 
6762         See also
6763         --------
6764         DataArray.argmin
6765 
6766         """
6767         if dim is None and axis is None:
6768             warnings.warn(
6769                 "Once the behaviour of DataArray.argmin() and Variable.argmin() with "
6770                 "neither dim nor axis argument changes to return a dict of indices of "
6771                 "each dimension, for consistency it will be an error to call "
6772                 "Dataset.argmin() with no argument, since we don't return a dict of "
6773                 "Datasets.",
6774                 DeprecationWarning,
6775                 stacklevel=2,
6776             )
6777         if (
6778             dim is None
6779             or axis is not None
6780             or (not isinstance(dim, Sequence) and dim is not ...)
6781             or isinstance(dim, str)
6782         ):
6783             # Return int index if single dimension is passed, and is not part of a
6784             # sequence
6785             argmin_func = getattr(duck_array_ops, "argmin")
6786             return self.reduce(argmin_func, dim=dim, axis=axis, **kwargs)
6787         else:
6788             raise ValueError(
6789                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6790                 "dicts cannot be contained in a Dataset, so cannot call "
6791                 "Dataset.argmin() with a sequence or ... for dim"
6792             )
6793 
6794     def argmax(self, dim=None, axis=None, **kwargs):
6795         """Indices of the maxima of the member variables.
6796 
6797         If there are multiple maxima, the indices of the first one found will be
6798         returned.
6799 
6800         Parameters
6801         ----------
6802         dim : str, optional
6803             The dimension over which to find the maximum. By default, finds maximum over
6804             all dimensions - for now returning an int for backward compatibility, but
6805             this is deprecated, in future will be an error, since DataArray.argmax will
6806             return a dict with indices for all dimensions, which does not make sense for
6807             a Dataset.
6808         axis : int, optional
6809             Axis over which to apply `argmax`. Only one of the 'dim' and 'axis' arguments
6810             can be supplied.
6811         keep_attrs : bool, optional
6812             If True, the attributes (`attrs`) will be copied from the original
6813             object to the new one.  If False (default), the new object will be
6814             returned without attributes.
6815         skipna : bool, optional
6816             If True, skip missing values (as marked by NaN). By default, only
6817             skips missing values for float dtypes; other dtypes either do not
6818             have a sentinel missing value (int) or skipna=True has not been
6819             implemented (object, datetime64 or timedelta64).
6820 
6821         Returns
6822         -------
6823         result : Dataset
6824 
6825         See also
6826         --------
6827         DataArray.argmax
6828 
6829         """
6830         if dim is None and axis is None:
6831             warnings.warn(
6832                 "Once the behaviour of DataArray.argmax() and Variable.argmax() with "
6833                 "neither dim nor axis argument changes to return a dict of indices of "
6834                 "each dimension, for consistency it will be an error to call "
6835                 "Dataset.argmax() with no argument, since we don't return a dict of "
6836                 "Datasets.",
6837                 DeprecationWarning,
6838                 stacklevel=2,
6839             )
6840         if (
6841             dim is None
6842             or axis is not None
6843             or (not isinstance(dim, Sequence) and dim is not ...)
6844             or isinstance(dim, str)
6845         ):
6846             # Return int index if single dimension is passed, and is not part of a
6847             # sequence
6848             argmax_func = getattr(duck_array_ops, "argmax")
6849             return self.reduce(argmax_func, dim=dim, axis=axis, **kwargs)
6850         else:
6851             raise ValueError(
6852                 "When dim is a sequence or ..., DataArray.argmin() returns a dict. "
6853                 "dicts cannot be contained in a Dataset, so cannot call "
6854                 "Dataset.argmin() with a sequence or ... for dim"
6855             )

</code><intended_behavior>The intended behavior at this location is to add a new method named `drop_isel` which would allow users to drop indices from a dimension without the need for coordinate labels. The `drop_isel` method should mimic the behavior of the `drop_sel` method but utilize integer-based indexing instead of label-based. This method should be designed to accept integer indices or boolean masks as inputs and remove the corresponding elements from the dataset. It should support multi-dimensional operations and ensure compatibility with the existing API designs. Exception handling should be robust, providing informative errors or warnings when invalid indices are provided, or when an operation cannot be performed due to dataset constraints. The existing functionality of the Dataset class should be preserved, ensuring that this new method integrates seamlessly without affecting the current capabilities. Integration with the dataset's indexing, selection, and manipulation interface should be considered, providing users with a coherent and flexible API for handling data within xarray.</intended_behavior>]
2025-10-18 15:29:58.148 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 15:29:58.148 | INFO     | Search-only: skipping patch selection
2025-10-18 15:29:58.214 | INFO     | Task pydata__xarray-4819 completed successfully.
2025-10-18 15:29:58.215 | INFO     | No patch generated. You can try running ACR again.
