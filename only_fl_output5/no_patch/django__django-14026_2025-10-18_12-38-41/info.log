2025-10-18 12:38:41.870 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 12:38:44.922 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=196, output_tokens=12, cost=0.002320
2025-10-18 12:38:44.923 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 12:38:44.927 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:38:53.260 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=505, output_tokens=309, cost=0.014320
2025-10-18 12:38:53.264 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:38:55.241 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=663, output_tokens=42, cost=0.007890
2025-10-18 12:38:55.242 | DEBUG    | Extracted a valid json.
2025-10-18 12:38:55.243 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"Sum\")",
        "search_method(\"annotate\")",
        "search_code(\"COALESCE\")"
    ],
    "bug_locations": []
}
2025-10-18 12:38:55.299 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 12:38:55.300 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 12:39:11.033 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1650, output_tokens=679, cost=0.036870
2025-10-18 12:39:11.045 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 12:39:19.474 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2545, output_tokens=341, cost=0.035680
2025-10-18 12:39:19.482 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 12:39:27.822 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=695, output_tokens=331, cost=0.016880
2025-10-18 12:39:27.823 | DEBUG    | Extracted a valid json.
2025-10-18 12:39:27.823 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "django/db/models/aggregates.py", 
            "class": "Sum", 
            "method": "", 
            "intended_behavior": "Modify or extend the Sum class to include an optional parameter that allows specifying a default value for cases where the result of the sum operation is NULL. This could involve either directly integrating COALESCE functionality within the class or providing an interface for the user to specify the desired behavior when encountering NULL values. The preservation of existing functionality is crucial; hence, the default behavior (without specifying a default value) should remain unchanged, returning NULL for empty sets or no values to sum. Adding this feature would enhance Sum by making it more flexible for various use cases, particularly those where a 0 is more desirable than NULL when no records are found."
        },
        {
            "file": "django/db/models/query.py", 
            "class": "QuerySet", 
            "method": "annotate", 
            "intended_behavior": "While the primary change to support a default value for Sum will likely be within the Sum class itself, ensuring that annotate properly supports this extended functionality is important. The intended behavior here would not necessarily be a direct modification but rather ensuring compatibility—that annotate can seamlessly pass through any additional parameters specified for Sum (or similar aggregated functions) to handle NULL values appropriately. This may involve adjustments in how parameters are handled or passed to aggregated functions within annotate to allow for the application of default values via COALESCE or similar mechanisms implicitly or explicitly specified by the developer."
        }
    ]
}
2025-10-18 12:39:27.836 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/aggregates.py</file>
<class>Sum</class>
<method>None</method>
<code>
142 class Sum(FixDurationInputMixin, Aggregate):
143     function = 'SUM'
144     name = 'Sum'
145     allow_distinct = True

</code><intended_behavior>Modify or extend the Sum class to include an optional parameter that allows specifying a default value for cases where the result of the sum operation is NULL. This could involve either directly integrating COALESCE functionality within the class or providing an interface for the user to specify the desired behavior when encountering NULL values. The preservation of existing functionality is crucial; hence, the default behavior (without specifying a default value) should remain unchanged, returning NULL for empty sets or no values to sum. Adding this feature would enhance Sum by making it more flexible for various use cases, particularly those where a 0 is more desirable than NULL when no records are found.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1114     def annotate(self, *args, **kwargs):
1115         """
1116         Return a query set in which the returned objects have been annotated
1117         with extra data or aggregations.
1118         """
1119         self._not_support_combined_queries('annotate')
1120         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>While the primary change to support a default value for Sum will likely be within the Sum class itself, ensuring that annotate properly supports this extended functionality is important. The intended behavior here would not necessarily be a direct modification but rather ensuring compatibility—that annotate can seamlessly pass through any additional parameters specified for Sum (or similar aggregated functions) to handle NULL values appropriately. This may involve adjustments in how parameters are handled or passed to aggregated functions within annotate to allow for the application of default values via COALESCE or similar mechanisms implicitly or explicitly specified by the developer.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         if (
295             (isinstance(k, int) and k < 0) or
296             (isinstance(k, slice) and (
297                 (k.start is not None and k.start < 0) or
298                 (k.stop is not None and k.stop < 0)
299             ))
300         ):
301             raise ValueError('Negative indexing is not supported.')
302 
303         if self._result_cache is not None:
304             return self._result_cache[k]
305 
306         if isinstance(k, slice):
307             qs = self._chain()
308             if k.start is not None:
309                 start = int(k.start)
310             else:
311                 start = None
312             if k.stop is not None:
313                 stop = int(k.stop)
314             else:
315                 stop = None
316             qs.query.set_limits(start, stop)
317             return list(qs)[::k.step] if k.step else qs
318 
319         qs = self._chain()
320         qs.query.set_limits(k, k + 1)
321         qs._fetch_all()
322         return qs._result_cache[0]
323 
324     def __class_getitem__(cls, *args, **kwargs):
325         return cls
326 
327     def __and__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(other, EmptyQuerySet):
330             return other
331         if isinstance(self, EmptyQuerySet):
332             return self
333         combined = self._chain()
334         combined._merge_known_related_objects(other)
335         combined.query.combine(other.query, sql.AND)
336         return combined
337 
338     def __or__(self, other):
339         self._merge_sanity_check(other)
340         if isinstance(self, EmptyQuerySet):
341             return other
342         if isinstance(other, EmptyQuerySet):
343             return self
344         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
345         combined = query._chain()
346         combined._merge_known_related_objects(other)
347         if not other.query.can_filter():
348             other = other.model._base_manager.filter(pk__in=other.values('pk'))
349         combined.query.combine(other.query, sql.OR)
350         return combined
351 
352     ####################################
353     # METHODS THAT DO DATABASE QUERIES #
354     ####################################
355 
356     def _iterator(self, use_chunked_fetch, chunk_size):
357         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
358 
359     def iterator(self, chunk_size=2000):
360         """
361         An iterator over the results from applying this QuerySet to the
362         database.
363         """
364         if chunk_size <= 0:
365             raise ValueError('Chunk size must be strictly positive.')
366         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
367         return self._iterator(use_chunked_fetch, chunk_size)
368 
369     def aggregate(self, *args, **kwargs):
370         """
371         Return a dictionary containing the calculations (aggregation)
372         over the current queryset.
373 
374         If args is present the expression is passed as a kwarg using
375         the Aggregate object's default alias.
376         """
377         if self.query.distinct_fields:
378             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
379         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
380         for arg in args:
381             # The default_alias property raises TypeError if default_alias
382             # can't be set automatically or AttributeError if it isn't an
383             # attribute.
384             try:
385                 arg.default_alias
386             except (AttributeError, TypeError):
387                 raise TypeError("Complex aggregates require an alias")
388             kwargs[arg.default_alias] = arg
389 
390         query = self.query.chain()
391         for (alias, aggregate_expr) in kwargs.items():
392             query.add_annotation(aggregate_expr, alias, is_summary=True)
393             annotation = query.annotations[alias]
394             if not annotation.contains_aggregate:
395                 raise TypeError("%s is not an aggregate expression" % alias)
396             for expr in annotation.get_source_expressions():
397                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
398                     name = expr.refs
399                     raise exceptions.FieldError(
400                         "Cannot compute %s('%s'): '%s' is an aggregate"
401                         % (annotation.name, name, name)
402                     )
403         return query.get_aggregation(self.db, kwargs)
404 
405     def count(self):
406         """
407         Perform a SELECT COUNT() and return the number of records as an
408         integer.
409 
410         If the QuerySet is already fully cached, return the length of the
411         cached results set to avoid multiple SELECT COUNT(*) calls.
412         """
413         if self._result_cache is not None:
414             return len(self._result_cache)
415 
416         return self.query.get_count(using=self.db)
417 
418     def get(self, *args, **kwargs):
419         """
420         Perform the query and return a single object matching the given
421         keyword arguments.
422         """
423         if self.query.combinator and (args or kwargs):
424             raise NotSupportedError(
425                 'Calling QuerySet.get(...) with filters after %s() is not '
426                 'supported.' % self.query.combinator
427             )
428         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
429         if self.query.can_filter() and not self.query.distinct_fields:
430             clone = clone.order_by()
431         limit = None
432         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
433             limit = MAX_GET_RESULTS
434             clone.query.set_limits(high=limit)
435         num = len(clone)
436         if num == 1:
437             return clone._result_cache[0]
438         if not num:
439             raise self.model.DoesNotExist(
440                 "%s matching query does not exist." %
441                 self.model._meta.object_name
442             )
443         raise self.model.MultipleObjectsReturned(
444             'get() returned more than one %s -- it returned %s!' % (
445                 self.model._meta.object_name,
446                 num if not limit or num < limit else 'more than %s' % (limit - 1),
447             )
448         )
449 
450     def create(self, **kwargs):
451         """
452         Create a new object with the given kwargs, saving it to the database
453         and returning the created object.
454         """
455         obj = self.model(**kwargs)
456         self._for_write = True
457         obj.save(force_insert=True, using=self.db)
458         return obj
459 
460     def _prepare_for_bulk_create(self, objs):
461         for obj in objs:
462             if obj.pk is None:
463                 # Populate new PK values.
464                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
465             obj._prepare_related_fields_for_save(operation_name='bulk_create')
466 
467     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
468         """
469         Insert each of the instances into the database. Do *not* call
470         save() on each of the instances, do not send any pre/post_save
471         signals, and do not set the primary key attribute if it is an
472         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
473         Multi-table models are not supported.
474         """
475         # When you bulk insert you don't get the primary keys back (if it's an
476         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
477         # you can't insert into the child tables which references this. There
478         # are two workarounds:
479         # 1) This could be implemented if you didn't have an autoincrement pk
480         # 2) You could do it by doing O(n) normal inserts into the parent
481         #    tables to get the primary keys back and then doing a single bulk
482         #    insert into the childmost table.
483         # We currently set the primary keys on the objects when using
484         # PostgreSQL via the RETURNING ID clause. It should be possible for
485         # Oracle as well, but the semantics for extracting the primary keys is
486         # trickier so it's not done yet.
487         if batch_size is not None and batch_size <= 0:
488             raise ValueError('Batch size must be a positive integer.')
489         # Check that the parents share the same concrete model with the our
490         # model to detect the inheritance pattern ConcreteGrandParent ->
491         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
492         # would not identify that case as involving multiple tables.
493         for parent in self.model._meta.get_parent_list():
494             if parent._meta.concrete_model is not self.model._meta.concrete_model:
495                 raise ValueError("Can't bulk create a multi-table inherited model")
496         if not objs:
497             return objs
498         self._for_write = True
499         connection = connections[self.db]
500         opts = self.model._meta
501         fields = opts.concrete_fields
502         objs = list(objs)
503         self._prepare_for_bulk_create(objs)
504         with transaction.atomic(using=self.db, savepoint=False):
505             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
506             if objs_with_pk:
507                 returned_columns = self._batched_insert(
508                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
509                 )
510                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
511                     for result, field in zip(results, opts.db_returning_fields):
512                         if field != opts.pk:
513                             setattr(obj_with_pk, field.attname, result)
514                 for obj_with_pk in objs_with_pk:
515                     obj_with_pk._state.adding = False
516                     obj_with_pk._state.db = self.db
517             if objs_without_pk:
518                 fields = [f for f in fields if not isinstance(f, AutoField)]
519                 returned_columns = self._batched_insert(
520                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
521                 )
522                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
523                     assert len(returned_columns) == len(objs_without_pk)
524                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
525                     for result, field in zip(results, opts.db_returning_fields):
526                         setattr(obj_without_pk, field.attname, result)
527                     obj_without_pk._state.adding = False
528                     obj_without_pk._state.db = self.db
529 
530         return objs
531 
532     def bulk_update(self, objs, fields, batch_size=None):
533         """
534         Update the given fields in each of the given objects in the database.
535         """
536         if batch_size is not None and batch_size < 0:
537             raise ValueError('Batch size must be a positive integer.')
538         if not fields:
539             raise ValueError('Field names must be given to bulk_update().')
540         objs = tuple(objs)
541         if any(obj.pk is None for obj in objs):
542             raise ValueError('All bulk_update() objects must have a primary key set.')
543         fields = [self.model._meta.get_field(name) for name in fields]
544         if any(not f.concrete or f.many_to_many for f in fields):
545             raise ValueError('bulk_update() can only be used with concrete fields.')
546         if any(f.primary_key for f in fields):
547             raise ValueError('bulk_update() cannot be used with primary key fields.')
548         if not objs:
549             return 0
550         # PK is used twice in the resulting update query, once in the filter
551         # and once in the WHEN. Each field will also have one CAST.
552         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
553         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
554         requires_casting = connections[self.db].features.requires_casted_case_in_updates
555         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
556         updates = []
557         for batch_objs in batches:
558             update_kwargs = {}
559             for field in fields:
560                 when_statements = []
561                 for obj in batch_objs:
562                     attr = getattr(obj, field.attname)
563                     if not isinstance(attr, Expression):
564                         attr = Value(attr, output_field=field)
565                     when_statements.append(When(pk=obj.pk, then=attr))
566                 case_statement = Case(*when_statements, output_field=field)
567                 if requires_casting:
568                     case_statement = Cast(case_statement, output_field=field)
569                 update_kwargs[field.attname] = case_statement
570             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
571         rows_updated = 0
572         with transaction.atomic(using=self.db, savepoint=False):
573             for pks, update_kwargs in updates:
574                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
575         return rows_updated
576     bulk_update.alters_data = True
577 
578     def get_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, creating one if necessary.
581         Return a tuple of (object, created), where created is a boolean
582         specifying whether an object was created.
583         """
584         # The get() needs to be targeted at the write database in order
585         # to avoid potential transaction consistency problems.
586         self._for_write = True
587         try:
588             return self.get(**kwargs), False
589         except self.model.DoesNotExist:
590             params = self._extract_model_params(defaults, **kwargs)
591             # Try to create an object using passed params.
592             try:
593                 with transaction.atomic(using=self.db):
594                     params = dict(resolve_callables(params))
595                     return self.create(**params), True
596             except IntegrityError:
597                 try:
598                     return self.get(**kwargs), False
599                 except self.model.DoesNotExist:
600                     pass
601                 raise
602 
603     def update_or_create(self, defaults=None, **kwargs):
604         """
605         Look up an object with the given kwargs, updating one with defaults
606         if it exists, otherwise create a new one.
607         Return a tuple (object, created), where created is a boolean
608         specifying whether an object was created.
609         """
610         defaults = defaults or {}
611         self._for_write = True
612         with transaction.atomic(using=self.db):
613             # Lock the row so that a concurrent update is blocked until
614             # update_or_create() has performed its save.
615             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
616             if created:
617                 return obj, created
618             for k, v in resolve_callables(defaults):
619                 setattr(obj, k, v)
620             obj.save(using=self.db)
621         return obj, False
622 
623     def _extract_model_params(self, defaults, **kwargs):
624         """
625         Prepare `params` for creating a model instance based on the given
626         kwargs; for use by get_or_create().
627         """
628         defaults = defaults or {}
629         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
630         params.update(defaults)
631         property_names = self.model._meta._property_names
632         invalid_params = []
633         for param in params:
634             try:
635                 self.model._meta.get_field(param)
636             except exceptions.FieldDoesNotExist:
637                 # It's okay to use a model's property if it has a setter.
638                 if not (param in property_names and getattr(self.model, param).fset):
639                     invalid_params.append(param)
640         if invalid_params:
641             raise exceptions.FieldError(
642                 "Invalid field name(s) for model %s: '%s'." % (
643                     self.model._meta.object_name,
644                     "', '".join(sorted(invalid_params)),
645                 ))
646         return params
647 
648     def _earliest(self, *fields):
649         """
650         Return the earliest object according to fields (if given) or by the
651         model's Meta.get_latest_by.
652         """
653         if fields:
654             order_by = fields
655         else:
656             order_by = getattr(self.model._meta, 'get_latest_by')
657             if order_by and not isinstance(order_by, (tuple, list)):
658                 order_by = (order_by,)
659         if order_by is None:
660             raise ValueError(
661                 "earliest() and latest() require either fields as positional "
662                 "arguments or 'get_latest_by' in the model's Meta."
663             )
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         if self.query.is_sliced:
672             raise TypeError('Cannot change a query once a slice has been taken.')
673         return self._earliest(*fields)
674 
675     def latest(self, *fields):
676         if self.query.is_sliced:
677             raise TypeError('Cannot change a query once a slice has been taken.')
678         return self.reverse()._earliest(*fields)
679 
680     def first(self):
681         """Return the first object of a query or None if no match is found."""
682         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
683             return obj
684 
685     def last(self):
686         """Return the last object of a query or None if no match is found."""
687         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
688             return obj
689 
690     def in_bulk(self, id_list=None, *, field_name='pk'):
691         """
692         Return a dictionary mapping each of the given IDs to the object with
693         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
694         """
695         if self.query.is_sliced:
696             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
697         opts = self.model._meta
698         unique_fields = [
699             constraint.fields[0]
700             for constraint in opts.total_unique_constraints
701             if len(constraint.fields) == 1
702         ]
703         if (
704             field_name != 'pk' and
705             not opts.get_field(field_name).unique and
706             field_name not in unique_fields and
707             self.query.distinct_fields != (field_name,)
708         ):
709             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
710         if id_list is not None:
711             if not id_list:
712                 return {}
713             filter_key = '{}__in'.format(field_name)
714             batch_size = connections[self.db].features.max_query_params
715             id_list = tuple(id_list)
716             # If the database has a limit on the number of query parameters
717             # (e.g. SQLite), retrieve objects in batches if necessary.
718             if batch_size and batch_size < len(id_list):
719                 qs = ()
720                 for offset in range(0, len(id_list), batch_size):
721                     batch = id_list[offset:offset + batch_size]
722                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
723             else:
724                 qs = self.filter(**{filter_key: id_list}).order_by()
725         else:
726             qs = self._chain()
727         return {getattr(obj, field_name): obj for obj in qs}
728 
729     def delete(self):
730         """Delete the records in the current QuerySet."""
731         self._not_support_combined_queries('delete')
732         if self.query.is_sliced:
733             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
734         if self.query.distinct or self.query.distinct_fields:
735             raise TypeError('Cannot call delete() after .distinct().')
736         if self._fields is not None:
737             raise TypeError("Cannot call delete() after .values() or .values_list()")
738 
739         del_query = self._chain()
740 
741         # The delete is actually 2 queries - one to find related objects,
742         # and one to delete. Make sure that the discovery of related
743         # objects is performed on the same database as the deletion.
744         del_query._for_write = True
745 
746         # Disable non-supported fields.
747         del_query.query.select_for_update = False
748         del_query.query.select_related = False
749         del_query.query.clear_ordering(force=True)
750 
751         collector = Collector(using=del_query.db)
752         collector.collect(del_query)
753         deleted, _rows_count = collector.delete()
754 
755         # Clear the result cache, in case this QuerySet gets reused.
756         self._result_cache = None
757         return deleted, _rows_count
758 
759     delete.alters_data = True
760     delete.queryset_only = True
761 
762     def _raw_delete(self, using):
763         """
764         Delete objects found from the given queryset in single direct SQL
765         query. No signals are sent and there is no protection for cascades.
766         """
767         query = self.query.clone()
768         query.__class__ = sql.DeleteQuery
769         cursor = query.get_compiler(using).execute_sql(CURSOR)
770         if cursor:
771             with cursor:
772                 return cursor.rowcount
773         return 0
774     _raw_delete.alters_data = True
775 
776     def update(self, **kwargs):
777         """
778         Update all elements in the current QuerySet, setting all the given
779         fields to the appropriate values.
780         """
781         self._not_support_combined_queries('update')
782         if self.query.is_sliced:
783             raise TypeError('Cannot update a query once a slice has been taken.')
784         self._for_write = True
785         query = self.query.chain(sql.UpdateQuery)
786         query.add_update_values(kwargs)
787         # Clear any annotations so that they won't be present in subqueries.
788         query.annotations = {}
789         with transaction.mark_for_rollback_on_error(using=self.db):
790             rows = query.get_compiler(self.db).execute_sql(CURSOR)
791         self._result_cache = None
792         return rows
793     update.alters_data = True
794 
795     def _update(self, values):
796         """
797         A version of update() that accepts field objects instead of field names.
798         Used primarily for model saving and not intended for use by general
799         code (it requires too much poking around at model internals to be
800         useful at that level).
801         """
802         if self.query.is_sliced:
803             raise TypeError('Cannot update a query once a slice has been taken.')
804         query = self.query.chain(sql.UpdateQuery)
805         query.add_update_fields(values)
806         # Clear any annotations so that they won't be present in subqueries.
807         query.annotations = {}
808         self._result_cache = None
809         return query.get_compiler(self.db).execute_sql(CURSOR)
810     _update.alters_data = True
811     _update.queryset_only = False
812 
813     def exists(self):
814         if self._result_cache is None:
815             return self.query.has_results(using=self.db)
816         return bool(self._result_cache)
817 
818     def contains(self, obj):
819         """Return True if the queryset contains an object."""
820         self._not_support_combined_queries('contains')
821         if self._fields is not None:
822             raise TypeError(
823                 'Cannot call QuerySet.contains() after .values() or '
824                 '.values_list().'
825             )
826         try:
827             if obj._meta.concrete_model != self.model._meta.concrete_model:
828                 return False
829         except AttributeError:
830             raise TypeError("'obj' must be a model instance.")
831         if obj.pk is None:
832             raise ValueError(
833                 'QuerySet.contains() cannot be used on unsaved objects.'
834             )
835         if self._result_cache is not None:
836             return obj in self._result_cache
837         return self.filter(pk=obj.pk).exists()
838 
839     def _prefetch_related_objects(self):
840         # This method can only be called once the result cache has been filled.
841         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
842         self._prefetch_done = True
843 
844     def explain(self, *, format=None, **options):
845         return self.query.explain(using=self.db, format=format, **options)
846 
847     ##################################################
848     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
849     ##################################################
850 
851     def raw(self, raw_query, params=(), translations=None, using=None):
852         if using is None:
853             using = self.db
854         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
855         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
856         return qs
857 
858     def _values(self, *fields, **expressions):
859         clone = self._chain()
860         if expressions:
861             clone = clone.annotate(**expressions)
862         clone._fields = fields
863         clone.query.set_values(fields)
864         return clone
865 
866     def values(self, *fields, **expressions):
867         fields += tuple(expressions)
868         clone = self._values(*fields, **expressions)
869         clone._iterable_class = ValuesIterable
870         return clone
871 
872     def values_list(self, *fields, flat=False, named=False):
873         if flat and named:
874             raise TypeError("'flat' and 'named' can't be used together.")
875         if flat and len(fields) > 1:
876             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
877 
878         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
879         _fields = []
880         expressions = {}
881         counter = 1
882         for field in fields:
883             if hasattr(field, 'resolve_expression'):
884                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
885                 while True:
886                     field_id = field_id_prefix + str(counter)
887                     counter += 1
888                     if field_id not in field_names:
889                         break
890                 expressions[field_id] = field
891                 _fields.append(field_id)
892             else:
893                 _fields.append(field)
894 
895         clone = self._values(*_fields, **expressions)
896         clone._iterable_class = (
897             NamedValuesListIterable if named
898             else FlatValuesListIterable if flat
899             else ValuesListIterable
900         )
901         return clone
902 
903     def dates(self, field_name, kind, order='ASC'):
904         """
905         Return a list of date objects representing all available dates for
906         the given field_name, scoped to 'kind'.
907         """
908         if kind not in ('year', 'month', 'week', 'day'):
909             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
910         if order not in ('ASC', 'DESC'):
911             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
912         return self.annotate(
913             datefield=Trunc(field_name, kind, output_field=DateField()),
914             plain_field=F(field_name)
915         ).values_list(
916             'datefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
918 
919     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
920         """
921         Return a list of datetime objects representing all available
922         datetimes for the given field_name, scoped to 'kind'.
923         """
924         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
925             raise ValueError(
926                 "'kind' must be one of 'year', 'month', 'week', 'day', "
927                 "'hour', 'minute', or 'second'."
928             )
929         if order not in ('ASC', 'DESC'):
930             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
931         if settings.USE_TZ:
932             if tzinfo is None:
933                 tzinfo = timezone.get_current_timezone()
934         else:
935             tzinfo = None
936         return self.annotate(
937             datetimefield=Trunc(
938                 field_name,
939                 kind,
940                 output_field=DateTimeField(),
941                 tzinfo=tzinfo,
942                 is_dst=is_dst,
943             ),
944             plain_field=F(field_name)
945         ).values_list(
946             'datetimefield', flat=True
947         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
948 
949     def none(self):
950         """Return an empty QuerySet."""
951         clone = self._chain()
952         clone.query.set_empty()
953         return clone
954 
955     ##################################################################
956     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
957     ##################################################################
958 
959     def all(self):
960         """
961         Return a new QuerySet that is a copy of the current one. This allows a
962         QuerySet to proxy for a model manager in some cases.
963         """
964         return self._chain()
965 
966     def filter(self, *args, **kwargs):
967         """
968         Return a new QuerySet instance with the args ANDed to the existing
969         set.
970         """
971         self._not_support_combined_queries('filter')
972         return self._filter_or_exclude(False, args, kwargs)
973 
974     def exclude(self, *args, **kwargs):
975         """
976         Return a new QuerySet instance with NOT (args) ANDed to the existing
977         set.
978         """
979         self._not_support_combined_queries('exclude')
980         return self._filter_or_exclude(True, args, kwargs)
981 
982     def _filter_or_exclude(self, negate, args, kwargs):
983         if (args or kwargs) and self.query.is_sliced:
984             raise TypeError('Cannot filter a query once a slice has been taken.')
985         clone = self._chain()
986         if self._defer_next_filter:
987             self._defer_next_filter = False
988             clone._deferred_filter = negate, args, kwargs
989         else:
990             clone._filter_or_exclude_inplace(negate, args, kwargs)
991         return clone
992 
993     def _filter_or_exclude_inplace(self, negate, args, kwargs):
994         if negate:
995             self._query.add_q(~Q(*args, **kwargs))
996         else:
997             self._query.add_q(Q(*args, **kwargs))
998 
999     def complex_filter(self, filter_obj):
1000         """
1001         Return a new QuerySet instance with filter_obj added to the filters.
1002 
1003         filter_obj can be a Q object or a dictionary of keyword lookup
1004         arguments.
1005 
1006         This exists to support framework features such as 'limit_choices_to',
1007         and usually it will be more natural to use other methods.
1008         """
1009         if isinstance(filter_obj, Q):
1010             clone = self._chain()
1011             clone.query.add_q(filter_obj)
1012             return clone
1013         else:
1014             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1015 
1016     def _combinator_query(self, combinator, *other_qs, all=False):
1017         # Clone the query to inherit the select list and everything
1018         clone = self._chain()
1019         # Clear limits and ordering so they can be reapplied
1020         clone.query.clear_ordering(force=True)
1021         clone.query.clear_limits()
1022         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1023         clone.query.combinator = combinator
1024         clone.query.combinator_all = all
1025         return clone
1026 
1027     def union(self, *other_qs, all=False):
1028         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1029         if isinstance(self, EmptyQuerySet):
1030             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1031             if not qs:
1032                 return self
1033             if len(qs) == 1:
1034                 return qs[0]
1035             return qs[0]._combinator_query('union', *qs[1:], all=all)
1036         return self._combinator_query('union', *other_qs, all=all)
1037 
1038     def intersection(self, *other_qs):
1039         # If any query is an EmptyQuerySet, return it.
1040         if isinstance(self, EmptyQuerySet):
1041             return self
1042         for other in other_qs:
1043             if isinstance(other, EmptyQuerySet):
1044                 return other
1045         return self._combinator_query('intersection', *other_qs)
1046 
1047     def difference(self, *other_qs):
1048         # If the query is an EmptyQuerySet, return it.
1049         if isinstance(self, EmptyQuerySet):
1050             return self
1051         return self._combinator_query('difference', *other_qs)
1052 
1053     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1054         """
1055         Return a new QuerySet instance that will select objects with a
1056         FOR UPDATE lock.
1057         """
1058         if nowait and skip_locked:
1059             raise ValueError('The nowait option cannot be used with skip_locked.')
1060         obj = self._chain()
1061         obj._for_write = True
1062         obj.query.select_for_update = True
1063         obj.query.select_for_update_nowait = nowait
1064         obj.query.select_for_update_skip_locked = skip_locked
1065         obj.query.select_for_update_of = of
1066         obj.query.select_for_no_key_update = no_key
1067         return obj
1068 
1069     def select_related(self, *fields):
1070         """
1071         Return a new QuerySet instance that will select related objects.
1072 
1073         If fields are specified, they must be ForeignKey fields and only those
1074         related objects are included in the selection.
1075 
1076         If select_related(None) is called, clear the list.
1077         """
1078         self._not_support_combined_queries('select_related')
1079         if self._fields is not None:
1080             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1081 
1082         obj = self._chain()
1083         if fields == (None,):
1084             obj.query.select_related = False
1085         elif fields:
1086             obj.query.add_select_related(fields)
1087         else:
1088             obj.query.select_related = True
1089         return obj
1090 
1091     def prefetch_related(self, *lookups):
1092         """
1093         Return a new QuerySet instance that will prefetch the specified
1094         Many-To-One and Many-To-Many related objects when the QuerySet is
1095         evaluated.
1096 
1097         When prefetch_related() is called more than once, append to the list of
1098         prefetch lookups. If prefetch_related(None) is called, clear the list.
1099         """
1100         self._not_support_combined_queries('prefetch_related')
1101         clone = self._chain()
1102         if lookups == (None,):
1103             clone._prefetch_related_lookups = ()
1104         else:
1105             for lookup in lookups:
1106                 if isinstance(lookup, Prefetch):
1107                     lookup = lookup.prefetch_to
1108                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1109                 if lookup in self.query._filtered_relations:
1110                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1111             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1112         return clone
1113 
1114     def annotate(self, *args, **kwargs):
1115         """
1116         Return a query set in which the returned objects have been annotated
1117         with extra data or aggregations.
1118         """
1119         self._not_support_combined_queries('annotate')
1120         return self._annotate(args, kwargs, select=True)
1121 
1122     def alias(self, *args, **kwargs):
1123         """
1124         Return a query set with added aliases for extra data or aggregations.
1125         """
1126         self._not_support_combined_queries('alias')
1127         return self._annotate(args, kwargs, select=False)
1128 
1129     def _annotate(self, args, kwargs, select=True):
1130         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1131         annotations = {}
1132         for arg in args:
1133             # The default_alias property may raise a TypeError.
1134             try:
1135                 if arg.default_alias in kwargs:
1136                     raise ValueError("The named annotation '%s' conflicts with the "
1137                                      "default name for another annotation."
1138                                      % arg.default_alias)
1139             except TypeError:
1140                 raise TypeError("Complex annotations require an alias")
1141             annotations[arg.default_alias] = arg
1142         annotations.update(kwargs)
1143 
1144         clone = self._chain()
1145         names = self._fields
1146         if names is None:
1147             names = set(chain.from_iterable(
1148                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1149                 for field in self.model._meta.get_fields()
1150             ))
1151 
1152         for alias, annotation in annotations.items():
1153             if alias in names:
1154                 raise ValueError("The annotation '%s' conflicts with a field on "
1155                                  "the model." % alias)
1156             if isinstance(annotation, FilteredRelation):
1157                 clone.query.add_filtered_relation(annotation, alias)
1158             else:
1159                 clone.query.add_annotation(
1160                     annotation, alias, is_summary=False, select=select,
1161                 )
1162         for alias, annotation in clone.query.annotations.items():
1163             if alias in annotations and annotation.contains_aggregate:
1164                 if clone._fields is None:
1165                     clone.query.group_by = True
1166                 else:
1167                     clone.query.set_group_by()
1168                 break
1169 
1170         return clone
1171 
1172     def order_by(self, *field_names):
1173         """Return a new QuerySet instance with the ordering changed."""
1174         if self.query.is_sliced:
1175             raise TypeError('Cannot reorder a query once a slice has been taken.')
1176         obj = self._chain()
1177         obj.query.clear_ordering(force=True, clear_default=False)
1178         obj.query.add_ordering(*field_names)
1179         return obj
1180 
1181     def distinct(self, *field_names):
1182         """
1183         Return a new QuerySet instance that will select only distinct results.
1184         """
1185         self._not_support_combined_queries('distinct')
1186         if self.query.is_sliced:
1187             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1188         obj = self._chain()
1189         obj.query.add_distinct_fields(*field_names)
1190         return obj
1191 
1192     def extra(self, select=None, where=None, params=None, tables=None,
1193               order_by=None, select_params=None):
1194         """Add extra SQL fragments to the query."""
1195         self._not_support_combined_queries('extra')
1196         if self.query.is_sliced:
1197             raise TypeError('Cannot change a query once a slice has been taken.')
1198         clone = self._chain()
1199         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1200         return clone
1201 
1202     def reverse(self):
1203         """Reverse the ordering of the QuerySet."""
1204         if self.query.is_sliced:
1205             raise TypeError('Cannot reverse a query once a slice has been taken.')
1206         clone = self._chain()
1207         clone.query.standard_ordering = not clone.query.standard_ordering
1208         return clone
1209 
1210     def defer(self, *fields):
1211         """
1212         Defer the loading of data for certain fields until they are accessed.
1213         Add the set of deferred fields to any existing set of deferred fields.
1214         The only exception to this is if None is passed in as the only
1215         parameter, in which case removal all deferrals.
1216         """
1217         self._not_support_combined_queries('defer')
1218         if self._fields is not None:
1219             raise TypeError("Cannot call defer() after .values() or .values_list()")
1220         clone = self._chain()
1221         if fields == (None,):
1222             clone.query.clear_deferred_loading()
1223         else:
1224             clone.query.add_deferred_loading(fields)
1225         return clone
1226 
1227     def only(self, *fields):
1228         """
1229         Essentially, the opposite of defer(). Only the fields passed into this
1230         method and that are not already specified as deferred are loaded
1231         immediately when the queryset is evaluated.
1232         """
1233         self._not_support_combined_queries('only')
1234         if self._fields is not None:
1235             raise TypeError("Cannot call only() after .values() or .values_list()")
1236         if fields == (None,):
1237             # Can only pass None to defer(), not only(), as the rest option.
1238             # That won't stop people trying to do this, so let's be explicit.
1239             raise TypeError("Cannot pass None as an argument to only().")
1240         for field in fields:
1241             field = field.split(LOOKUP_SEP, 1)[0]
1242             if field in self.query._filtered_relations:
1243                 raise ValueError('only() is not supported with FilteredRelation.')
1244         clone = self._chain()
1245         clone.query.add_immediate_loading(fields)
1246         return clone
1247 
1248     def using(self, alias):
1249         """Select which database this QuerySet should execute against."""
1250         clone = self._chain()
1251         clone._db = alias
1252         return clone
1253 
1254     ###################################
1255     # PUBLIC INTROSPECTION ATTRIBUTES #
1256     ###################################
1257 
1258     @property
1259     def ordered(self):
1260         """
1261         Return True if the QuerySet is ordered -- i.e. has an order_by()
1262         clause or a default ordering on the model (or is empty).
1263         """
1264         if isinstance(self, EmptyQuerySet):
1265             return True
1266         if self.query.extra_order_by or self.query.order_by:
1267             return True
1268         elif (
1269             self.query.default_ordering and
1270             self.query.get_meta().ordering and
1271             # A default ordering doesn't affect GROUP BY queries.
1272             not self.query.group_by
1273         ):
1274             return True
1275         else:
1276             return False
1277 
1278     @property
1279     def db(self):
1280         """Return the database used if this query is executed now."""
1281         if self._for_write:
1282             return self._db or router.db_for_write(self.model, **self._hints)
1283         return self._db or router.db_for_read(self.model, **self._hints)
1284 
1285     ###################
1286     # PRIVATE METHODS #
1287     ###################
1288 
1289     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1290         """
1291         Insert a new record for the given model. This provides an interface to
1292         the InsertQuery class and is how Model.save() is implemented.
1293         """
1294         self._for_write = True
1295         if using is None:
1296             using = self.db
1297         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1298         query.insert_values(fields, objs, raw=raw)
1299         return query.get_compiler(using=using).execute_sql(returning_fields)
1300     _insert.alters_data = True
1301     _insert.queryset_only = False
1302 
1303     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1304         """
1305         Helper method for bulk_create() to insert objs one batch at a time.
1306         """
1307         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1308             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1309         ops = connections[self.db].ops
1310         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1311         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1312         inserted_rows = []
1313         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1314         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1315             if bulk_return and not ignore_conflicts:
1316                 inserted_rows.extend(self._insert(
1317                     item, fields=fields, using=self.db,
1318                     returning_fields=self.model._meta.db_returning_fields,
1319                     ignore_conflicts=ignore_conflicts,
1320                 ))
1321             else:
1322                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1323         return inserted_rows
1324 
1325     def _chain(self, **kwargs):
1326         """
1327         Return a copy of the current QuerySet that's ready for another
1328         operation.
1329         """
1330         obj = self._clone()
1331         if obj._sticky_filter:
1332             obj.query.filter_is_sticky = True
1333             obj._sticky_filter = False
1334         obj.__dict__.update(kwargs)
1335         return obj
1336 
1337     def _clone(self):
1338         """
1339         Return a copy of the current QuerySet. A lightweight alternative
1340         to deepcopy().
1341         """
1342         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1343         c._sticky_filter = self._sticky_filter
1344         c._for_write = self._for_write
1345         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1346         c._known_related_objects = self._known_related_objects
1347         c._iterable_class = self._iterable_class
1348         c._fields = self._fields
1349         return c
1350 
1351     def _fetch_all(self):
1352         if self._result_cache is None:
1353             self._result_cache = list(self._iterable_class(self))
1354         if self._prefetch_related_lookups and not self._prefetch_done:
1355             self._prefetch_related_objects()
1356 
1357     def _next_is_sticky(self):
1358         """
1359         Indicate that the next filter call and the one following that should
1360         be treated as a single filter. This is only important when it comes to
1361         determining when to reuse tables for many-to-many filters. Required so
1362         that we can filter naturally on the results of related managers.
1363 
1364         This doesn't return a clone of the current QuerySet (it returns
1365         "self"). The method is only used internally and should be immediately
1366         followed by a filter() that does create a clone.
1367         """
1368         self._sticky_filter = True
1369         return self
1370 
1371     def _merge_sanity_check(self, other):
1372         """Check that two QuerySet classes may be merged."""
1373         if self._fields is not None and (
1374                 set(self.query.values_select) != set(other.query.values_select) or
1375                 set(self.query.extra_select) != set(other.query.extra_select) or
1376                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1377             raise TypeError(
1378                 "Merging '%s' classes must involve the same values in each case."
1379                 % self.__class__.__name__
1380             )
1381 
1382     def _merge_known_related_objects(self, other):
1383         """
1384         Keep track of all known related objects from either QuerySet instance.
1385         """
1386         for field, objects in other._known_related_objects.items():
1387             self._known_related_objects.setdefault(field, {}).update(objects)
1388 
1389     def resolve_expression(self, *args, **kwargs):
1390         if self._fields and len(self._fields) > 1:
1391             # values() queryset can only be used as nested queries
1392             # if they are set up to select only a single field.
1393             raise TypeError('Cannot use multi-field values as a filter value.')
1394         query = self.query.resolve_expression(*args, **kwargs)
1395         query._db = self._db
1396         return query
1397     resolve_expression.queryset_only = True
1398 
1399     def _add_hints(self, **hints):
1400         """
1401         Update hinting information for use by routers. Add new key/values or
1402         overwrite existing key/values.
1403         """
1404         self._hints.update(hints)
1405 
1406     def _has_filters(self):
1407         """
1408         Check if this QuerySet has any filtering going on. This isn't
1409         equivalent with checking if all objects are present in results, for
1410         example, qs[1:]._has_filters() -> False.
1411         """
1412         return self.query.has_filters()
1413 
1414     @staticmethod
1415     def _validate_values_are_expressions(values, method_name):
1416         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1417         if invalid_args:
1418             raise TypeError(
1419                 'QuerySet.%s() received non-expression(s): %s.' % (
1420                     method_name,
1421                     ', '.join(invalid_args),
1422                 )
1423             )
1424 
1425     def _not_support_combined_queries(self, operation_name):
1426         if self.query.combinator:
1427             raise NotSupportedError(
1428                 'Calling QuerySet.%s() after %s() is not supported.'
1429                 % (operation_name, self.query.combinator)
1430             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 12:39:27.837 | INFO     | Search completed. Bug locations: [<file>django/db/models/aggregates.py</file>
<class>Sum</class>
<method>None</method>
<code>
142 class Sum(FixDurationInputMixin, Aggregate):
143     function = 'SUM'
144     name = 'Sum'
145     allow_distinct = True

</code><intended_behavior>Modify or extend the Sum class to include an optional parameter that allows specifying a default value for cases where the result of the sum operation is NULL. This could involve either directly integrating COALESCE functionality within the class or providing an interface for the user to specify the desired behavior when encountering NULL values. The preservation of existing functionality is crucial; hence, the default behavior (without specifying a default value) should remain unchanged, returning NULL for empty sets or no values to sum. Adding this feature would enhance Sum by making it more flexible for various use cases, particularly those where a 0 is more desirable than NULL when no records are found.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>annotate</method>
<code>
1114     def annotate(self, *args, **kwargs):
1115         """
1116         Return a query set in which the returned objects have been annotated
1117         with extra data or aggregations.
1118         """
1119         self._not_support_combined_queries('annotate')
1120         return self._annotate(args, kwargs, select=True)

</code><intended_behavior>While the primary change to support a default value for Sum will likely be within the Sum class itself, ensuring that annotate properly supports this extended functionality is important. The intended behavior here would not necessarily be a direct modification but rather ensuring compatibility—that annotate can seamlessly pass through any additional parameters specified for Sum (or similar aggregated functions) to handle NULL values appropriately. This may involve adjustments in how parameters are handled or passed to aggregated functions within annotate to allow for the application of default values via COALESCE or similar mechanisms implicitly or explicitly specified by the developer.</intended_behavior>, <file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
175 class QuerySet:
176     """Represent a lazy database lookup for a set of objects."""
177 
178     def __init__(self, model=None, query=None, using=None, hints=None):
179         self.model = model
180         self._db = using
181         self._hints = hints or {}
182         self._query = query or sql.Query(self.model)
183         self._result_cache = None
184         self._sticky_filter = False
185         self._for_write = False
186         self._prefetch_related_lookups = ()
187         self._prefetch_done = False
188         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
189         self._iterable_class = ModelIterable
190         self._fields = None
191         self._defer_next_filter = False
192         self._deferred_filter = None
193 
194     @property
195     def query(self):
196         if self._deferred_filter:
197             negate, args, kwargs = self._deferred_filter
198             self._filter_or_exclude_inplace(negate, args, kwargs)
199             self._deferred_filter = None
200         return self._query
201 
202     @query.setter
203     def query(self, value):
204         if value.values_select:
205             self._iterable_class = ValuesIterable
206         self._query = value
207 
208     def as_manager(cls):
209         # Address the circular dependency between `Queryset` and `Manager`.
210         from django.db.models.manager import Manager
211         manager = Manager.from_queryset(cls)()
212         manager._built_with_as_manager = True
213         return manager
214     as_manager.queryset_only = True
215     as_manager = classmethod(as_manager)
216 
217     ########################
218     # PYTHON MAGIC METHODS #
219     ########################
220 
221     def __deepcopy__(self, memo):
222         """Don't populate the QuerySet's cache."""
223         obj = self.__class__()
224         for k, v in self.__dict__.items():
225             if k == '_result_cache':
226                 obj.__dict__[k] = None
227             else:
228                 obj.__dict__[k] = copy.deepcopy(v, memo)
229         return obj
230 
231     def __getstate__(self):
232         # Force the cache to be fully populated.
233         self._fetch_all()
234         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: django.__version__}
235 
236     def __setstate__(self, state):
237         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
238         if pickled_version:
239             if pickled_version != django.__version__:
240                 warnings.warn(
241                     "Pickled queryset instance's Django version %s does not "
242                     "match the current version %s."
243                     % (pickled_version, django.__version__),
244                     RuntimeWarning,
245                     stacklevel=2,
246                 )
247         else:
248             warnings.warn(
249                 "Pickled queryset instance's Django version is not specified.",
250                 RuntimeWarning,
251                 stacklevel=2,
252             )
253         self.__dict__.update(state)
254 
255     def __repr__(self):
256         data = list(self[:REPR_OUTPUT_SIZE + 1])
257         if len(data) > REPR_OUTPUT_SIZE:
258             data[-1] = "...(remaining elements truncated)..."
259         return '<%s %r>' % (self.__class__.__name__, data)
260 
261     def __len__(self):
262         self._fetch_all()
263         return len(self._result_cache)
264 
265     def __iter__(self):
266         """
267         The queryset iterator protocol uses three nested iterators in the
268         default case:
269             1. sql.compiler.execute_sql()
270                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
271                  using cursor.fetchmany(). This part is responsible for
272                  doing some column masking, and returning the rows in chunks.
273             2. sql.compiler.results_iter()
274                - Returns one row at time. At this point the rows are still just
275                  tuples. In some cases the return values are converted to
276                  Python values at this location.
277             3. self.iterator()
278                - Responsible for turning the rows into model objects.
279         """
280         self._fetch_all()
281         return iter(self._result_cache)
282 
283     def __bool__(self):
284         self._fetch_all()
285         return bool(self._result_cache)
286 
287     def __getitem__(self, k):
288         """Retrieve an item or slice from the set of results."""
289         if not isinstance(k, (int, slice)):
290             raise TypeError(
291                 'QuerySet indices must be integers or slices, not %s.'
292                 % type(k).__name__
293             )
294         if (
295             (isinstance(k, int) and k < 0) or
296             (isinstance(k, slice) and (
297                 (k.start is not None and k.start < 0) or
298                 (k.stop is not None and k.stop < 0)
299             ))
300         ):
301             raise ValueError('Negative indexing is not supported.')
302 
303         if self._result_cache is not None:
304             return self._result_cache[k]
305 
306         if isinstance(k, slice):
307             qs = self._chain()
308             if k.start is not None:
309                 start = int(k.start)
310             else:
311                 start = None
312             if k.stop is not None:
313                 stop = int(k.stop)
314             else:
315                 stop = None
316             qs.query.set_limits(start, stop)
317             return list(qs)[::k.step] if k.step else qs
318 
319         qs = self._chain()
320         qs.query.set_limits(k, k + 1)
321         qs._fetch_all()
322         return qs._result_cache[0]
323 
324     def __class_getitem__(cls, *args, **kwargs):
325         return cls
326 
327     def __and__(self, other):
328         self._merge_sanity_check(other)
329         if isinstance(other, EmptyQuerySet):
330             return other
331         if isinstance(self, EmptyQuerySet):
332             return self
333         combined = self._chain()
334         combined._merge_known_related_objects(other)
335         combined.query.combine(other.query, sql.AND)
336         return combined
337 
338     def __or__(self, other):
339         self._merge_sanity_check(other)
340         if isinstance(self, EmptyQuerySet):
341             return other
342         if isinstance(other, EmptyQuerySet):
343             return self
344         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
345         combined = query._chain()
346         combined._merge_known_related_objects(other)
347         if not other.query.can_filter():
348             other = other.model._base_manager.filter(pk__in=other.values('pk'))
349         combined.query.combine(other.query, sql.OR)
350         return combined
351 
352     ####################################
353     # METHODS THAT DO DATABASE QUERIES #
354     ####################################
355 
356     def _iterator(self, use_chunked_fetch, chunk_size):
357         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
358 
359     def iterator(self, chunk_size=2000):
360         """
361         An iterator over the results from applying this QuerySet to the
362         database.
363         """
364         if chunk_size <= 0:
365             raise ValueError('Chunk size must be strictly positive.')
366         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
367         return self._iterator(use_chunked_fetch, chunk_size)
368 
369     def aggregate(self, *args, **kwargs):
370         """
371         Return a dictionary containing the calculations (aggregation)
372         over the current queryset.
373 
374         If args is present the expression is passed as a kwarg using
375         the Aggregate object's default alias.
376         """
377         if self.query.distinct_fields:
378             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
379         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
380         for arg in args:
381             # The default_alias property raises TypeError if default_alias
382             # can't be set automatically or AttributeError if it isn't an
383             # attribute.
384             try:
385                 arg.default_alias
386             except (AttributeError, TypeError):
387                 raise TypeError("Complex aggregates require an alias")
388             kwargs[arg.default_alias] = arg
389 
390         query = self.query.chain()
391         for (alias, aggregate_expr) in kwargs.items():
392             query.add_annotation(aggregate_expr, alias, is_summary=True)
393             annotation = query.annotations[alias]
394             if not annotation.contains_aggregate:
395                 raise TypeError("%s is not an aggregate expression" % alias)
396             for expr in annotation.get_source_expressions():
397                 if expr.contains_aggregate and isinstance(expr, Ref) and expr.refs in kwargs:
398                     name = expr.refs
399                     raise exceptions.FieldError(
400                         "Cannot compute %s('%s'): '%s' is an aggregate"
401                         % (annotation.name, name, name)
402                     )
403         return query.get_aggregation(self.db, kwargs)
404 
405     def count(self):
406         """
407         Perform a SELECT COUNT() and return the number of records as an
408         integer.
409 
410         If the QuerySet is already fully cached, return the length of the
411         cached results set to avoid multiple SELECT COUNT(*) calls.
412         """
413         if self._result_cache is not None:
414             return len(self._result_cache)
415 
416         return self.query.get_count(using=self.db)
417 
418     def get(self, *args, **kwargs):
419         """
420         Perform the query and return a single object matching the given
421         keyword arguments.
422         """
423         if self.query.combinator and (args or kwargs):
424             raise NotSupportedError(
425                 'Calling QuerySet.get(...) with filters after %s() is not '
426                 'supported.' % self.query.combinator
427             )
428         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
429         if self.query.can_filter() and not self.query.distinct_fields:
430             clone = clone.order_by()
431         limit = None
432         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
433             limit = MAX_GET_RESULTS
434             clone.query.set_limits(high=limit)
435         num = len(clone)
436         if num == 1:
437             return clone._result_cache[0]
438         if not num:
439             raise self.model.DoesNotExist(
440                 "%s matching query does not exist." %
441                 self.model._meta.object_name
442             )
443         raise self.model.MultipleObjectsReturned(
444             'get() returned more than one %s -- it returned %s!' % (
445                 self.model._meta.object_name,
446                 num if not limit or num < limit else 'more than %s' % (limit - 1),
447             )
448         )
449 
450     def create(self, **kwargs):
451         """
452         Create a new object with the given kwargs, saving it to the database
453         and returning the created object.
454         """
455         obj = self.model(**kwargs)
456         self._for_write = True
457         obj.save(force_insert=True, using=self.db)
458         return obj
459 
460     def _prepare_for_bulk_create(self, objs):
461         for obj in objs:
462             if obj.pk is None:
463                 # Populate new PK values.
464                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
465             obj._prepare_related_fields_for_save(operation_name='bulk_create')
466 
467     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
468         """
469         Insert each of the instances into the database. Do *not* call
470         save() on each of the instances, do not send any pre/post_save
471         signals, and do not set the primary key attribute if it is an
472         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
473         Multi-table models are not supported.
474         """
475         # When you bulk insert you don't get the primary keys back (if it's an
476         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
477         # you can't insert into the child tables which references this. There
478         # are two workarounds:
479         # 1) This could be implemented if you didn't have an autoincrement pk
480         # 2) You could do it by doing O(n) normal inserts into the parent
481         #    tables to get the primary keys back and then doing a single bulk
482         #    insert into the childmost table.
483         # We currently set the primary keys on the objects when using
484         # PostgreSQL via the RETURNING ID clause. It should be possible for
485         # Oracle as well, but the semantics for extracting the primary keys is
486         # trickier so it's not done yet.
487         if batch_size is not None and batch_size <= 0:
488             raise ValueError('Batch size must be a positive integer.')
489         # Check that the parents share the same concrete model with the our
490         # model to detect the inheritance pattern ConcreteGrandParent ->
491         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
492         # would not identify that case as involving multiple tables.
493         for parent in self.model._meta.get_parent_list():
494             if parent._meta.concrete_model is not self.model._meta.concrete_model:
495                 raise ValueError("Can't bulk create a multi-table inherited model")
496         if not objs:
497             return objs
498         self._for_write = True
499         connection = connections[self.db]
500         opts = self.model._meta
501         fields = opts.concrete_fields
502         objs = list(objs)
503         self._prepare_for_bulk_create(objs)
504         with transaction.atomic(using=self.db, savepoint=False):
505             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
506             if objs_with_pk:
507                 returned_columns = self._batched_insert(
508                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
509                 )
510                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
511                     for result, field in zip(results, opts.db_returning_fields):
512                         if field != opts.pk:
513                             setattr(obj_with_pk, field.attname, result)
514                 for obj_with_pk in objs_with_pk:
515                     obj_with_pk._state.adding = False
516                     obj_with_pk._state.db = self.db
517             if objs_without_pk:
518                 fields = [f for f in fields if not isinstance(f, AutoField)]
519                 returned_columns = self._batched_insert(
520                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
521                 )
522                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
523                     assert len(returned_columns) == len(objs_without_pk)
524                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
525                     for result, field in zip(results, opts.db_returning_fields):
526                         setattr(obj_without_pk, field.attname, result)
527                     obj_without_pk._state.adding = False
528                     obj_without_pk._state.db = self.db
529 
530         return objs
531 
532     def bulk_update(self, objs, fields, batch_size=None):
533         """
534         Update the given fields in each of the given objects in the database.
535         """
536         if batch_size is not None and batch_size < 0:
537             raise ValueError('Batch size must be a positive integer.')
538         if not fields:
539             raise ValueError('Field names must be given to bulk_update().')
540         objs = tuple(objs)
541         if any(obj.pk is None for obj in objs):
542             raise ValueError('All bulk_update() objects must have a primary key set.')
543         fields = [self.model._meta.get_field(name) for name in fields]
544         if any(not f.concrete or f.many_to_many for f in fields):
545             raise ValueError('bulk_update() can only be used with concrete fields.')
546         if any(f.primary_key for f in fields):
547             raise ValueError('bulk_update() cannot be used with primary key fields.')
548         if not objs:
549             return 0
550         # PK is used twice in the resulting update query, once in the filter
551         # and once in the WHEN. Each field will also have one CAST.
552         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
553         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
554         requires_casting = connections[self.db].features.requires_casted_case_in_updates
555         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
556         updates = []
557         for batch_objs in batches:
558             update_kwargs = {}
559             for field in fields:
560                 when_statements = []
561                 for obj in batch_objs:
562                     attr = getattr(obj, field.attname)
563                     if not isinstance(attr, Expression):
564                         attr = Value(attr, output_field=field)
565                     when_statements.append(When(pk=obj.pk, then=attr))
566                 case_statement = Case(*when_statements, output_field=field)
567                 if requires_casting:
568                     case_statement = Cast(case_statement, output_field=field)
569                 update_kwargs[field.attname] = case_statement
570             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
571         rows_updated = 0
572         with transaction.atomic(using=self.db, savepoint=False):
573             for pks, update_kwargs in updates:
574                 rows_updated += self.filter(pk__in=pks).update(**update_kwargs)
575         return rows_updated
576     bulk_update.alters_data = True
577 
578     def get_or_create(self, defaults=None, **kwargs):
579         """
580         Look up an object with the given kwargs, creating one if necessary.
581         Return a tuple of (object, created), where created is a boolean
582         specifying whether an object was created.
583         """
584         # The get() needs to be targeted at the write database in order
585         # to avoid potential transaction consistency problems.
586         self._for_write = True
587         try:
588             return self.get(**kwargs), False
589         except self.model.DoesNotExist:
590             params = self._extract_model_params(defaults, **kwargs)
591             # Try to create an object using passed params.
592             try:
593                 with transaction.atomic(using=self.db):
594                     params = dict(resolve_callables(params))
595                     return self.create(**params), True
596             except IntegrityError:
597                 try:
598                     return self.get(**kwargs), False
599                 except self.model.DoesNotExist:
600                     pass
601                 raise
602 
603     def update_or_create(self, defaults=None, **kwargs):
604         """
605         Look up an object with the given kwargs, updating one with defaults
606         if it exists, otherwise create a new one.
607         Return a tuple (object, created), where created is a boolean
608         specifying whether an object was created.
609         """
610         defaults = defaults or {}
611         self._for_write = True
612         with transaction.atomic(using=self.db):
613             # Lock the row so that a concurrent update is blocked until
614             # update_or_create() has performed its save.
615             obj, created = self.select_for_update().get_or_create(defaults, **kwargs)
616             if created:
617                 return obj, created
618             for k, v in resolve_callables(defaults):
619                 setattr(obj, k, v)
620             obj.save(using=self.db)
621         return obj, False
622 
623     def _extract_model_params(self, defaults, **kwargs):
624         """
625         Prepare `params` for creating a model instance based on the given
626         kwargs; for use by get_or_create().
627         """
628         defaults = defaults or {}
629         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
630         params.update(defaults)
631         property_names = self.model._meta._property_names
632         invalid_params = []
633         for param in params:
634             try:
635                 self.model._meta.get_field(param)
636             except exceptions.FieldDoesNotExist:
637                 # It's okay to use a model's property if it has a setter.
638                 if not (param in property_names and getattr(self.model, param).fset):
639                     invalid_params.append(param)
640         if invalid_params:
641             raise exceptions.FieldError(
642                 "Invalid field name(s) for model %s: '%s'." % (
643                     self.model._meta.object_name,
644                     "', '".join(sorted(invalid_params)),
645                 ))
646         return params
647 
648     def _earliest(self, *fields):
649         """
650         Return the earliest object according to fields (if given) or by the
651         model's Meta.get_latest_by.
652         """
653         if fields:
654             order_by = fields
655         else:
656             order_by = getattr(self.model._meta, 'get_latest_by')
657             if order_by and not isinstance(order_by, (tuple, list)):
658                 order_by = (order_by,)
659         if order_by is None:
660             raise ValueError(
661                 "earliest() and latest() require either fields as positional "
662                 "arguments or 'get_latest_by' in the model's Meta."
663             )
664         obj = self._chain()
665         obj.query.set_limits(high=1)
666         obj.query.clear_ordering(force=True)
667         obj.query.add_ordering(*order_by)
668         return obj.get()
669 
670     def earliest(self, *fields):
671         if self.query.is_sliced:
672             raise TypeError('Cannot change a query once a slice has been taken.')
673         return self._earliest(*fields)
674 
675     def latest(self, *fields):
676         if self.query.is_sliced:
677             raise TypeError('Cannot change a query once a slice has been taken.')
678         return self.reverse()._earliest(*fields)
679 
680     def first(self):
681         """Return the first object of a query or None if no match is found."""
682         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
683             return obj
684 
685     def last(self):
686         """Return the last object of a query or None if no match is found."""
687         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
688             return obj
689 
690     def in_bulk(self, id_list=None, *, field_name='pk'):
691         """
692         Return a dictionary mapping each of the given IDs to the object with
693         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
694         """
695         if self.query.is_sliced:
696             raise TypeError("Cannot use 'limit' or 'offset' with in_bulk().")
697         opts = self.model._meta
698         unique_fields = [
699             constraint.fields[0]
700             for constraint in opts.total_unique_constraints
701             if len(constraint.fields) == 1
702         ]
703         if (
704             field_name != 'pk' and
705             not opts.get_field(field_name).unique and
706             field_name not in unique_fields and
707             self.query.distinct_fields != (field_name,)
708         ):
709             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
710         if id_list is not None:
711             if not id_list:
712                 return {}
713             filter_key = '{}__in'.format(field_name)
714             batch_size = connections[self.db].features.max_query_params
715             id_list = tuple(id_list)
716             # If the database has a limit on the number of query parameters
717             # (e.g. SQLite), retrieve objects in batches if necessary.
718             if batch_size and batch_size < len(id_list):
719                 qs = ()
720                 for offset in range(0, len(id_list), batch_size):
721                     batch = id_list[offset:offset + batch_size]
722                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
723             else:
724                 qs = self.filter(**{filter_key: id_list}).order_by()
725         else:
726             qs = self._chain()
727         return {getattr(obj, field_name): obj for obj in qs}
728 
729     def delete(self):
730         """Delete the records in the current QuerySet."""
731         self._not_support_combined_queries('delete')
732         if self.query.is_sliced:
733             raise TypeError("Cannot use 'limit' or 'offset' with delete().")
734         if self.query.distinct or self.query.distinct_fields:
735             raise TypeError('Cannot call delete() after .distinct().')
736         if self._fields is not None:
737             raise TypeError("Cannot call delete() after .values() or .values_list()")
738 
739         del_query = self._chain()
740 
741         # The delete is actually 2 queries - one to find related objects,
742         # and one to delete. Make sure that the discovery of related
743         # objects is performed on the same database as the deletion.
744         del_query._for_write = True
745 
746         # Disable non-supported fields.
747         del_query.query.select_for_update = False
748         del_query.query.select_related = False
749         del_query.query.clear_ordering(force=True)
750 
751         collector = Collector(using=del_query.db)
752         collector.collect(del_query)
753         deleted, _rows_count = collector.delete()
754 
755         # Clear the result cache, in case this QuerySet gets reused.
756         self._result_cache = None
757         return deleted, _rows_count
758 
759     delete.alters_data = True
760     delete.queryset_only = True
761 
762     def _raw_delete(self, using):
763         """
764         Delete objects found from the given queryset in single direct SQL
765         query. No signals are sent and there is no protection for cascades.
766         """
767         query = self.query.clone()
768         query.__class__ = sql.DeleteQuery
769         cursor = query.get_compiler(using).execute_sql(CURSOR)
770         if cursor:
771             with cursor:
772                 return cursor.rowcount
773         return 0
774     _raw_delete.alters_data = True
775 
776     def update(self, **kwargs):
777         """
778         Update all elements in the current QuerySet, setting all the given
779         fields to the appropriate values.
780         """
781         self._not_support_combined_queries('update')
782         if self.query.is_sliced:
783             raise TypeError('Cannot update a query once a slice has been taken.')
784         self._for_write = True
785         query = self.query.chain(sql.UpdateQuery)
786         query.add_update_values(kwargs)
787         # Clear any annotations so that they won't be present in subqueries.
788         query.annotations = {}
789         with transaction.mark_for_rollback_on_error(using=self.db):
790             rows = query.get_compiler(self.db).execute_sql(CURSOR)
791         self._result_cache = None
792         return rows
793     update.alters_data = True
794 
795     def _update(self, values):
796         """
797         A version of update() that accepts field objects instead of field names.
798         Used primarily for model saving and not intended for use by general
799         code (it requires too much poking around at model internals to be
800         useful at that level).
801         """
802         if self.query.is_sliced:
803             raise TypeError('Cannot update a query once a slice has been taken.')
804         query = self.query.chain(sql.UpdateQuery)
805         query.add_update_fields(values)
806         # Clear any annotations so that they won't be present in subqueries.
807         query.annotations = {}
808         self._result_cache = None
809         return query.get_compiler(self.db).execute_sql(CURSOR)
810     _update.alters_data = True
811     _update.queryset_only = False
812 
813     def exists(self):
814         if self._result_cache is None:
815             return self.query.has_results(using=self.db)
816         return bool(self._result_cache)
817 
818     def contains(self, obj):
819         """Return True if the queryset contains an object."""
820         self._not_support_combined_queries('contains')
821         if self._fields is not None:
822             raise TypeError(
823                 'Cannot call QuerySet.contains() after .values() or '
824                 '.values_list().'
825             )
826         try:
827             if obj._meta.concrete_model != self.model._meta.concrete_model:
828                 return False
829         except AttributeError:
830             raise TypeError("'obj' must be a model instance.")
831         if obj.pk is None:
832             raise ValueError(
833                 'QuerySet.contains() cannot be used on unsaved objects.'
834             )
835         if self._result_cache is not None:
836             return obj in self._result_cache
837         return self.filter(pk=obj.pk).exists()
838 
839     def _prefetch_related_objects(self):
840         # This method can only be called once the result cache has been filled.
841         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
842         self._prefetch_done = True
843 
844     def explain(self, *, format=None, **options):
845         return self.query.explain(using=self.db, format=format, **options)
846 
847     ##################################################
848     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
849     ##################################################
850 
851     def raw(self, raw_query, params=(), translations=None, using=None):
852         if using is None:
853             using = self.db
854         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
855         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
856         return qs
857 
858     def _values(self, *fields, **expressions):
859         clone = self._chain()
860         if expressions:
861             clone = clone.annotate(**expressions)
862         clone._fields = fields
863         clone.query.set_values(fields)
864         return clone
865 
866     def values(self, *fields, **expressions):
867         fields += tuple(expressions)
868         clone = self._values(*fields, **expressions)
869         clone._iterable_class = ValuesIterable
870         return clone
871 
872     def values_list(self, *fields, flat=False, named=False):
873         if flat and named:
874             raise TypeError("'flat' and 'named' can't be used together.")
875         if flat and len(fields) > 1:
876             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
877 
878         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
879         _fields = []
880         expressions = {}
881         counter = 1
882         for field in fields:
883             if hasattr(field, 'resolve_expression'):
884                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
885                 while True:
886                     field_id = field_id_prefix + str(counter)
887                     counter += 1
888                     if field_id not in field_names:
889                         break
890                 expressions[field_id] = field
891                 _fields.append(field_id)
892             else:
893                 _fields.append(field)
894 
895         clone = self._values(*_fields, **expressions)
896         clone._iterable_class = (
897             NamedValuesListIterable if named
898             else FlatValuesListIterable if flat
899             else ValuesListIterable
900         )
901         return clone
902 
903     def dates(self, field_name, kind, order='ASC'):
904         """
905         Return a list of date objects representing all available dates for
906         the given field_name, scoped to 'kind'.
907         """
908         if kind not in ('year', 'month', 'week', 'day'):
909             raise ValueError("'kind' must be one of 'year', 'month', 'week', or 'day'.")
910         if order not in ('ASC', 'DESC'):
911             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
912         return self.annotate(
913             datefield=Trunc(field_name, kind, output_field=DateField()),
914             plain_field=F(field_name)
915         ).values_list(
916             'datefield', flat=True
917         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
918 
919     def datetimes(self, field_name, kind, order='ASC', tzinfo=None, is_dst=None):
920         """
921         Return a list of datetime objects representing all available
922         datetimes for the given field_name, scoped to 'kind'.
923         """
924         if kind not in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'):
925             raise ValueError(
926                 "'kind' must be one of 'year', 'month', 'week', 'day', "
927                 "'hour', 'minute', or 'second'."
928             )
929         if order not in ('ASC', 'DESC'):
930             raise ValueError("'order' must be either 'ASC' or 'DESC'.")
931         if settings.USE_TZ:
932             if tzinfo is None:
933                 tzinfo = timezone.get_current_timezone()
934         else:
935             tzinfo = None
936         return self.annotate(
937             datetimefield=Trunc(
938                 field_name,
939                 kind,
940                 output_field=DateTimeField(),
941                 tzinfo=tzinfo,
942                 is_dst=is_dst,
943             ),
944             plain_field=F(field_name)
945         ).values_list(
946             'datetimefield', flat=True
947         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
948 
949     def none(self):
950         """Return an empty QuerySet."""
951         clone = self._chain()
952         clone.query.set_empty()
953         return clone
954 
955     ##################################################################
956     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
957     ##################################################################
958 
959     def all(self):
960         """
961         Return a new QuerySet that is a copy of the current one. This allows a
962         QuerySet to proxy for a model manager in some cases.
963         """
964         return self._chain()
965 
966     def filter(self, *args, **kwargs):
967         """
968         Return a new QuerySet instance with the args ANDed to the existing
969         set.
970         """
971         self._not_support_combined_queries('filter')
972         return self._filter_or_exclude(False, args, kwargs)
973 
974     def exclude(self, *args, **kwargs):
975         """
976         Return a new QuerySet instance with NOT (args) ANDed to the existing
977         set.
978         """
979         self._not_support_combined_queries('exclude')
980         return self._filter_or_exclude(True, args, kwargs)
981 
982     def _filter_or_exclude(self, negate, args, kwargs):
983         if (args or kwargs) and self.query.is_sliced:
984             raise TypeError('Cannot filter a query once a slice has been taken.')
985         clone = self._chain()
986         if self._defer_next_filter:
987             self._defer_next_filter = False
988             clone._deferred_filter = negate, args, kwargs
989         else:
990             clone._filter_or_exclude_inplace(negate, args, kwargs)
991         return clone
992 
993     def _filter_or_exclude_inplace(self, negate, args, kwargs):
994         if negate:
995             self._query.add_q(~Q(*args, **kwargs))
996         else:
997             self._query.add_q(Q(*args, **kwargs))
998 
999     def complex_filter(self, filter_obj):
1000         """
1001         Return a new QuerySet instance with filter_obj added to the filters.
1002 
1003         filter_obj can be a Q object or a dictionary of keyword lookup
1004         arguments.
1005 
1006         This exists to support framework features such as 'limit_choices_to',
1007         and usually it will be more natural to use other methods.
1008         """
1009         if isinstance(filter_obj, Q):
1010             clone = self._chain()
1011             clone.query.add_q(filter_obj)
1012             return clone
1013         else:
1014             return self._filter_or_exclude(False, args=(), kwargs=filter_obj)
1015 
1016     def _combinator_query(self, combinator, *other_qs, all=False):
1017         # Clone the query to inherit the select list and everything
1018         clone = self._chain()
1019         # Clear limits and ordering so they can be reapplied
1020         clone.query.clear_ordering(force=True)
1021         clone.query.clear_limits()
1022         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
1023         clone.query.combinator = combinator
1024         clone.query.combinator_all = all
1025         return clone
1026 
1027     def union(self, *other_qs, all=False):
1028         # If the query is an EmptyQuerySet, combine all nonempty querysets.
1029         if isinstance(self, EmptyQuerySet):
1030             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
1031             if not qs:
1032                 return self
1033             if len(qs) == 1:
1034                 return qs[0]
1035             return qs[0]._combinator_query('union', *qs[1:], all=all)
1036         return self._combinator_query('union', *other_qs, all=all)
1037 
1038     def intersection(self, *other_qs):
1039         # If any query is an EmptyQuerySet, return it.
1040         if isinstance(self, EmptyQuerySet):
1041             return self
1042         for other in other_qs:
1043             if isinstance(other, EmptyQuerySet):
1044                 return other
1045         return self._combinator_query('intersection', *other_qs)
1046 
1047     def difference(self, *other_qs):
1048         # If the query is an EmptyQuerySet, return it.
1049         if isinstance(self, EmptyQuerySet):
1050             return self
1051         return self._combinator_query('difference', *other_qs)
1052 
1053     def select_for_update(self, nowait=False, skip_locked=False, of=(), no_key=False):
1054         """
1055         Return a new QuerySet instance that will select objects with a
1056         FOR UPDATE lock.
1057         """
1058         if nowait and skip_locked:
1059             raise ValueError('The nowait option cannot be used with skip_locked.')
1060         obj = self._chain()
1061         obj._for_write = True
1062         obj.query.select_for_update = True
1063         obj.query.select_for_update_nowait = nowait
1064         obj.query.select_for_update_skip_locked = skip_locked
1065         obj.query.select_for_update_of = of
1066         obj.query.select_for_no_key_update = no_key
1067         return obj
1068 
1069     def select_related(self, *fields):
1070         """
1071         Return a new QuerySet instance that will select related objects.
1072 
1073         If fields are specified, they must be ForeignKey fields and only those
1074         related objects are included in the selection.
1075 
1076         If select_related(None) is called, clear the list.
1077         """
1078         self._not_support_combined_queries('select_related')
1079         if self._fields is not None:
1080             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1081 
1082         obj = self._chain()
1083         if fields == (None,):
1084             obj.query.select_related = False
1085         elif fields:
1086             obj.query.add_select_related(fields)
1087         else:
1088             obj.query.select_related = True
1089         return obj
1090 
1091     def prefetch_related(self, *lookups):
1092         """
1093         Return a new QuerySet instance that will prefetch the specified
1094         Many-To-One and Many-To-Many related objects when the QuerySet is
1095         evaluated.
1096 
1097         When prefetch_related() is called more than once, append to the list of
1098         prefetch lookups. If prefetch_related(None) is called, clear the list.
1099         """
1100         self._not_support_combined_queries('prefetch_related')
1101         clone = self._chain()
1102         if lookups == (None,):
1103             clone._prefetch_related_lookups = ()
1104         else:
1105             for lookup in lookups:
1106                 if isinstance(lookup, Prefetch):
1107                     lookup = lookup.prefetch_to
1108                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1109                 if lookup in self.query._filtered_relations:
1110                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1111             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1112         return clone
1113 
1114     def annotate(self, *args, **kwargs):
1115         """
1116         Return a query set in which the returned objects have been annotated
1117         with extra data or aggregations.
1118         """
1119         self._not_support_combined_queries('annotate')
1120         return self._annotate(args, kwargs, select=True)
1121 
1122     def alias(self, *args, **kwargs):
1123         """
1124         Return a query set with added aliases for extra data or aggregations.
1125         """
1126         self._not_support_combined_queries('alias')
1127         return self._annotate(args, kwargs, select=False)
1128 
1129     def _annotate(self, args, kwargs, select=True):
1130         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1131         annotations = {}
1132         for arg in args:
1133             # The default_alias property may raise a TypeError.
1134             try:
1135                 if arg.default_alias in kwargs:
1136                     raise ValueError("The named annotation '%s' conflicts with the "
1137                                      "default name for another annotation."
1138                                      % arg.default_alias)
1139             except TypeError:
1140                 raise TypeError("Complex annotations require an alias")
1141             annotations[arg.default_alias] = arg
1142         annotations.update(kwargs)
1143 
1144         clone = self._chain()
1145         names = self._fields
1146         if names is None:
1147             names = set(chain.from_iterable(
1148                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1149                 for field in self.model._meta.get_fields()
1150             ))
1151 
1152         for alias, annotation in annotations.items():
1153             if alias in names:
1154                 raise ValueError("The annotation '%s' conflicts with a field on "
1155                                  "the model." % alias)
1156             if isinstance(annotation, FilteredRelation):
1157                 clone.query.add_filtered_relation(annotation, alias)
1158             else:
1159                 clone.query.add_annotation(
1160                     annotation, alias, is_summary=False, select=select,
1161                 )
1162         for alias, annotation in clone.query.annotations.items():
1163             if alias in annotations and annotation.contains_aggregate:
1164                 if clone._fields is None:
1165                     clone.query.group_by = True
1166                 else:
1167                     clone.query.set_group_by()
1168                 break
1169 
1170         return clone
1171 
1172     def order_by(self, *field_names):
1173         """Return a new QuerySet instance with the ordering changed."""
1174         if self.query.is_sliced:
1175             raise TypeError('Cannot reorder a query once a slice has been taken.')
1176         obj = self._chain()
1177         obj.query.clear_ordering(force=True, clear_default=False)
1178         obj.query.add_ordering(*field_names)
1179         return obj
1180 
1181     def distinct(self, *field_names):
1182         """
1183         Return a new QuerySet instance that will select only distinct results.
1184         """
1185         self._not_support_combined_queries('distinct')
1186         if self.query.is_sliced:
1187             raise TypeError('Cannot create distinct fields once a slice has been taken.')
1188         obj = self._chain()
1189         obj.query.add_distinct_fields(*field_names)
1190         return obj
1191 
1192     def extra(self, select=None, where=None, params=None, tables=None,
1193               order_by=None, select_params=None):
1194         """Add extra SQL fragments to the query."""
1195         self._not_support_combined_queries('extra')
1196         if self.query.is_sliced:
1197             raise TypeError('Cannot change a query once a slice has been taken.')
1198         clone = self._chain()
1199         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1200         return clone
1201 
1202     def reverse(self):
1203         """Reverse the ordering of the QuerySet."""
1204         if self.query.is_sliced:
1205             raise TypeError('Cannot reverse a query once a slice has been taken.')
1206         clone = self._chain()
1207         clone.query.standard_ordering = not clone.query.standard_ordering
1208         return clone
1209 
1210     def defer(self, *fields):
1211         """
1212         Defer the loading of data for certain fields until they are accessed.
1213         Add the set of deferred fields to any existing set of deferred fields.
1214         The only exception to this is if None is passed in as the only
1215         parameter, in which case removal all deferrals.
1216         """
1217         self._not_support_combined_queries('defer')
1218         if self._fields is not None:
1219             raise TypeError("Cannot call defer() after .values() or .values_list()")
1220         clone = self._chain()
1221         if fields == (None,):
1222             clone.query.clear_deferred_loading()
1223         else:
1224             clone.query.add_deferred_loading(fields)
1225         return clone
1226 
1227     def only(self, *fields):
1228         """
1229         Essentially, the opposite of defer(). Only the fields passed into this
1230         method and that are not already specified as deferred are loaded
1231         immediately when the queryset is evaluated.
1232         """
1233         self._not_support_combined_queries('only')
1234         if self._fields is not None:
1235             raise TypeError("Cannot call only() after .values() or .values_list()")
1236         if fields == (None,):
1237             # Can only pass None to defer(), not only(), as the rest option.
1238             # That won't stop people trying to do this, so let's be explicit.
1239             raise TypeError("Cannot pass None as an argument to only().")
1240         for field in fields:
1241             field = field.split(LOOKUP_SEP, 1)[0]
1242             if field in self.query._filtered_relations:
1243                 raise ValueError('only() is not supported with FilteredRelation.')
1244         clone = self._chain()
1245         clone.query.add_immediate_loading(fields)
1246         return clone
1247 
1248     def using(self, alias):
1249         """Select which database this QuerySet should execute against."""
1250         clone = self._chain()
1251         clone._db = alias
1252         return clone
1253 
1254     ###################################
1255     # PUBLIC INTROSPECTION ATTRIBUTES #
1256     ###################################
1257 
1258     @property
1259     def ordered(self):
1260         """
1261         Return True if the QuerySet is ordered -- i.e. has an order_by()
1262         clause or a default ordering on the model (or is empty).
1263         """
1264         if isinstance(self, EmptyQuerySet):
1265             return True
1266         if self.query.extra_order_by or self.query.order_by:
1267             return True
1268         elif (
1269             self.query.default_ordering and
1270             self.query.get_meta().ordering and
1271             # A default ordering doesn't affect GROUP BY queries.
1272             not self.query.group_by
1273         ):
1274             return True
1275         else:
1276             return False
1277 
1278     @property
1279     def db(self):
1280         """Return the database used if this query is executed now."""
1281         if self._for_write:
1282             return self._db or router.db_for_write(self.model, **self._hints)
1283         return self._db or router.db_for_read(self.model, **self._hints)
1284 
1285     ###################
1286     # PRIVATE METHODS #
1287     ###################
1288 
1289     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1290         """
1291         Insert a new record for the given model. This provides an interface to
1292         the InsertQuery class and is how Model.save() is implemented.
1293         """
1294         self._for_write = True
1295         if using is None:
1296             using = self.db
1297         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1298         query.insert_values(fields, objs, raw=raw)
1299         return query.get_compiler(using=using).execute_sql(returning_fields)
1300     _insert.alters_data = True
1301     _insert.queryset_only = False
1302 
1303     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1304         """
1305         Helper method for bulk_create() to insert objs one batch at a time.
1306         """
1307         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1308             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1309         ops = connections[self.db].ops
1310         max_batch_size = max(ops.bulk_batch_size(fields, objs), 1)
1311         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
1312         inserted_rows = []
1313         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1314         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1315             if bulk_return and not ignore_conflicts:
1316                 inserted_rows.extend(self._insert(
1317                     item, fields=fields, using=self.db,
1318                     returning_fields=self.model._meta.db_returning_fields,
1319                     ignore_conflicts=ignore_conflicts,
1320                 ))
1321             else:
1322                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1323         return inserted_rows
1324 
1325     def _chain(self, **kwargs):
1326         """
1327         Return a copy of the current QuerySet that's ready for another
1328         operation.
1329         """
1330         obj = self._clone()
1331         if obj._sticky_filter:
1332             obj.query.filter_is_sticky = True
1333             obj._sticky_filter = False
1334         obj.__dict__.update(kwargs)
1335         return obj
1336 
1337     def _clone(self):
1338         """
1339         Return a copy of the current QuerySet. A lightweight alternative
1340         to deepcopy().
1341         """
1342         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1343         c._sticky_filter = self._sticky_filter
1344         c._for_write = self._for_write
1345         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1346         c._known_related_objects = self._known_related_objects
1347         c._iterable_class = self._iterable_class
1348         c._fields = self._fields
1349         return c
1350 
1351     def _fetch_all(self):
1352         if self._result_cache is None:
1353             self._result_cache = list(self._iterable_class(self))
1354         if self._prefetch_related_lookups and not self._prefetch_done:
1355             self._prefetch_related_objects()
1356 
1357     def _next_is_sticky(self):
1358         """
1359         Indicate that the next filter call and the one following that should
1360         be treated as a single filter. This is only important when it comes to
1361         determining when to reuse tables for many-to-many filters. Required so
1362         that we can filter naturally on the results of related managers.
1363 
1364         This doesn't return a clone of the current QuerySet (it returns
1365         "self"). The method is only used internally and should be immediately
1366         followed by a filter() that does create a clone.
1367         """
1368         self._sticky_filter = True
1369         return self
1370 
1371     def _merge_sanity_check(self, other):
1372         """Check that two QuerySet classes may be merged."""
1373         if self._fields is not None and (
1374                 set(self.query.values_select) != set(other.query.values_select) or
1375                 set(self.query.extra_select) != set(other.query.extra_select) or
1376                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1377             raise TypeError(
1378                 "Merging '%s' classes must involve the same values in each case."
1379                 % self.__class__.__name__
1380             )
1381 
1382     def _merge_known_related_objects(self, other):
1383         """
1384         Keep track of all known related objects from either QuerySet instance.
1385         """
1386         for field, objects in other._known_related_objects.items():
1387             self._known_related_objects.setdefault(field, {}).update(objects)
1388 
1389     def resolve_expression(self, *args, **kwargs):
1390         if self._fields and len(self._fields) > 1:
1391             # values() queryset can only be used as nested queries
1392             # if they are set up to select only a single field.
1393             raise TypeError('Cannot use multi-field values as a filter value.')
1394         query = self.query.resolve_expression(*args, **kwargs)
1395         query._db = self._db
1396         return query
1397     resolve_expression.queryset_only = True
1398 
1399     def _add_hints(self, **hints):
1400         """
1401         Update hinting information for use by routers. Add new key/values or
1402         overwrite existing key/values.
1403         """
1404         self._hints.update(hints)
1405 
1406     def _has_filters(self):
1407         """
1408         Check if this QuerySet has any filtering going on. This isn't
1409         equivalent with checking if all objects are present in results, for
1410         example, qs[1:]._has_filters() -> False.
1411         """
1412         return self.query.has_filters()
1413 
1414     @staticmethod
1415     def _validate_values_are_expressions(values, method_name):
1416         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1417         if invalid_args:
1418             raise TypeError(
1419                 'QuerySet.%s() received non-expression(s): %s.' % (
1420                     method_name,
1421                     ', '.join(invalid_args),
1422                 )
1423             )
1424 
1425     def _not_support_combined_queries(self, operation_name):
1426         if self.query.combinator:
1427             raise NotSupportedError(
1428                 'Calling QuerySet.%s() after %s() is not supported.'
1429                 % (operation_name, self.query.combinator)
1430             )

</code><intended_behavior>This class provides additional context to the issue.</intended_behavior>]
2025-10-18 12:39:27.837 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 12:39:27.837 | INFO     | Search-only: skipping patch selection
2025-10-18 12:39:28.068 | INFO     | Task django__django-14026 completed successfully.
2025-10-18 12:39:28.069 | INFO     | No patch generated. You can try running ACR again.
