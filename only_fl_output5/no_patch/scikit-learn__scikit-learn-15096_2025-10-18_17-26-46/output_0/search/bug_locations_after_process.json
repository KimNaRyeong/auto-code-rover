[
    {
        "rel_file_path": "sklearn/model_selection/_search.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/model_selection/_search.py",
        "start": 839,
        "end": 1149,
        "class_name": "GridSearchCV",
        "method_name": null,
        "code": "839 class GridSearchCV(BaseSearchCV):\n840     \"\"\"Exhaustive search over specified parameter values for an estimator.\n841 \n842     Important members are fit, predict.\n843 \n844     GridSearchCV implements a \"fit\" and a \"score\" method.\n845     It also implements \"predict\", \"predict_proba\", \"decision_function\",\n846     \"transform\" and \"inverse_transform\" if they are implemented in the\n847     estimator used.\n848 \n849     The parameters of the estimator used to apply these methods are optimized\n850     by cross-validated grid-search over a parameter grid.\n851 \n852     Read more in the :ref:`User Guide <grid_search>`.\n853 \n854     Parameters\n855     ----------\n856     estimator : estimator object.\n857         This is assumed to implement the scikit-learn estimator interface.\n858         Either estimator needs to provide a ``score`` function,\n859         or ``scoring`` must be passed.\n860 \n861     param_grid : dict or list of dictionaries\n862         Dictionary with parameters names (string) as keys and lists of\n863         parameter settings to try as values, or a list of such\n864         dictionaries, in which case the grids spanned by each dictionary\n865         in the list are explored. This enables searching over any sequence\n866         of parameter settings.\n867 \n868     scoring : string, callable, list/tuple, dict or None, default: None\n869         A single string (see :ref:`scoring_parameter`) or a callable\n870         (see :ref:`scoring`) to evaluate the predictions on the test set.\n871 \n872         For evaluating multiple metrics, either give a list of (unique) strings\n873         or a dict with names as keys and callables as values.\n874 \n875         NOTE that when using custom scorers, each scorer should return a single\n876         value. Metric functions returning a list/array of values can be wrapped\n877         into multiple scorers that return one value each.\n878 \n879         See :ref:`multimetric_grid_search` for an example.\n880 \n881         If None, the estimator's score method is used.\n882 \n883     n_jobs : int or None, optional (default=None)\n884         Number of jobs to run in parallel.\n885         ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n886         ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n887         for more details.\n888 \n889     pre_dispatch : int, or string, optional\n890         Controls the number of jobs that get dispatched during parallel\n891         execution. Reducing this number can be useful to avoid an\n892         explosion of memory consumption when more jobs get dispatched\n893         than CPUs can process. This parameter can be:\n894 \n895             - None, in which case all the jobs are immediately\n896               created and spawned. Use this for lightweight and\n897               fast-running jobs, to avoid delays due to on-demand\n898               spawning of the jobs\n899 \n900             - An int, giving the exact number of total jobs that are\n901               spawned\n902 \n903             - A string, giving an expression as a function of n_jobs,\n904               as in '2*n_jobs'\n905 \n906     iid : boolean, default=False\n907         If True, return the average score across folds, weighted by the number\n908         of samples in each test set. In this case, the data is assumed to be\n909         identically distributed across the folds, and the loss minimized is\n910         the total loss per sample, and not the mean loss across the folds.\n911 \n912         .. deprecated:: 0.22\n913             Parameter ``iid`` is deprecated in 0.22 and will be removed in 0.24\n914 \n915     cv : int, cross-validation generator or an iterable, optional\n916         Determines the cross-validation splitting strategy.\n917         Possible inputs for cv are:\n918 \n919         - None, to use the default 5-fold cross validation,\n920         - integer, to specify the number of folds in a `(Stratified)KFold`,\n921         - :term:`CV splitter`,\n922         - An iterable yielding (train, test) splits as arrays of indices.\n923 \n924         For integer/None inputs, if the estimator is a classifier and ``y`` is\n925         either binary or multiclass, :class:`StratifiedKFold` is used. In all\n926         other cases, :class:`KFold` is used.\n927 \n928         Refer :ref:`User Guide <cross_validation>` for the various\n929         cross-validation strategies that can be used here.\n930 \n931         .. versionchanged:: 0.22\n932             ``cv`` default value if None changed from 3-fold to 5-fold.\n933 \n934     refit : boolean, string, or callable, default=True\n935         Refit an estimator using the best found parameters on the whole\n936         dataset.\n937 \n938         For multiple metric evaluation, this needs to be a string denoting the\n939         scorer that would be used to find the best parameters for refitting\n940         the estimator at the end.\n941 \n942         Where there are considerations other than maximum score in\n943         choosing a best estimator, ``refit`` can be set to a function which\n944         returns the selected ``best_index_`` given ``cv_results_``. In that\n945         case, the ``best_estimator_`` and ``best_parameters_`` will be set\n946         according to the returned ``best_index_`` while the ``best_score_``\n947         attribute will not be availble.\n948 \n949         The refitted estimator is made available at the ``best_estimator_``\n950         attribute and permits using ``predict`` directly on this\n951         ``GridSearchCV`` instance.\n952 \n953         Also for multiple metric evaluation, the attributes ``best_index_``,\n954         ``best_score_`` and ``best_params_`` will only be available if\n955         ``refit`` is set and all of them will be determined w.r.t this specific\n956         scorer.\n957 \n958         See ``scoring`` parameter to know more about multiple metric\n959         evaluation.\n960 \n961         .. versionchanged:: 0.20\n962             Support for callable added.\n963 \n964     verbose : integer\n965         Controls the verbosity: the higher, the more messages.\n966 \n967     error_score : 'raise' or numeric\n968         Value to assign to the score if an error occurs in estimator fitting.\n969         If set to 'raise', the error is raised. If a numeric value is given,\n970         FitFailedWarning is raised. This parameter does not affect the refit\n971         step, which will always raise the error. Default is ``np.nan``.\n972 \n973     return_train_score : boolean, default=False\n974         If ``False``, the ``cv_results_`` attribute will not include training\n975         scores.\n976         Computing training scores is used to get insights on how different\n977         parameter settings impact the overfitting/underfitting trade-off.\n978         However computing the scores on the training set can be computationally\n979         expensive and is not strictly required to select the parameters that\n980         yield the best generalization performance.\n981 \n982 \n983     Examples\n984     --------\n985     >>> from sklearn import svm, datasets\n986     >>> from sklearn.model_selection import GridSearchCV\n987     >>> iris = datasets.load_iris()\n988     >>> parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n989     >>> svc = svm.SVC()\n990     >>> clf = GridSearchCV(svc, parameters)\n991     >>> clf.fit(iris.data, iris.target)\n992     GridSearchCV(estimator=SVC(),\n993                  param_grid={'C': [1, 10], 'kernel': ('linear', 'rbf')})\n994     >>> sorted(clf.cv_results_.keys())\n995     ['mean_fit_time', 'mean_score_time', 'mean_test_score',...\n996      'param_C', 'param_kernel', 'params',...\n997      'rank_test_score', 'split0_test_score',...\n998      'split2_test_score', ...\n999      'std_fit_time', 'std_score_time', 'std_test_score']\n1000 \n1001     Attributes\n1002     ----------\n1003     cv_results_ : dict of numpy (masked) ndarrays\n1004         A dict with keys as column headers and values as columns, that can be\n1005         imported into a pandas ``DataFrame``.\n1006 \n1007         For instance the below given table\n1008 \n1009         +------------+-----------+------------+-----------------+---+---------+\n1010         |param_kernel|param_gamma|param_degree|split0_test_score|...|rank_t...|\n1011         +============+===========+============+=================+===+=========+\n1012         |  'poly'    |     --    |      2     |       0.80      |...|    2    |\n1013         +------------+-----------+------------+-----------------+---+---------+\n1014         |  'poly'    |     --    |      3     |       0.70      |...|    4    |\n1015         +------------+-----------+------------+-----------------+---+---------+\n1016         |  'rbf'     |     0.1   |     --     |       0.80      |...|    3    |\n1017         +------------+-----------+------------+-----------------+---+---------+\n1018         |  'rbf'     |     0.2   |     --     |       0.93      |...|    1    |\n1019         +------------+-----------+------------+-----------------+---+---------+\n1020 \n1021         will be represented by a ``cv_results_`` dict of::\n1022 \n1023             {\n1024             'param_kernel': masked_array(data = ['poly', 'poly', 'rbf', 'rbf'],\n1025                                          mask = [False False False False]...)\n1026             'param_gamma': masked_array(data = [-- -- 0.1 0.2],\n1027                                         mask = [ True  True False False]...),\n1028             'param_degree': masked_array(data = [2.0 3.0 -- --],\n1029                                          mask = [False False  True  True]...),\n1030             'split0_test_score'  : [0.80, 0.70, 0.80, 0.93],\n1031             'split1_test_score'  : [0.82, 0.50, 0.70, 0.78],\n1032             'mean_test_score'    : [0.81, 0.60, 0.75, 0.85],\n1033             'std_test_score'     : [0.01, 0.10, 0.05, 0.08],\n1034             'rank_test_score'    : [2, 4, 3, 1],\n1035             'split0_train_score' : [0.80, 0.92, 0.70, 0.93],\n1036             'split1_train_score' : [0.82, 0.55, 0.70, 0.87],\n1037             'mean_train_score'   : [0.81, 0.74, 0.70, 0.90],\n1038             'std_train_score'    : [0.01, 0.19, 0.00, 0.03],\n1039             'mean_fit_time'      : [0.73, 0.63, 0.43, 0.49],\n1040             'std_fit_time'       : [0.01, 0.02, 0.01, 0.01],\n1041             'mean_score_time'    : [0.01, 0.06, 0.04, 0.04],\n1042             'std_score_time'     : [0.00, 0.00, 0.00, 0.01],\n1043             'params'             : [{'kernel': 'poly', 'degree': 2}, ...],\n1044             }\n1045 \n1046         NOTE\n1047 \n1048         The key ``'params'`` is used to store a list of parameter\n1049         settings dicts for all the parameter candidates.\n1050 \n1051         The ``mean_fit_time``, ``std_fit_time``, ``mean_score_time`` and\n1052         ``std_score_time`` are all in seconds.\n1053 \n1054         For multi-metric evaluation, the scores for all the scorers are\n1055         available in the ``cv_results_`` dict at the keys ending with that\n1056         scorer's name (``'_<scorer_name>'``) instead of ``'_score'`` shown\n1057         above. ('split0_test_precision', 'mean_train_precision' etc.)\n1058 \n1059     best_estimator_ : estimator\n1060         Estimator that was chosen by the search, i.e. estimator\n1061         which gave highest score (or smallest loss if specified)\n1062         on the left out data. Not available if ``refit=False``.\n1063 \n1064         See ``refit`` parameter for more information on allowed values.\n1065 \n1066     best_score_ : float\n1067         Mean cross-validated score of the best_estimator\n1068 \n1069         For multi-metric evaluation, this is present only if ``refit`` is\n1070         specified.\n1071 \n1072         This attribute is not available if ``refit`` is a function.\n1073 \n1074     best_params_ : dict\n1075         Parameter setting that gave the best results on the hold out data.\n1076 \n1077         For multi-metric evaluation, this is present only if ``refit`` is\n1078         specified.\n1079 \n1080     best_index_ : int\n1081         The index (of the ``cv_results_`` arrays) which corresponds to the best\n1082         candidate parameter setting.\n1083 \n1084         The dict at ``search.cv_results_['params'][search.best_index_]`` gives\n1085         the parameter setting for the best model, that gives the highest\n1086         mean score (``search.best_score_``).\n1087 \n1088         For multi-metric evaluation, this is present only if ``refit`` is\n1089         specified.\n1090 \n1091     scorer_ : function or a dict\n1092         Scorer function used on the held out data to choose the best\n1093         parameters for the model.\n1094 \n1095         For multi-metric evaluation, this attribute holds the validated\n1096         ``scoring`` dict which maps the scorer key to the scorer callable.\n1097 \n1098     n_splits_ : int\n1099         The number of cross-validation splits (folds/iterations).\n1100 \n1101     refit_time_ : float\n1102         Seconds used for refitting the best model on the whole dataset.\n1103 \n1104         This is present only if ``refit`` is not False.\n1105 \n1106     Notes\n1107     -----\n1108     The parameters selected are those that maximize the score of the left out\n1109     data, unless an explicit score is passed in which case it is used instead.\n1110 \n1111     If `n_jobs` was set to a value higher than one, the data is copied for each\n1112     point in the grid (and not `n_jobs` times). This is done for efficiency\n1113     reasons if individual jobs take very little time, but may raise errors if\n1114     the dataset is large and not enough memory is available.  A workaround in\n1115     this case is to set `pre_dispatch`. Then, the memory is copied only\n1116     `pre_dispatch` many times. A reasonable value for `pre_dispatch` is `2 *\n1117     n_jobs`.\n1118 \n1119     See Also\n1120     ---------\n1121     :class:`ParameterGrid`:\n1122         generates all the combinations of a hyperparameter grid.\n1123 \n1124     :func:`sklearn.model_selection.train_test_split`:\n1125         utility function to split the data into a development set usable\n1126         for fitting a GridSearchCV instance and an evaluation set for\n1127         its final evaluation.\n1128 \n1129     :func:`sklearn.metrics.make_scorer`:\n1130         Make a scorer from a performance metric or loss function.\n1131 \n1132     \"\"\"\n1133     _required_parameters = [\"estimator\", \"param_grid\"]\n1134 \n1135     def __init__(self, estimator, param_grid, scoring=None,\n1136                  n_jobs=None, iid='deprecated', refit=True, cv=None,\n1137                  verbose=0, pre_dispatch='2*n_jobs',\n1138                  error_score=np.nan, return_train_score=False):\n1139         super().__init__(\n1140             estimator=estimator, scoring=scoring,\n1141             n_jobs=n_jobs, iid=iid, refit=refit, cv=cv, verbose=verbose,\n1142             pre_dispatch=pre_dispatch, error_score=error_score,\n1143             return_train_score=return_train_score)\n1144         self.param_grid = param_grid\n1145         _check_param_grid(param_grid)\n1146 \n1147     def _run_search(self, evaluate_candidates):\n1148         \"\"\"Search all candidates in param_grid\"\"\"\n1149         evaluate_candidates(ParameterGrid(self.param_grid))\n",
        "intended_behavior": "The intended behavior at this location should ensure that before any estimator in the parameter grid is fitted, a clone of it is created and used for the fitting. This would prevent the save of fitted estimators into `cv_results_['params']` and ensure that `cv_results_` only contains references to the parameters as they were passed to the grid search, without any fitted instances. This can be achieved by ensuring that any estimator parameter passed through the grid is cloned using the `clone` utility method before fitting. This cloning process should be explicitly documented in the method implementation to clarify the behavior and intended usage, preserving the functionality of parameter grid handling while avoiding the unintended retention of fitted model instances in `cv_results_`."
    }
]