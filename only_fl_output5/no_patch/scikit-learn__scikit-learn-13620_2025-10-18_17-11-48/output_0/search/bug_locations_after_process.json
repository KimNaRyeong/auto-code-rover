[
    {
        "rel_file_path": "sklearn/ensemble/gradient_boosting.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.21/sklearn/ensemble/gradient_boosting.py",
        "start": 2233,
        "end": 2565,
        "class_name": "GradientBoostingRegressor",
        "method_name": null,
        "code": "2233 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):\n2234     \"\"\"Gradient Boosting for regression.\n2235 \n2236     GB builds an additive model in a forward stage-wise fashion;\n2237     it allows for the optimization of arbitrary differentiable loss functions.\n2238     In each stage a regression tree is fit on the negative gradient of the\n2239     given loss function.\n2240 \n2241     Read more in the :ref:`User Guide <gradient_boosting>`.\n2242 \n2243     Parameters\n2244     ----------\n2245     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')\n2246         loss function to be optimized. 'ls' refers to least squares\n2247         regression. 'lad' (least absolute deviation) is a highly robust\n2248         loss function solely based on order information of the input\n2249         variables. 'huber' is a combination of the two. 'quantile'\n2250         allows quantile regression (use `alpha` to specify the quantile).\n2251 \n2252     learning_rate : float, optional (default=0.1)\n2253         learning rate shrinks the contribution of each tree by `learning_rate`.\n2254         There is a trade-off between learning_rate and n_estimators.\n2255 \n2256     n_estimators : int (default=100)\n2257         The number of boosting stages to perform. Gradient boosting\n2258         is fairly robust to over-fitting so a large number usually\n2259         results in better performance.\n2260 \n2261     subsample : float, optional (default=1.0)\n2262         The fraction of samples to be used for fitting the individual base\n2263         learners. If smaller than 1.0 this results in Stochastic Gradient\n2264         Boosting. `subsample` interacts with the parameter `n_estimators`.\n2265         Choosing `subsample < 1.0` leads to a reduction of variance\n2266         and an increase in bias.\n2267 \n2268     criterion : string, optional (default=\"friedman_mse\")\n2269         The function to measure the quality of a split. Supported criteria\n2270         are \"friedman_mse\" for the mean squared error with improvement\n2271         score by Friedman, \"mse\" for mean squared error, and \"mae\" for\n2272         the mean absolute error. The default value of \"friedman_mse\" is\n2273         generally the best as it can provide a better approximation in\n2274         some cases.\n2275 \n2276         .. versionadded:: 0.18\n2277 \n2278     min_samples_split : int, float, optional (default=2)\n2279         The minimum number of samples required to split an internal node:\n2280 \n2281         - If int, then consider `min_samples_split` as the minimum number.\n2282         - If float, then `min_samples_split` is a fraction and\n2283           `ceil(min_samples_split * n_samples)` are the minimum\n2284           number of samples for each split.\n2285 \n2286         .. versionchanged:: 0.18\n2287            Added float values for fractions.\n2288 \n2289     min_samples_leaf : int, float, optional (default=1)\n2290         The minimum number of samples required to be at a leaf node.\n2291         A split point at any depth will only be considered if it leaves at\n2292         least ``min_samples_leaf`` training samples in each of the left and\n2293         right branches.  This may have the effect of smoothing the model,\n2294         especially in regression.\n2295 \n2296         - If int, then consider `min_samples_leaf` as the minimum number.\n2297         - If float, then `min_samples_leaf` is a fraction and\n2298           `ceil(min_samples_leaf * n_samples)` are the minimum\n2299           number of samples for each node.\n2300 \n2301         .. versionchanged:: 0.18\n2302            Added float values for fractions.\n2303 \n2304     min_weight_fraction_leaf : float, optional (default=0.)\n2305         The minimum weighted fraction of the sum total of weights (of all\n2306         the input samples) required to be at a leaf node. Samples have\n2307         equal weight when sample_weight is not provided.\n2308 \n2309     max_depth : integer, optional (default=3)\n2310         maximum depth of the individual regression estimators. The maximum\n2311         depth limits the number of nodes in the tree. Tune this parameter\n2312         for best performance; the best value depends on the interaction\n2313         of the input variables.\n2314 \n2315     min_impurity_decrease : float, optional (default=0.)\n2316         A node will be split if this split induces a decrease of the impurity\n2317         greater than or equal to this value.\n2318 \n2319         The weighted impurity decrease equation is the following::\n2320 \n2321             N_t / N * (impurity - N_t_R / N_t * right_impurity\n2322                                 - N_t_L / N_t * left_impurity)\n2323 \n2324         where ``N`` is the total number of samples, ``N_t`` is the number of\n2325         samples at the current node, ``N_t_L`` is the number of samples in the\n2326         left child, and ``N_t_R`` is the number of samples in the right child.\n2327 \n2328         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n2329         if ``sample_weight`` is passed.\n2330 \n2331         .. versionadded:: 0.19\n2332 \n2333     min_impurity_split : float, (default=1e-7)\n2334         Threshold for early stopping in tree growth. A node will split\n2335         if its impurity is above the threshold, otherwise it is a leaf.\n2336 \n2337         .. deprecated:: 0.19\n2338            ``min_impurity_split`` has been deprecated in favor of\n2339            ``min_impurity_decrease`` in 0.19. The default value of\n2340            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it\n2341            will be removed in 0.25. Use ``min_impurity_decrease`` instead.\n2342 \n2343     init : estimator or 'zero', optional (default=None)\n2344         An estimator object that is used to compute the initial predictions.\n2345         ``init`` has to provide `fit` and `predict`. If 'zero', the initial\n2346         raw predictions are set to zero. By default a ``DummyEstimator`` is\n2347         used, predicting either the average target value (for loss='ls'), or\n2348         a quantile for the other losses.\n2349 \n2350     random_state : int, RandomState instance or None, optional (default=None)\n2351         If int, random_state is the seed used by the random number generator;\n2352         If RandomState instance, random_state is the random number generator;\n2353         If None, the random number generator is the RandomState instance used\n2354         by `np.random`.\n2355 \n2356     max_features : int, float, string or None, optional (default=None)\n2357         The number of features to consider when looking for the best split:\n2358 \n2359         - If int, then consider `max_features` features at each split.\n2360         - If float, then `max_features` is a fraction and\n2361           `int(max_features * n_features)` features are considered at each\n2362           split.\n2363         - If \"auto\", then `max_features=n_features`.\n2364         - If \"sqrt\", then `max_features=sqrt(n_features)`.\n2365         - If \"log2\", then `max_features=log2(n_features)`.\n2366         - If None, then `max_features=n_features`.\n2367 \n2368         Choosing `max_features < n_features` leads to a reduction of variance\n2369         and an increase in bias.\n2370 \n2371         Note: the search for a split does not stop until at least one\n2372         valid partition of the node samples is found, even if it requires to\n2373         effectively inspect more than ``max_features`` features.\n2374 \n2375     alpha : float (default=0.9)\n2376         The alpha-quantile of the huber loss function and the quantile\n2377         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.\n2378 \n2379     verbose : int, default: 0\n2380         Enable verbose output. If 1 then it prints progress and performance\n2381         once in a while (the more trees the lower the frequency). If greater\n2382         than 1 then it prints progress and performance for every tree.\n2383 \n2384     max_leaf_nodes : int or None, optional (default=None)\n2385         Grow trees with ``max_leaf_nodes`` in best-first fashion.\n2386         Best nodes are defined as relative reduction in impurity.\n2387         If None then unlimited number of leaf nodes.\n2388 \n2389     warm_start : bool, default: False\n2390         When set to ``True``, reuse the solution of the previous call to fit\n2391         and add more estimators to the ensemble, otherwise, just erase the\n2392         previous solution. See :term:`the Glossary <warm_start>`.\n2393 \n2394     presort : bool or 'auto', optional (default='auto')\n2395         Whether to presort the data to speed up the finding of best splits in\n2396         fitting. Auto mode by default will use presorting on dense data and\n2397         default to normal sorting on sparse data. Setting presort to true on\n2398         sparse data will raise an error.\n2399 \n2400         .. versionadded:: 0.17\n2401            optional parameter *presort*.\n2402 \n2403     validation_fraction : float, optional, default 0.1\n2404         The proportion of training data to set aside as validation set for\n2405         early stopping. Must be between 0 and 1.\n2406         Only used if ``n_iter_no_change`` is set to an integer.\n2407 \n2408         .. versionadded:: 0.20\n2409 \n2410     n_iter_no_change : int, default None\n2411         ``n_iter_no_change`` is used to decide if early stopping will be used\n2412         to terminate training when validation score is not improving. By\n2413         default it is set to None to disable early stopping. If set to a\n2414         number, it will set aside ``validation_fraction`` size of the training\n2415         data as validation and terminate training when validation score is not\n2416         improving in all of the previous ``n_iter_no_change`` numbers of\n2417         iterations.\n2418 \n2419         .. versionadded:: 0.20\n2420 \n2421     tol : float, optional, default 1e-4\n2422         Tolerance for the early stopping. When the loss is not improving\n2423         by at least tol for ``n_iter_no_change`` iterations (if set to a\n2424         number), the training stops.\n2425 \n2426         .. versionadded:: 0.20\n2427 \n2428 \n2429     Attributes\n2430     ----------\n2431     feature_importances_ : array, shape (n_features,)\n2432         The feature importances (the higher, the more important the feature).\n2433 \n2434     oob_improvement_ : array, shape (n_estimators,)\n2435         The improvement in loss (= deviance) on the out-of-bag samples\n2436         relative to the previous iteration.\n2437         ``oob_improvement_[0]`` is the improvement in\n2438         loss of the first stage over the ``init`` estimator.\n2439 \n2440     train_score_ : array, shape (n_estimators,)\n2441         The i-th score ``train_score_[i]`` is the deviance (= loss) of the\n2442         model at iteration ``i`` on the in-bag sample.\n2443         If ``subsample == 1`` this is the deviance on the training data.\n2444 \n2445     loss_ : LossFunction\n2446         The concrete ``LossFunction`` object.\n2447 \n2448     init_ : estimator\n2449         The estimator that provides the initial predictions.\n2450         Set via the ``init`` argument or ``loss.init_estimator``.\n2451 \n2452     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)\n2453         The collection of fitted sub-estimators.\n2454 \n2455     Notes\n2456     -----\n2457     The features are always randomly permuted at each split. Therefore,\n2458     the best found split may vary, even with the same training data and\n2459     ``max_features=n_features``, if the improvement of the criterion is\n2460     identical for several splits enumerated during the search of the best\n2461     split. To obtain a deterministic behaviour during fitting,\n2462     ``random_state`` has to be fixed.\n2463 \n2464     See also\n2465     --------\n2466     DecisionTreeRegressor, RandomForestRegressor\n2467 \n2468     References\n2469     ----------\n2470     J. Friedman, Greedy Function Approximation: A Gradient Boosting\n2471     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.\n2472 \n2473     J. Friedman, Stochastic Gradient Boosting, 1999\n2474 \n2475     T. Hastie, R. Tibshirani and J. Friedman.\n2476     Elements of Statistical Learning Ed. 2, Springer, 2009.\n2477     \"\"\"\n2478 \n2479     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')\n2480 \n2481     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,\n2482                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,\n2483                  min_samples_leaf=1, min_weight_fraction_leaf=0.,\n2484                  max_depth=3, min_impurity_decrease=0.,\n2485                  min_impurity_split=None, init=None, random_state=None,\n2486                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,\n2487                  warm_start=False, presort='auto', validation_fraction=0.1,\n2488                  n_iter_no_change=None, tol=1e-4):\n2489 \n2490         super().__init__(\n2491             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,\n2492             criterion=criterion, min_samples_split=min_samples_split,\n2493             min_samples_leaf=min_samples_leaf,\n2494             min_weight_fraction_leaf=min_weight_fraction_leaf,\n2495             max_depth=max_depth, init=init, subsample=subsample,\n2496             max_features=max_features,\n2497             min_impurity_decrease=min_impurity_decrease,\n2498             min_impurity_split=min_impurity_split,\n2499             random_state=random_state, alpha=alpha, verbose=verbose,\n2500             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,\n2501             presort=presort, validation_fraction=validation_fraction,\n2502             n_iter_no_change=n_iter_no_change, tol=tol)\n2503 \n2504     def predict(self, X):\n2505         \"\"\"Predict regression target for X.\n2506 \n2507         Parameters\n2508         ----------\n2509         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2510             The input samples. Internally, it will be converted to\n2511             ``dtype=np.float32`` and if a sparse matrix is provided\n2512             to a sparse ``csr_matrix``.\n2513 \n2514         Returns\n2515         -------\n2516         y : array, shape (n_samples,)\n2517             The predicted values.\n2518         \"\"\"\n2519         X = check_array(X, dtype=DTYPE, order=\"C\",  accept_sparse='csr')\n2520         # In regression we can directly return the raw value from the trees.\n2521         return self._raw_predict(X).ravel()\n2522 \n2523     def staged_predict(self, X):\n2524         \"\"\"Predict regression target at each stage for X.\n2525 \n2526         This method allows monitoring (i.e. determine error on testing set)\n2527         after each stage.\n2528 \n2529         Parameters\n2530         ----------\n2531         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2532             The input samples. Internally, it will be converted to\n2533             ``dtype=np.float32`` and if a sparse matrix is provided\n2534             to a sparse ``csr_matrix``.\n2535 \n2536         Returns\n2537         -------\n2538         y : generator of array of shape (n_samples,)\n2539             The predicted value of the input samples.\n2540         \"\"\"\n2541         for raw_predictions in self._staged_raw_predict(X):\n2542             yield raw_predictions.ravel()\n2543 \n2544     def apply(self, X):\n2545         \"\"\"Apply trees in the ensemble to X, return leaf indices.\n2546 \n2547         .. versionadded:: 0.17\n2548 \n2549         Parameters\n2550         ----------\n2551         X : {array-like, sparse matrix}, shape (n_samples, n_features)\n2552             The input samples. Internally, its dtype will be converted to\n2553             ``dtype=np.float32``. If a sparse matrix is provided, it will\n2554             be converted to a sparse ``csr_matrix``.\n2555 \n2556         Returns\n2557         -------\n2558         X_leaves : array-like, shape (n_samples, n_estimators)\n2559             For each datapoint x in X and for each tree in the ensemble,\n2560             return the index of the leaf x ends up in each estimator.\n2561         \"\"\"\n2562 \n2563         leaves = super().apply(X)\n2564         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])\n2565         return leaves\n",
        "intended_behavior": "The intended behavior should ensure that the feature importances, when calculated, should always sum up to 1 across all features. This might involve reviewing and potentially revising the logic within the method that aggregates feature importance scores from each estimator in the ensemble. The fix should ensure that no floating-point arithmetic issue or logic flaw causes the importance sum to drift from 1, especially as the number of estimators increases. An audit of the calculation process is necessary, ensuring that normalization occurs correctly and consistently, considering large numbers of estimators and edge cases possible with varying tree depths and dataset sizes."
    }
]