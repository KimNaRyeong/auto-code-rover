2025-10-18 16:58:07.224 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 16:58:10.672 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=190, output_tokens=12, cost=0.002260
2025-10-18 16:58:10.673 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 16:58:10.677 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:58:17.736 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=497, output_tokens=251, cost=0.012500
2025-10-18 16:58:17.741 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:58:21.306 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=605, output_tokens=43, cost=0.007340
2025-10-18 16:58:21.307 | DEBUG    | Extracted a valid json.
2025-10-18 16:58:21.307 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_class(\"GradientBoosting\")",
        "search_method(\"predict\")",
        "search_code(\"sample weights\")"
    ],
    "bug_locations": []
}
2025-10-18 16:58:21.381 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 16:58:21.382 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 16:58:35.933 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2023, output_tokens=724, cost=0.041950
2025-10-18 16:58:35.940 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:58:41.572 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2963, output_tokens=255, cost=0.037280
2025-10-18 16:58:41.579 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:58:48.024 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=609, output_tokens=246, cost=0.013470
2025-10-18 16:58:48.025 | DEBUG    | Extracted a valid json.
2025-10-18 16:58:48.025 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "",
            "method": "predict",
            "intended_behavior": "The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats."
        },
        {
            "file": "sklearn/ensemble/gradient_boosting.py",
            "class": "",
            "method": "",
            "intended_behavior": "If the issue involves the handling of sample weights with initial estimators that do not support them, the implementation needs to ensure that these weights are only applied where applicable or provide a fallback mechanism. This could mean introducing a check or compatibility layer within the gradient boosting initialization process to verify if the `init` estimator supports sample weights and handle cases where it doesn't, possibly by ignoring the weights or applying a default weighting mechanism that ensures the initial model fitting process proceeds without errors."
        }
    ]
}
2025-10-18 16:58:48.064 | DEBUG    | Bug location extracted successfully: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>QuantileEstimator</class>
<method>predict</method>
<code>
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>MeanEstimator</class>
<method>predict</method>
<code>
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>LogOddsEstimator</class>
<method>predict</method>
<code>
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>PriorProbabilityEstimator</class>
<method>predict</method>
<code>
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>ZeroEstimator</class>
<method>predict</method>
<code>
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>GradientBoostingClassifier</class>
<method>predict</method>
<code>
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>GradientBoostingRegressor</class>
<method>predict</method>
<code>
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Gradient Boosted Regression Trees
2 
3 This module contains methods for fitting gradient boosted regression trees for
4 both classification and regression.
5 
6 The module structure is the following:
7 
8 - The ``BaseGradientBoosting`` base class implements a common ``fit`` method
9   for all the estimators in the module. Regression and classification
10   only differ in the concrete ``LossFunction`` used.
11 
12 - ``GradientBoostingClassifier`` implements gradient boosting for
13   classification problems.
14 
15 - ``GradientBoostingRegressor`` implements gradient boosting for
16   regression problems.
17 """
18 
19 # Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,
20 #          Arnaud Joly, Jacob Schreiber
21 # License: BSD 3 clause
22 
23 from abc import ABCMeta
24 from abc import abstractmethod
25 
26 from .base import BaseEnsemble
27 from ..base import ClassifierMixin
28 from ..base import RegressorMixin
29 
30 from ._gradient_boosting import predict_stages
31 from ._gradient_boosting import predict_stage
32 from ._gradient_boosting import _random_sample_mask
33 
34 import numbers
35 import numpy as np
36 
37 from scipy.sparse import csc_matrix
38 from scipy.sparse import csr_matrix
39 from scipy.sparse import issparse
40 from scipy.special import expit
41 
42 from time import time
43 from ..model_selection import train_test_split
44 from ..tree.tree import DecisionTreeRegressor
45 from ..tree._tree import DTYPE
46 from ..tree._tree import TREE_LEAF
47 
48 from ..utils import check_random_state
49 from ..utils import check_array
50 from ..utils import check_X_y
51 from ..utils import column_or_1d
52 from ..utils import check_consistent_length
53 from ..utils import deprecated
54 from ..utils.fixes import logsumexp
55 from ..utils.stats import _weighted_percentile
56 from ..utils.validation import check_is_fitted
57 from ..utils.multiclass import check_classification_targets
58 from ..exceptions import NotFittedError
59 
60 
61 class QuantileEstimator:
62     """An estimator predicting the alpha-quantile of the training targets.
63 
64     Parameters
65     ----------
66     alpha : float
67         The quantile
68     """
69     def __init__(self, alpha=0.9):
70         if not 0 < alpha < 1.0:
71             raise ValueError("`alpha` must be in (0, 1.0) but was %r" % alpha)
72         self.alpha = alpha
73 
74     def fit(self, X, y, sample_weight=None):
75         """Fit the estimator.
76 
77         Parameters
78         ----------
79         X : {array-like, sparse matrix}, shape (n_samples, n_features)
80             Training data
81 
82         y : array, shape (n_samples, n_targets)
83             Target values. Will be cast to X's dtype if necessary
84 
85         sample_weight : numpy array of shape (n_samples,)
86             Individual weights for each sample
87         """
88         if sample_weight is None:
89             self.quantile = np.percentile(y, self.alpha * 100.0)
90         else:
91             self.quantile = _weighted_percentile(y, sample_weight,
92                                                  self.alpha * 100.0)
93 
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y
112 
113 
114 class MeanEstimator:
115     """An estimator predicting the mean of the training targets."""
116     def fit(self, X, y, sample_weight=None):
117         """Fit the estimator.
118 
119         Parameters
120         ----------
121         X : {array-like, sparse matrix}, shape (n_samples, n_features)
122             Training data
123 
124         y : array, shape (n_samples, n_targets)
125             Target values. Will be cast to X's dtype if necessary
126 
127         sample_weight : numpy array of shape (n_samples,)
128             Individual weights for each sample
129         """
130         if sample_weight is None:
131             self.mean = np.mean(y)
132         else:
133             self.mean = np.average(y, weights=sample_weight)
134 
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y
153 
154 
155 class LogOddsEstimator:
156     """An estimator predicting the log odds ratio."""
157     scale = 1.0
158 
159     def fit(self, X, y, sample_weight=None):
160         """Fit the estimator.
161 
162         Parameters
163         ----------
164         X : {array-like, sparse matrix}, shape (n_samples, n_features)
165             Training data
166 
167         y : array, shape (n_samples, n_targets)
168             Target values. Will be cast to X's dtype if necessary
169 
170         sample_weight : numpy array of shape (n_samples,)
171             Individual weights for each sample
172         """
173         # pre-cond: pos, neg are encoded as 1, 0
174         if sample_weight is None:
175             pos = np.sum(y)
176             neg = y.shape[0] - pos
177         else:
178             pos = np.sum(sample_weight * y)
179             neg = np.sum(sample_weight * (1 - y))
180 
181         if neg == 0 or pos == 0:
182             raise ValueError('y contains non binary labels.')
183         self.prior = self.scale * np.log(pos / neg)
184 
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y
203 
204 
205 class ScaledLogOddsEstimator(LogOddsEstimator):
206     """Log odds ratio scaled by 0.5 -- for exponential loss. """
207     scale = 0.5
208 
209 
210 class PriorProbabilityEstimator:
211     """An estimator predicting the probability of each
212     class in the training data.
213     """
214     def fit(self, X, y, sample_weight=None):
215         """Fit the estimator.
216 
217         Parameters
218         ----------
219         X : {array-like, sparse matrix}, shape (n_samples, n_features)
220             Training data
221 
222         y : array, shape (n_samples, n_targets)
223             Target values. Will be cast to X's dtype if necessary
224 
225         sample_weight : array, shape (n_samples,)
226             Individual weights for each sample
227         """
228         if sample_weight is None:
229             sample_weight = np.ones_like(y, dtype=np.float64)
230         class_counts = np.bincount(y, weights=sample_weight)
231         self.priors = class_counts / class_counts.sum()
232 
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y
251 
252 
253 class ZeroEstimator:
254     """An estimator that simply predicts zero. """
255 
256     def fit(self, X, y, sample_weight=None):
257         """Fit the estimator.
258 
259         Parameters
260         ----------
261         X : {array-like, sparse matrix}, shape (n_samples, n_features)
262             Training data
263 
264         y : numpy, shape (n_samples, n_targets)
265             Target values. Will be cast to X's dtype if necessary
266 
267         sample_weight : array, shape (n_samples,)
268             Individual weights for each sample
269         """
270         if np.issubdtype(y.dtype, np.signedinteger):
271             # classification
272             self.n_classes = np.unique(y).shape[0]
273             if self.n_classes == 2:
274                 self.n_classes = 1
275         else:
276             # regression
277             self.n_classes = 1
278 
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y
297 
298 
299 class LossFunction(metaclass=ABCMeta):
300     """Abstract base class for various loss functions.
301 
302     Parameters
303     ----------
304     n_classes : int
305         Number of classes
306 
307     Attributes
308     ----------
309     K : int
310         The number of regression trees to be induced;
311         1 for regression and binary classification;
312         ``n_classes`` for multi-class classification.
313     """
314 
315     is_multi_class = False
316 
317     def __init__(self, n_classes):
318         self.K = n_classes
319 
320     def init_estimator(self):
321         """Default ``init`` estimator for loss function. """
322         raise NotImplementedError()
323 
324     @abstractmethod
325     def __call__(self, y, pred, sample_weight=None):
326         """Compute the loss.
327 
328         Parameters
329         ----------
330         y : array, shape (n_samples,)
331             True labels
332 
333         pred : array, shape (n_samples,)
334             Predicted labels
335 
336         sample_weight : array-like, shape (n_samples,), optional
337             Sample weights.
338         """
339 
340     @abstractmethod
341     def negative_gradient(self, y, y_pred, **kargs):
342         """Compute the negative gradient.
343 
344         Parameters
345         ----------
346         y : array, shape (n_samples,)
347             The target labels.
348 
349         y_pred : array, shape (n_samples,)
350             The predictions.
351         """
352 
353     def update_terminal_regions(self, tree, X, y, residual, y_pred,
354                                 sample_weight, sample_mask,
355                                 learning_rate=0.1, k=0):
356         """Update the terminal regions (=leaves) of the given tree and
357         updates the current predictions of the model. Traverses tree
358         and invokes template method `_update_terminal_region`.
359 
360         Parameters
361         ----------
362         tree : tree.Tree
363             The tree object.
364         X : array, shape (n, m)
365             The data array.
366         y : array, shape (n,)
367             The target labels.
368         residual : array, shape (n,)
369             The residuals (usually the negative gradient).
370         y_pred : array, shape (n,)
371             The predictions.
372         sample_weight : array, shape (n,)
373             The weight of each sample.
374         sample_mask : array, shape (n,)
375             The sample mask to be used.
376         learning_rate : float, default=0.1
377             learning rate shrinks the contribution of each tree by
378              ``learning_rate``.
379         k : int, default 0
380             The index of the estimator being updated.
381 
382         """
383         # compute leaf for each sample in ``X``.
384         terminal_regions = tree.apply(X)
385 
386         # mask all which are not in sample mask.
387         masked_terminal_regions = terminal_regions.copy()
388         masked_terminal_regions[~sample_mask] = -1
389 
390         # update each leaf (= perform line search)
391         for leaf in np.where(tree.children_left == TREE_LEAF)[0]:
392             self._update_terminal_region(tree, masked_terminal_regions,
393                                          leaf, X, y, residual,
394                                          y_pred[:, k], sample_weight)
395 
396         # update predictions (both in-bag and out-of-bag)
397         y_pred[:, k] += (learning_rate
398                          * tree.value[:, 0, 0].take(terminal_regions, axis=0))
399 
400     @abstractmethod
401     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
402                                 residual, pred, sample_weight):
403         """Template method for updating terminal regions (=leaves). """
404 
405 
406 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
407     """Base class for regression loss functions.
408 
409     Parameters
410     ----------
411     n_classes : int
412         Number of classes
413     """
414     def __init__(self, n_classes):
415         if n_classes != 1:
416             raise ValueError("``n_classes`` must be 1 for regression but "
417                              "was %r" % n_classes)
418         super().__init__(n_classes)
419 
420 
421 class LeastSquaresError(RegressionLossFunction):
422     """Loss function for least squares (LS) estimation.
423     Terminal regions need not to be updated for least squares.
424 
425     Parameters
426     ----------
427     n_classes : int
428         Number of classes
429     """
430 
431     def init_estimator(self):
432         return MeanEstimator()
433 
434     def __call__(self, y, pred, sample_weight=None):
435         """Compute the least squares loss.
436 
437         Parameters
438         ----------
439         y : array, shape (n_samples,)
440             True labels
441 
442         pred : array, shape (n_samples,)
443             Predicted labels
444 
445         sample_weight : array-like, shape (n_samples,), optional
446             Sample weights.
447         """
448         if sample_weight is None:
449             return np.mean((y - pred.ravel()) ** 2.0)
450         else:
451             return (1.0 / sample_weight.sum() *
452                     np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))
453 
454     def negative_gradient(self, y, pred, **kargs):
455         """Compute the negative gradient.
456 
457         Parameters
458         ----------
459         y : array, shape (n_samples,)
460             The target labels.
461 
462         pred : array, shape (n_samples,)
463             The predictions.
464         """
465         return y - pred.ravel()
466 
467     def update_terminal_regions(self, tree, X, y, residual, y_pred,
468                                 sample_weight, sample_mask,
469                                 learning_rate=0.1, k=0):
470         """Least squares does not need to update terminal regions.
471 
472         But it has to update the predictions.
473 
474         Parameters
475         ----------
476         tree : tree.Tree
477             The tree object.
478         X : array, shape (n, m)
479             The data array.
480         y : array, shape (n,)
481             The target labels.
482         residual : array, shape (n,)
483             The residuals (usually the negative gradient).
484         y_pred : array, shape (n,)
485             The predictions.
486         sample_weight : array, shape (n,)
487             The weight of each sample.
488         sample_mask : array, shape (n,)
489             The sample mask to be used.
490         learning_rate : float, default=0.1
491             learning rate shrinks the contribution of each tree by
492              ``learning_rate``.
493         k : int, default 0
494             The index of the estimator being updated.
495         """
496         # update predictions
497         y_pred[:, k] += learning_rate * tree.predict(X).ravel()
498 
499     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
500                                 residual, pred, sample_weight):
501         pass
502 
503 
504 class LeastAbsoluteError(RegressionLossFunction):
505     """Loss function for least absolute deviation (LAD) regression.
506 
507     Parameters
508     ----------
509     n_classes : int
510         Number of classes
511     """
512     def init_estimator(self):
513         return QuantileEstimator(alpha=0.5)
514 
515     def __call__(self, y, pred, sample_weight=None):
516         """Compute the least absolute error.
517 
518         Parameters
519         ----------
520         y : array, shape (n_samples,)
521             True labels
522 
523         pred : array, shape (n_samples,)
524             Predicted labels
525 
526         sample_weight : array-like, shape (n_samples,), optional
527             Sample weights.
528         """
529         if sample_weight is None:
530             return np.abs(y - pred.ravel()).mean()
531         else:
532             return (1.0 / sample_weight.sum() *
533                     np.sum(sample_weight * np.abs(y - pred.ravel())))
534 
535     def negative_gradient(self, y, pred, **kargs):
536         """Compute the negative gradient.
537 
538         1.0 if y - pred > 0.0 else -1.0
539 
540         Parameters
541         ----------
542         y : array, shape (n_samples,)
543             The target labels.
544 
545         pred : array, shape (n_samples,)
546             The predictions.
547         """
548         pred = pred.ravel()
549         return 2.0 * (y - pred > 0.0) - 1.0
550 
551     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
552                                 residual, pred, sample_weight):
553         """LAD updates terminal regions to median estimates. """
554         terminal_region = np.where(terminal_regions == leaf)[0]
555         sample_weight = sample_weight.take(terminal_region, axis=0)
556         diff = y.take(terminal_region, axis=0) - pred.take(terminal_region, axis=0)
557         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)
558 
559 
560 class HuberLossFunction(RegressionLossFunction):
561     """Huber loss function for robust regression.
562 
563     M-Regression proposed in Friedman 2001.
564 
565     References
566     ----------
567     J. Friedman, Greedy Function Approximation: A Gradient Boosting
568     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
569 
570     Parameters
571     ----------
572     n_classes : int
573         Number of classes
574 
575     alpha : float
576         Percentile at which to extract score
577     """
578 
579     def __init__(self, n_classes, alpha=0.9):
580         super().__init__(n_classes)
581         self.alpha = alpha
582         self.gamma = None
583 
584     def init_estimator(self):
585         return QuantileEstimator(alpha=0.5)
586 
587     def __call__(self, y, pred, sample_weight=None):
588         """Compute the Huber loss.
589 
590         Parameters
591         ----------
592         y : array, shape (n_samples,)
593             True labels
594 
595         pred : array, shape (n_samples,)
596             Predicted labels
597 
598         sample_weight : array-like, shape (n_samples,), optional
599             Sample weights.
600         """
601         pred = pred.ravel()
602         diff = y - pred
603         gamma = self.gamma
604         if gamma is None:
605             if sample_weight is None:
606                 gamma = np.percentile(np.abs(diff), self.alpha * 100)
607             else:
608                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
609 
610         gamma_mask = np.abs(diff) <= gamma
611         if sample_weight is None:
612             sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2.0)
613             lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) - gamma / 2.0))
614             loss = (sq_loss + lin_loss) / y.shape[0]
615         else:
616             sq_loss = np.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2.0)
617             lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *
618                               (np.abs(diff[~gamma_mask]) - gamma / 2.0))
619             loss = (sq_loss + lin_loss) / sample_weight.sum()
620         return loss
621 
622     def negative_gradient(self, y, pred, sample_weight=None, **kargs):
623         """Compute the negative gradient.
624 
625         Parameters
626         ----------
627         y : array, shape (n_samples,)
628             The target labels.
629 
630         pred : array, shape (n_samples,)
631             The predictions.
632 
633         sample_weight : array-like, shape (n_samples,), optional
634             Sample weights.
635         """
636         pred = pred.ravel()
637         diff = y - pred
638         if sample_weight is None:
639             gamma = np.percentile(np.abs(diff), self.alpha * 100)
640         else:
641             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
642         gamma_mask = np.abs(diff) <= gamma
643         residual = np.zeros((y.shape[0],), dtype=np.float64)
644         residual[gamma_mask] = diff[gamma_mask]
645         residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])
646         self.gamma = gamma
647         return residual
648 
649     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
650                                 residual, pred, sample_weight):
651         terminal_region = np.where(terminal_regions == leaf)[0]
652         sample_weight = sample_weight.take(terminal_region, axis=0)
653         gamma = self.gamma
654         diff = (y.take(terminal_region, axis=0)
655                 - pred.take(terminal_region, axis=0))
656         median = _weighted_percentile(diff, sample_weight, percentile=50)
657         diff_minus_median = diff - median
658         tree.value[leaf, 0] = median + np.mean(
659             np.sign(diff_minus_median) *
660             np.minimum(np.abs(diff_minus_median), gamma))
661 
662 
663 class QuantileLossFunction(RegressionLossFunction):
664     """Loss function for quantile regression.
665 
666     Quantile regression allows to estimate the percentiles
667     of the conditional distribution of the target.
668 
669     Parameters
670     ----------
671     n_classes : int
672         Number of classes.
673 
674     alpha : float, optional (default = 0.9)
675         The percentile
676     """
677     def __init__(self, n_classes, alpha=0.9):
678         super().__init__(n_classes)
679         self.alpha = alpha
680         self.percentile = alpha * 100.0
681 
682     def init_estimator(self):
683         return QuantileEstimator(self.alpha)
684 
685     def __call__(self, y, pred, sample_weight=None):
686         """Compute the Quantile loss.
687 
688         Parameters
689         ----------
690         y : array, shape (n_samples,)
691             True labels
692 
693         pred : array, shape (n_samples,)
694             Predicted labels
695 
696         sample_weight : array-like, shape (n_samples,), optional
697             Sample weights.
698         """
699         pred = pred.ravel()
700         diff = y - pred
701         alpha = self.alpha
702 
703         mask = y > pred
704         if sample_weight is None:
705             loss = (alpha * diff[mask].sum() -
706                     (1.0 - alpha) * diff[~mask].sum()) / y.shape[0]
707         else:
708             loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
709                     (1.0 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])) /
710                     sample_weight.sum())
711         return loss
712 
713     def negative_gradient(self, y, pred, **kargs):
714         """Compute the negative gradient.
715 
716         Parameters
717         ----------
718         y : array, shape (n_samples,)
719             The target labels.
720 
721         pred : array, shape (n_samples,)
722             The predictions.
723         """
724         alpha = self.alpha
725         pred = pred.ravel()
726         mask = y > pred
727         return (alpha * mask) - ((1.0 - alpha) * ~mask)
728 
729     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
730                                 residual, pred, sample_weight):
731         terminal_region = np.where(terminal_regions == leaf)[0]
732         diff = (y.take(terminal_region, axis=0)
733                 - pred.take(terminal_region, axis=0))
734         sample_weight = sample_weight.take(terminal_region, axis=0)
735 
736         val = _weighted_percentile(diff, sample_weight, self.percentile)
737         tree.value[leaf, 0] = val
738 
739 
740 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
741     """Base class for classification loss functions. """
742 
743     def _score_to_proba(self, score):
744         """Template method to convert scores to probabilities.
745 
746          the does not support probabilities raises AttributeError.
747         """
748         raise TypeError('%s does not support predict_proba' % type(self).__name__)
749 
750     @abstractmethod
751     def _score_to_decision(self, score):
752         """Template method to convert scores to decisions.
753 
754         Returns int arrays.
755         """
756 
757 
758 class BinomialDeviance(ClassificationLossFunction):
759     """Binomial deviance loss function for binary classification.
760 
761     Binary classification is a special case; here, we only need to
762     fit one tree instead of ``n_classes`` trees.
763 
764     Parameters
765     ----------
766     n_classes : int
767         Number of classes.
768     """
769     def __init__(self, n_classes):
770         if n_classes != 2:
771             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
772                              .format(self.__class__.__name__, n_classes))
773         # we only need to fit one tree for binary clf.
774         super().__init__(1)
775 
776     def init_estimator(self):
777         return LogOddsEstimator()
778 
779     def __call__(self, y, pred, sample_weight=None):
780         """Compute the deviance (= 2 * negative log-likelihood).
781 
782         Parameters
783         ----------
784         y : array, shape (n_samples,)
785             True labels
786 
787         pred : array, shape (n_samples,)
788             Predicted labels
789 
790         sample_weight : array-like, shape (n_samples,), optional
791             Sample weights.
792         """
793         # logaddexp(0, v) == log(1.0 + exp(v))
794         pred = pred.ravel()
795         if sample_weight is None:
796             return -2.0 * np.mean((y * pred) - np.logaddexp(0.0, pred))
797         else:
798             return (-2.0 / sample_weight.sum() *
799                     np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))
800 
801     def negative_gradient(self, y, pred, **kargs):
802         """Compute the residual (= negative gradient).
803 
804         Parameters
805         ----------
806         y : array, shape (n_samples,)
807             True labels
808 
809         pred : array, shape (n_samples,)
810             Predicted labels
811         """
812         return y - expit(pred.ravel())
813 
814     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
815                                 residual, pred, sample_weight):
816         """Make a single Newton-Raphson step.
817 
818         our node estimate is given by:
819 
820             sum(w * (y - prob)) / sum(w * prob * (1 - prob))
821 
822         we take advantage that: y - prob = residual
823         """
824         terminal_region = np.where(terminal_regions == leaf)[0]
825         residual = residual.take(terminal_region, axis=0)
826         y = y.take(terminal_region, axis=0)
827         sample_weight = sample_weight.take(terminal_region, axis=0)
828 
829         numerator = np.sum(sample_weight * residual)
830         denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))
831 
832         # prevents overflow and division by zero
833         if abs(denominator) < 1e-150:
834             tree.value[leaf, 0, 0] = 0.0
835         else:
836             tree.value[leaf, 0, 0] = numerator / denominator
837 
838     def _score_to_proba(self, score):
839         proba = np.ones((score.shape[0], 2), dtype=np.float64)
840         proba[:, 1] = expit(score.ravel())
841         proba[:, 0] -= proba[:, 1]
842         return proba
843 
844     def _score_to_decision(self, score):
845         proba = self._score_to_proba(score)
846         return np.argmax(proba, axis=1)
847 
848 
849 class MultinomialDeviance(ClassificationLossFunction):
850     """Multinomial deviance loss function for multi-class classification.
851 
852     For multi-class classification we need to fit ``n_classes`` trees at
853     each stage.
854 
855     Parameters
856     ----------
857     n_classes : int
858         Number of classes
859     """
860 
861     is_multi_class = True
862 
863     def __init__(self, n_classes):
864         if n_classes < 3:
865             raise ValueError("{0:s} requires more than 2 classes.".format(
866                 self.__class__.__name__))
867         super().__init__(n_classes)
868 
869     def init_estimator(self):
870         return PriorProbabilityEstimator()
871 
872     def __call__(self, y, pred, sample_weight=None):
873         """Compute the Multinomial deviance.
874 
875         Parameters
876         ----------
877         y : array, shape (n_samples,)
878             True labels
879 
880         pred : array, shape (n_samples,)
881             Predicted labels
882 
883         sample_weight : array-like, shape (n_samples,), optional
884             Sample weights.
885         """
886         # create one-hot label encoding
887         Y = np.zeros((y.shape[0], self.K), dtype=np.float64)
888         for k in range(self.K):
889             Y[:, k] = y == k
890 
891         if sample_weight is None:
892             return np.sum(-1 * (Y * pred).sum(axis=1) +
893                           logsumexp(pred, axis=1))
894         else:
895             return np.sum(-1 * sample_weight * (Y * pred).sum(axis=1) +
896                           logsumexp(pred, axis=1))
897 
898     def negative_gradient(self, y, pred, k=0, **kwargs):
899         """Compute negative gradient for the ``k``-th class.
900 
901         Parameters
902         ----------
903         y : array, shape (n_samples,)
904             The target labels.
905 
906         pred : array, shape (n_samples,)
907             The predictions.
908 
909         k : int, optional (default=0)
910             The index of the class
911         """
912         return y - np.nan_to_num(np.exp(pred[:, k] -
913                                         logsumexp(pred, axis=1)))
914 
915     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
916                                 residual, pred, sample_weight):
917         """Make a single Newton-Raphson step. """
918         terminal_region = np.where(terminal_regions == leaf)[0]
919         residual = residual.take(terminal_region, axis=0)
920         y = y.take(terminal_region, axis=0)
921         sample_weight = sample_weight.take(terminal_region, axis=0)
922 
923         numerator = np.sum(sample_weight * residual)
924         numerator *= (self.K - 1) / self.K
925 
926         denominator = np.sum(sample_weight * (y - residual) *
927                              (1.0 - y + residual))
928 
929         # prevents overflow and division by zero
930         if abs(denominator) < 1e-150:
931             tree.value[leaf, 0, 0] = 0.0
932         else:
933             tree.value[leaf, 0, 0] = numerator / denominator
934 
935     def _score_to_proba(self, score):
936         return np.nan_to_num(
937             np.exp(score - (logsumexp(score, axis=1)[:, np.newaxis])))
938 
939     def _score_to_decision(self, score):
940         proba = self._score_to_proba(score)
941         return np.argmax(proba, axis=1)
942 
943 
944 class ExponentialLoss(ClassificationLossFunction):
945     """Exponential loss function for binary classification.
946 
947     Same loss as AdaBoost.
948 
949     References
950     ----------
951     Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
952 
953     Parameters
954     ----------
955     n_classes : int
956         Number of classes.
957     """
958     def __init__(self, n_classes):
959         if n_classes != 2:
960             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
961                              .format(self.__class__.__name__, n_classes))
962         # we only need to fit one tree for binary clf.
963         super().__init__(1)
964 
965     def init_estimator(self):
966         return ScaledLogOddsEstimator()
967 
968     def __call__(self, y, pred, sample_weight=None):
969         """Compute the exponential loss
970 
971         Parameters
972         ----------
973         y : array, shape (n_samples,)
974             True labels
975 
976         pred : array, shape (n_samples,)
977             Predicted labels
978 
979         sample_weight : array-like, shape (n_samples,), optional
980             Sample weights.
981         """
982         pred = pred.ravel()
983         if sample_weight is None:
984             return np.mean(np.exp(-(2. * y - 1.) * pred))
985         else:
986             return (1.0 / sample_weight.sum() *
987                     np.sum(sample_weight * np.exp(-(2 * y - 1) * pred)))
988 
989     def negative_gradient(self, y, pred, **kargs):
990         """Compute the residual (= negative gradient).
991 
992         Parameters
993         ----------
994         y : array, shape (n_samples,)
995             True labels
996 
997         pred : array, shape (n_samples,)
998             Predicted labels
999         """
1000         y_ = -(2. * y - 1.)
1001         return y_ * np.exp(y_ * pred.ravel())
1002 
1003     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
1004                                 residual, pred, sample_weight):
1005         terminal_region = np.where(terminal_regions == leaf)[0]
1006         pred = pred.take(terminal_region, axis=0)
1007         y = y.take(terminal_region, axis=0)
1008         sample_weight = sample_weight.take(terminal_region, axis=0)
1009 
1010         y_ = 2. * y - 1.
1011 
1012         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
1013         denominator = np.sum(sample_weight * np.exp(-y_ * pred))
1014 
1015         # prevents overflow and division by zero
1016         if abs(denominator) < 1e-150:
1017             tree.value[leaf, 0, 0] = 0.0
1018         else:
1019             tree.value[leaf, 0, 0] = numerator / denominator
1020 
1021     def _score_to_proba(self, score):
1022         proba = np.ones((score.shape[0], 2), dtype=np.float64)
1023         proba[:, 1] = expit(2.0 * score.ravel())
1024         proba[:, 0] -= proba[:, 1]
1025         return proba
1026 
1027     def _score_to_decision(self, score):
1028         return (score.ravel() >= 0.0).astype(np.int)
1029 
1030 
1031 LOSS_FUNCTIONS = {'ls': LeastSquaresError,
1032                   'lad': LeastAbsoluteError,
1033                   'huber': HuberLossFunction,
1034                   'quantile': QuantileLossFunction,
1035                   'deviance': None,    # for both, multinomial and binomial
1036                   'exponential': ExponentialLoss,
1037                   }
1038 
1039 
1040 INIT_ESTIMATORS = {'zero': ZeroEstimator}
1041 
1042 
1043 class VerboseReporter:
1044     """Reports verbose output to stdout.
1045 
1046     Parameters
1047     ----------
1048     verbose : int
1049         Verbosity level. If ``verbose==1`` output is printed once in a while
1050         (when iteration mod verbose_mod is zero).; if larger than 1 then output
1051         is printed for each update.
1052     """
1053 
1054     def __init__(self, verbose):
1055         self.verbose = verbose
1056 
1057     def init(self, est, begin_at_stage=0):
1058         """Initialize reporter
1059 
1060         Parameters
1061         ----------
1062         est : Estimator
1063             The estimator
1064 
1065         begin_at_stage : int
1066             stage at which to begin reporting
1067         """
1068         # header fields and line format str
1069         header_fields = ['Iter', 'Train Loss']
1070         verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']
1071         # do oob?
1072         if est.subsample < 1:
1073             header_fields.append('OOB Improve')
1074             verbose_fmt.append('{oob_impr:>16.4f}')
1075         header_fields.append('Remaining Time')
1076         verbose_fmt.append('{remaining_time:>16s}')
1077 
1078         # print the header line
1079         print(('%10s ' + '%16s ' *
1080                (len(header_fields) - 1)) % tuple(header_fields))
1081 
1082         self.verbose_fmt = ' '.join(verbose_fmt)
1083         # plot verbose info each time i % verbose_mod == 0
1084         self.verbose_mod = 1
1085         self.start_time = time()
1086         self.begin_at_stage = begin_at_stage
1087 
1088     def update(self, j, est):
1089         """Update reporter with new iteration.
1090 
1091         Parameters
1092         ----------
1093         j : int
1094             The new iteration
1095         est : Estimator
1096             The estimator
1097         """
1098         do_oob = est.subsample < 1
1099         # we need to take into account if we fit additional estimators.
1100         i = j - self.begin_at_stage  # iteration relative to the start iter
1101         if (i + 1) % self.verbose_mod == 0:
1102             oob_impr = est.oob_improvement_[j] if do_oob else 0
1103             remaining_time = ((est.n_estimators - (j + 1)) *
1104                               (time() - self.start_time) / float(i + 1))
1105             if remaining_time > 60:
1106                 remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)
1107             else:
1108                 remaining_time = '{0:.2f}s'.format(remaining_time)
1109             print(self.verbose_fmt.format(iter=j + 1,
1110                                           train_score=est.train_score_[j],
1111                                           oob_impr=oob_impr,
1112                                           remaining_time=remaining_time))
1113             if self.verbose == 1 and ((i + 1) // (self.verbose_mod * 10) > 0):
1114                 # adjust verbose frequency (powers of 10)
1115                 self.verbose_mod *= 10
1116 
1117 
1118 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1119     """Abstract base class for Gradient Boosting. """
1120 
1121     @abstractmethod
1122     def __init__(self, loss, learning_rate, n_estimators, criterion,
1123                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1124                  max_depth, min_impurity_decrease, min_impurity_split,
1125                  init, subsample, max_features,
1126                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1127                  warm_start=False, presort='auto',
1128                  validation_fraction=0.1, n_iter_no_change=None,
1129                  tol=1e-4):
1130 
1131         self.n_estimators = n_estimators
1132         self.learning_rate = learning_rate
1133         self.loss = loss
1134         self.criterion = criterion
1135         self.min_samples_split = min_samples_split
1136         self.min_samples_leaf = min_samples_leaf
1137         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1138         self.subsample = subsample
1139         self.max_features = max_features
1140         self.max_depth = max_depth
1141         self.min_impurity_decrease = min_impurity_decrease
1142         self.min_impurity_split = min_impurity_split
1143         self.init = init
1144         self.random_state = random_state
1145         self.alpha = alpha
1146         self.verbose = verbose
1147         self.max_leaf_nodes = max_leaf_nodes
1148         self.warm_start = warm_start
1149         self.presort = presort
1150         self.validation_fraction = validation_fraction
1151         self.n_iter_no_change = n_iter_no_change
1152         self.tol = tol
1153 
1154     def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
1155                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1156         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1157 
1158         assert sample_mask.dtype == np.bool
1159         loss = self.loss_
1160         original_y = y
1161 
1162         # Need to pass a copy of y_pred to negative_gradient() because y_pred
1163         # is partially updated at the end of the loop in
1164         # update_terminal_regions(), and gradients need to be evaluated at
1165         # iteration i - 1.
1166         y_pred_copy = y_pred.copy()
1167 
1168         for k in range(loss.K):
1169             if loss.is_multi_class:
1170                 y = np.array(original_y == k, dtype=np.float64)
1171 
1172             residual = loss.negative_gradient(y, y_pred_copy, k=k,
1173                                               sample_weight=sample_weight)
1174 
1175             # induce regression tree on residuals
1176             tree = DecisionTreeRegressor(
1177                 criterion=self.criterion,
1178                 splitter='best',
1179                 max_depth=self.max_depth,
1180                 min_samples_split=self.min_samples_split,
1181                 min_samples_leaf=self.min_samples_leaf,
1182                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1183                 min_impurity_decrease=self.min_impurity_decrease,
1184                 min_impurity_split=self.min_impurity_split,
1185                 max_features=self.max_features,
1186                 max_leaf_nodes=self.max_leaf_nodes,
1187                 random_state=random_state,
1188                 presort=self.presort)
1189 
1190             if self.subsample < 1.0:
1191                 # no inplace multiplication!
1192                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1193 
1194             X = X_csr if X_csr is not None else X
1195             tree.fit(X, residual, sample_weight=sample_weight,
1196                      check_input=False, X_idx_sorted=X_idx_sorted)
1197 
1198             # update tree leaves
1199             loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
1200                                          sample_weight, sample_mask,
1201                                          learning_rate=self.learning_rate, k=k)
1202 
1203             # add tree to ensemble
1204             self.estimators_[i, k] = tree
1205 
1206         return y_pred
1207 
1208     def _check_params(self):
1209         """Check validity of parameters and raise ValueError if not valid. """
1210         if self.n_estimators <= 0:
1211             raise ValueError("n_estimators must be greater than 0 but "
1212                              "was %r" % self.n_estimators)
1213 
1214         if self.learning_rate <= 0.0:
1215             raise ValueError("learning_rate must be greater than 0 but "
1216                              "was %r" % self.learning_rate)
1217 
1218         if (self.loss not in self._SUPPORTED_LOSS
1219                 or self.loss not in LOSS_FUNCTIONS):
1220             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1221 
1222         if self.loss == 'deviance':
1223             loss_class = (MultinomialDeviance
1224                           if len(self.classes_) > 2
1225                           else BinomialDeviance)
1226         else:
1227             loss_class = LOSS_FUNCTIONS[self.loss]
1228 
1229         if self.loss in ('huber', 'quantile'):
1230             self.loss_ = loss_class(self.n_classes_, self.alpha)
1231         else:
1232             self.loss_ = loss_class(self.n_classes_)
1233 
1234         if not (0.0 < self.subsample <= 1.0):
1235             raise ValueError("subsample must be in (0,1] but "
1236                              "was %r" % self.subsample)
1237 
1238         if self.init is not None:
1239             if isinstance(self.init, str):
1240                 if self.init not in INIT_ESTIMATORS:
1241                     raise ValueError('init="%s" is not supported' % self.init)
1242             else:
1243                 if (not hasattr(self.init, 'fit')
1244                         or not hasattr(self.init, 'predict')):
1245                     raise ValueError("init=%r must be valid BaseEstimator "
1246                                      "and support both fit and "
1247                                      "predict" % self.init)
1248 
1249         if not (0.0 < self.alpha < 1.0):
1250             raise ValueError("alpha must be in (0.0, 1.0) but "
1251                              "was %r" % self.alpha)
1252 
1253         if isinstance(self.max_features, str):
1254             if self.max_features == "auto":
1255                 # if is_classification
1256                 if self.n_classes_ > 1:
1257                     max_features = max(1, int(np.sqrt(self.n_features_)))
1258                 else:
1259                     # is regression
1260                     max_features = self.n_features_
1261             elif self.max_features == "sqrt":
1262                 max_features = max(1, int(np.sqrt(self.n_features_)))
1263             elif self.max_features == "log2":
1264                 max_features = max(1, int(np.log2(self.n_features_)))
1265             else:
1266                 raise ValueError("Invalid value for max_features: %r. "
1267                                  "Allowed string values are 'auto', 'sqrt' "
1268                                  "or 'log2'." % self.max_features)
1269         elif self.max_features is None:
1270             max_features = self.n_features_
1271         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1272             max_features = self.max_features
1273         else:  # float
1274             if 0. < self.max_features <= 1.:
1275                 max_features = max(int(self.max_features *
1276                                        self.n_features_), 1)
1277             else:
1278                 raise ValueError("max_features must be in (0, n_features]")
1279 
1280         self.max_features_ = max_features
1281 
1282         if not isinstance(self.n_iter_no_change,
1283                           (numbers.Integral, np.integer, type(None))):
1284             raise ValueError("n_iter_no_change should either be None or an "
1285                              "integer. %r was passed"
1286                              % self.n_iter_no_change)
1287 
1288         allowed_presort = ('auto', True, False)
1289         if self.presort not in allowed_presort:
1290             raise ValueError("'presort' should be in {}. Got {!r} instead."
1291                              .format(allowed_presort, self.presort))
1292 
1293     def _init_state(self):
1294         """Initialize model state and allocate model state data structures. """
1295 
1296         if self.init is None:
1297             self.init_ = self.loss_.init_estimator()
1298         elif isinstance(self.init, str):
1299             self.init_ = INIT_ESTIMATORS[self.init]()
1300         else:
1301             self.init_ = self.init
1302 
1303         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1304                                     dtype=np.object)
1305         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1306         # do oob?
1307         if self.subsample < 1.0:
1308             self.oob_improvement_ = np.zeros((self.n_estimators),
1309                                              dtype=np.float64)
1310 
1311     def _clear_state(self):
1312         """Clear the state of the gradient boosting model. """
1313         if hasattr(self, 'estimators_'):
1314             self.estimators_ = np.empty((0, 0), dtype=np.object)
1315         if hasattr(self, 'train_score_'):
1316             del self.train_score_
1317         if hasattr(self, 'oob_improvement_'):
1318             del self.oob_improvement_
1319         if hasattr(self, 'init_'):
1320             del self.init_
1321         if hasattr(self, '_rng'):
1322             del self._rng
1323 
1324     def _resize_state(self):
1325         """Add additional ``n_estimators`` entries to all attributes. """
1326         # self.n_estimators is the number of additional est to fit
1327         total_n_estimators = self.n_estimators
1328         if total_n_estimators < self.estimators_.shape[0]:
1329             raise ValueError('resize with smaller n_estimators %d < %d' %
1330                              (total_n_estimators, self.estimators_[0]))
1331 
1332         self.estimators_ = np.resize(self.estimators_,
1333                                      (total_n_estimators, self.loss_.K))
1334         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1335         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1336             # if do oob resize arrays or create new if not available
1337             if hasattr(self, 'oob_improvement_'):
1338                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1339                                                   total_n_estimators)
1340             else:
1341                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1342                                                  dtype=np.float64)
1343 
1344     def _is_initialized(self):
1345         return len(getattr(self, 'estimators_', [])) > 0
1346 
1347     def _check_initialized(self):
1348         """Check that the estimator is initialized, raising an error if not."""
1349         check_is_fitted(self, 'estimators_')
1350 
1351     @property
1352     @deprecated("Attribute n_features was deprecated in version 0.19 and "
1353                 "will be removed in 0.21.")
1354     def n_features(self):
1355         return self.n_features_
1356 
1357     def fit(self, X, y, sample_weight=None, monitor=None):
1358         """Fit the gradient boosting model.
1359 
1360         Parameters
1361         ----------
1362         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1363             The input samples. Internally, it will be converted to
1364             ``dtype=np.float32`` and if a sparse matrix is provided
1365             to a sparse ``csr_matrix``.
1366 
1367         y : array-like, shape (n_samples,)
1368             Target values (strings or integers in classification, real numbers
1369             in regression)
1370             For classification, labels must correspond to classes.
1371 
1372         sample_weight : array-like, shape (n_samples,) or None
1373             Sample weights. If None, then samples are equally weighted. Splits
1374             that would create child nodes with net zero or negative weight are
1375             ignored while searching for a split in each node. In the case of
1376             classification, splits are also ignored if they would result in any
1377             single class carrying a negative weight in either child node.
1378 
1379         monitor : callable, optional
1380             The monitor is called after each iteration with the current
1381             iteration, a reference to the estimator and the local variables of
1382             ``_fit_stages`` as keyword arguments ``callable(i, self,
1383             locals())``. If the callable returns ``True`` the fitting procedure
1384             is stopped. The monitor can be used for various things such as
1385             computing held-out estimates, early stopping, model introspect, and
1386             snapshoting.
1387 
1388         Returns
1389         -------
1390         self : object
1391         """
1392         # if not warmstart - clear the estimator state
1393         if not self.warm_start:
1394             self._clear_state()
1395 
1396         # Check input
1397         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1398         n_samples, self.n_features_ = X.shape
1399         if sample_weight is None:
1400             sample_weight = np.ones(n_samples, dtype=np.float32)
1401         else:
1402             sample_weight = column_or_1d(sample_weight, warn=True)
1403 
1404         check_consistent_length(X, y, sample_weight)
1405 
1406         y = self._validate_y(y, sample_weight)
1407 
1408         if self.n_iter_no_change is not None:
1409             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1410                 train_test_split(X, y, sample_weight,
1411                                  random_state=self.random_state,
1412                                  test_size=self.validation_fraction))
1413         else:
1414             X_val = y_val = sample_weight_val = None
1415 
1416         self._check_params()
1417 
1418         if not self._is_initialized():
1419             # init state
1420             self._init_state()
1421 
1422             # fit initial model - FIXME make sample_weight optional
1423             self.init_.fit(X, y, sample_weight)
1424 
1425             # init predictions
1426             y_pred = self.init_.predict(X)
1427             begin_at_stage = 0
1428 
1429             # The rng state must be preserved if warm_start is True
1430             self._rng = check_random_state(self.random_state)
1431 
1432         else:
1433             # add more estimators to fitted model
1434             # invariant: warm_start = True
1435             if self.n_estimators < self.estimators_.shape[0]:
1436                 raise ValueError('n_estimators=%d must be larger or equal to '
1437                                  'estimators_.shape[0]=%d when '
1438                                  'warm_start==True'
1439                                  % (self.n_estimators,
1440                                     self.estimators_.shape[0]))
1441             begin_at_stage = self.estimators_.shape[0]
1442             # The requirements of _decision_function (called in two lines
1443             # below) are more constrained than fit. It accepts only CSR
1444             # matrices.
1445             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1446             y_pred = self._decision_function(X)
1447             self._resize_state()
1448 
1449         if self.presort is True and issparse(X):
1450             raise ValueError(
1451                 "Presorting is not supported for sparse matrices.")
1452 
1453         presort = self.presort
1454         # Allow presort to be 'auto', which means True if the dataset is dense,
1455         # otherwise it will be False.
1456         if presort == 'auto':
1457             presort = not issparse(X)
1458 
1459         X_idx_sorted = None
1460         if presort:
1461             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1462                                              dtype=np.int32)
1463 
1464         # fit the boosting stages
1465         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
1466                                     X_val, y_val, sample_weight_val,
1467                                     begin_at_stage, monitor, X_idx_sorted)
1468 
1469         # change shape of arrays after fit (early-stopping or additional ests)
1470         if n_stages != self.estimators_.shape[0]:
1471             self.estimators_ = self.estimators_[:n_stages]
1472             self.train_score_ = self.train_score_[:n_stages]
1473             if hasattr(self, 'oob_improvement_'):
1474                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1475 
1476         self.n_estimators_ = n_stages
1477         return self
1478 
1479     def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
1480                     X_val, y_val, sample_weight_val,
1481                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1482         """Iteratively fits the stages.
1483 
1484         For each stage it computes the progress (OOB, train score)
1485         and delegates to ``_fit_stage``.
1486         Returns the number of stages fit; might differ from ``n_estimators``
1487         due to early stopping.
1488         """
1489         n_samples = X.shape[0]
1490         do_oob = self.subsample < 1.0
1491         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1492         n_inbag = max(1, int(self.subsample * n_samples))
1493         loss_ = self.loss_
1494 
1495         # Set min_weight_leaf from min_weight_fraction_leaf
1496         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1497             min_weight_leaf = (self.min_weight_fraction_leaf *
1498                                np.sum(sample_weight))
1499         else:
1500             min_weight_leaf = 0.
1501 
1502         if self.verbose:
1503             verbose_reporter = VerboseReporter(self.verbose)
1504             verbose_reporter.init(self, begin_at_stage)
1505 
1506         X_csc = csc_matrix(X) if issparse(X) else None
1507         X_csr = csr_matrix(X) if issparse(X) else None
1508 
1509         if self.n_iter_no_change is not None:
1510             loss_history = np.full(self.n_iter_no_change, np.inf)
1511             # We create a generator to get the predictions for X_val after
1512             # the addition of each successive stage
1513             y_val_pred_iter = self._staged_decision_function(X_val)
1514 
1515         # perform boosting iterations
1516         i = begin_at_stage
1517         for i in range(begin_at_stage, self.n_estimators):
1518 
1519             # subsampling
1520             if do_oob:
1521                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1522                                                   random_state)
1523                 # OOB score before adding this stage
1524                 old_oob_score = loss_(y[~sample_mask],
1525                                       y_pred[~sample_mask],
1526                                       sample_weight[~sample_mask])
1527 
1528             # fit next stage of trees
1529             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
1530                                      sample_mask, random_state, X_idx_sorted,
1531                                      X_csc, X_csr)
1532 
1533             # track deviance (= loss)
1534             if do_oob:
1535                 self.train_score_[i] = loss_(y[sample_mask],
1536                                              y_pred[sample_mask],
1537                                              sample_weight[sample_mask])
1538                 self.oob_improvement_[i] = (
1539                     old_oob_score - loss_(y[~sample_mask],
1540                                           y_pred[~sample_mask],
1541                                           sample_weight[~sample_mask]))
1542             else:
1543                 # no need to fancy index w/ no subsampling
1544                 self.train_score_[i] = loss_(y, y_pred, sample_weight)
1545 
1546             if self.verbose > 0:
1547                 verbose_reporter.update(i, self)
1548 
1549             if monitor is not None:
1550                 early_stopping = monitor(i, self, locals())
1551                 if early_stopping:
1552                     break
1553 
1554             # We also provide an early stopping based on the score from
1555             # validation set (X_val, y_val), if n_iter_no_change is set
1556             if self.n_iter_no_change is not None:
1557                 # By calling next(y_val_pred_iter), we get the predictions
1558                 # for X_val after the addition of the current stage
1559                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1560                                         sample_weight_val)
1561 
1562                 # Require validation_score to be better (less) than at least
1563                 # one of the last n_iter_no_change evaluations
1564                 if np.any(validation_loss + self.tol < loss_history):
1565                     loss_history[i % len(loss_history)] = validation_loss
1566                 else:
1567                     break
1568 
1569         return i + 1
1570 
1571     def _make_estimator(self, append=True):
1572         # we don't need _make_estimator
1573         raise NotImplementedError()
1574 
1575     def _init_decision_function(self, X):
1576         """Check input and compute prediction of ``init``. """
1577         self._check_initialized()
1578         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1579         if X.shape[1] != self.n_features_:
1580             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1581                 self.n_features_, X.shape[1]))
1582         score = self.init_.predict(X).astype(np.float64)
1583         return score
1584 
1585     def _decision_function(self, X):
1586         # for use in inner loop, not raveling the output in single-class case,
1587         # not doing input validation.
1588         score = self._init_decision_function(X)
1589         predict_stages(self.estimators_, X, self.learning_rate, score)
1590         return score
1591 
1592 
1593     def _staged_decision_function(self, X):
1594         """Compute decision function of ``X`` for each iteration.
1595 
1596         This method allows monitoring (i.e. determine error on testing set)
1597         after each stage.
1598 
1599         Parameters
1600         ----------
1601         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1602             The input samples. Internally, it will be converted to
1603             ``dtype=np.float32`` and if a sparse matrix is provided
1604             to a sparse ``csr_matrix``.
1605 
1606         Returns
1607         -------
1608         score : generator of array, shape (n_samples, k)
1609             The decision function of the input samples. The order of the
1610             classes corresponds to that in the attribute `classes_`.
1611             Regression and binary classification are special cases with
1612             ``k == 1``, otherwise ``k==n_classes``.
1613         """
1614         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1615         score = self._init_decision_function(X)
1616         for i in range(self.estimators_.shape[0]):
1617             predict_stage(self.estimators_, i, X, self.learning_rate, score)
1618             yield score.copy()
1619 
1620     @property
1621     def feature_importances_(self):
1622         """Return the feature importances (the higher, the more important the
1623            feature).
1624 
1625         Returns
1626         -------
1627         feature_importances_ : array, shape (n_features,)
1628         """
1629         self._check_initialized()
1630 
1631         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1632         for stage in self.estimators_:
1633             stage_sum = sum(tree.tree_.compute_feature_importances(
1634                 normalize=False) for tree in stage) / len(stage)
1635             total_sum += stage_sum
1636 
1637         importances = total_sum / total_sum.sum()
1638         return importances
1639 
1640     def _validate_y(self, y, sample_weight):
1641         # 'sample_weight' is not utilised but is used for
1642         # consistency with similar method _validate_y of GBC
1643         self.n_classes_ = 1
1644         if y.dtype.kind == 'O':
1645             y = y.astype(np.float64)
1646         # Default implementation
1647         return y
1648 
1649     def apply(self, X):
1650         """Apply trees in the ensemble to X, return leaf indices.
1651 
1652         .. versionadded:: 0.17
1653 
1654         Parameters
1655         ----------
1656         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1657             The input samples. Internally, its dtype will be converted to
1658             ``dtype=np.float32``. If a sparse matrix is provided, it will
1659             be converted to a sparse ``csr_matrix``.
1660 
1661         Returns
1662         -------
1663         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1664             For each datapoint x in X and for each tree in the ensemble,
1665             return the index of the leaf x ends up in each estimator.
1666             In the case of binary classification n_classes is 1.
1667         """
1668 
1669         self._check_initialized()
1670         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1671 
1672         # n_classes will be equal to 1 in the binary classification or the
1673         # regression case.
1674         n_estimators, n_classes = self.estimators_.shape
1675         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1676 
1677         for i in range(n_estimators):
1678             for j in range(n_classes):
1679                 estimator = self.estimators_[i, j]
1680                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1681 
1682         return leaves
1683 
1684 
1685 class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
1686     """Gradient Boosting for classification.
1687 
1688     GB builds an additive model in a
1689     forward stage-wise fashion; it allows for the optimization of
1690     arbitrary differentiable loss functions. In each stage ``n_classes_``
1691     regression trees are fit on the negative gradient of the
1692     binomial or multinomial deviance loss function. Binary classification
1693     is a special case where only a single regression tree is induced.
1694 
1695     Read more in the :ref:`User Guide <gradient_boosting>`.
1696 
1697     Parameters
1698     ----------
1699     loss : {'deviance', 'exponential'}, optional (default='deviance')
1700         loss function to be optimized. 'deviance' refers to
1701         deviance (= logistic regression) for classification
1702         with probabilistic outputs. For loss 'exponential' gradient
1703         boosting recovers the AdaBoost algorithm.
1704 
1705     learning_rate : float, optional (default=0.1)
1706         learning rate shrinks the contribution of each tree by `learning_rate`.
1707         There is a trade-off between learning_rate and n_estimators.
1708 
1709     n_estimators : int (default=100)
1710         The number of boosting stages to perform. Gradient boosting
1711         is fairly robust to over-fitting so a large number usually
1712         results in better performance.
1713 
1714     subsample : float, optional (default=1.0)
1715         The fraction of samples to be used for fitting the individual base
1716         learners. If smaller than 1.0 this results in Stochastic Gradient
1717         Boosting. `subsample` interacts with the parameter `n_estimators`.
1718         Choosing `subsample < 1.0` leads to a reduction of variance
1719         and an increase in bias.
1720 
1721     criterion : string, optional (default="friedman_mse")
1722         The function to measure the quality of a split. Supported criteria
1723         are "friedman_mse" for the mean squared error with improvement
1724         score by Friedman, "mse" for mean squared error, and "mae" for
1725         the mean absolute error. The default value of "friedman_mse" is
1726         generally the best as it can provide a better approximation in
1727         some cases.
1728 
1729         .. versionadded:: 0.18
1730 
1731     min_samples_split : int, float, optional (default=2)
1732         The minimum number of samples required to split an internal node:
1733 
1734         - If int, then consider `min_samples_split` as the minimum number.
1735         - If float, then `min_samples_split` is a fraction and
1736           `ceil(min_samples_split * n_samples)` are the minimum
1737           number of samples for each split.
1738 
1739         .. versionchanged:: 0.18
1740            Added float values for fractions.
1741 
1742     min_samples_leaf : int, float, optional (default=1)
1743         The minimum number of samples required to be at a leaf node.
1744         A split point at any depth will only be considered if it leaves at
1745         least ``min_samples_leaf`` training samples in each of the left and
1746         right branches.  This may have the effect of smoothing the model,
1747         especially in regression.
1748 
1749         - If int, then consider `min_samples_leaf` as the minimum number.
1750         - If float, then `min_samples_leaf` is a fraction and
1751           `ceil(min_samples_leaf * n_samples)` are the minimum
1752           number of samples for each node.
1753 
1754         .. versionchanged:: 0.18
1755            Added float values for fractions.
1756 
1757     min_weight_fraction_leaf : float, optional (default=0.)
1758         The minimum weighted fraction of the sum total of weights (of all
1759         the input samples) required to be at a leaf node. Samples have
1760         equal weight when sample_weight is not provided.
1761 
1762     max_depth : integer, optional (default=3)
1763         maximum depth of the individual regression estimators. The maximum
1764         depth limits the number of nodes in the tree. Tune this parameter
1765         for best performance; the best value depends on the interaction
1766         of the input variables.
1767 
1768     min_impurity_decrease : float, optional (default=0.)
1769         A node will be split if this split induces a decrease of the impurity
1770         greater than or equal to this value.
1771 
1772         The weighted impurity decrease equation is the following::
1773 
1774             N_t / N * (impurity - N_t_R / N_t * right_impurity
1775                                 - N_t_L / N_t * left_impurity)
1776 
1777         where ``N`` is the total number of samples, ``N_t`` is the number of
1778         samples at the current node, ``N_t_L`` is the number of samples in the
1779         left child, and ``N_t_R`` is the number of samples in the right child.
1780 
1781         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1782         if ``sample_weight`` is passed.
1783 
1784         .. versionadded:: 0.19
1785 
1786     min_impurity_split : float, (default=1e-7)
1787         Threshold for early stopping in tree growth. A node will split
1788         if its impurity is above the threshold, otherwise it is a leaf.
1789 
1790         .. deprecated:: 0.19
1791            ``min_impurity_split`` has been deprecated in favor of
1792            ``min_impurity_decrease`` in 0.19. The default value of
1793            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1794            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1795 
1796     init : estimator, optional
1797         An estimator object that is used to compute the initial
1798         predictions. ``init`` has to provide ``fit`` and ``predict``.
1799         If None it uses ``loss.init_estimator``.
1800 
1801     random_state : int, RandomState instance or None, optional (default=None)
1802         If int, random_state is the seed used by the random number generator;
1803         If RandomState instance, random_state is the random number generator;
1804         If None, the random number generator is the RandomState instance used
1805         by `np.random`.
1806 
1807     max_features : int, float, string or None, optional (default=None)
1808         The number of features to consider when looking for the best split:
1809 
1810         - If int, then consider `max_features` features at each split.
1811         - If float, then `max_features` is a fraction and
1812           `int(max_features * n_features)` features are considered at each
1813           split.
1814         - If "auto", then `max_features=sqrt(n_features)`.
1815         - If "sqrt", then `max_features=sqrt(n_features)`.
1816         - If "log2", then `max_features=log2(n_features)`.
1817         - If None, then `max_features=n_features`.
1818 
1819         Choosing `max_features < n_features` leads to a reduction of variance
1820         and an increase in bias.
1821 
1822         Note: the search for a split does not stop until at least one
1823         valid partition of the node samples is found, even if it requires to
1824         effectively inspect more than ``max_features`` features.
1825 
1826     verbose : int, default: 0
1827         Enable verbose output. If 1 then it prints progress and performance
1828         once in a while (the more trees the lower the frequency). If greater
1829         than 1 then it prints progress and performance for every tree.
1830 
1831     max_leaf_nodes : int or None, optional (default=None)
1832         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1833         Best nodes are defined as relative reduction in impurity.
1834         If None then unlimited number of leaf nodes.
1835 
1836     warm_start : bool, default: False
1837         When set to ``True``, reuse the solution of the previous call to fit
1838         and add more estimators to the ensemble, otherwise, just erase the
1839         previous solution. See :term:`the Glossary <warm_start>`.
1840 
1841     presort : bool or 'auto', optional (default='auto')
1842         Whether to presort the data to speed up the finding of best splits in
1843         fitting. Auto mode by default will use presorting on dense data and
1844         default to normal sorting on sparse data. Setting presort to true on
1845         sparse data will raise an error.
1846 
1847         .. versionadded:: 0.17
1848            *presort* parameter.
1849 
1850     validation_fraction : float, optional, default 0.1
1851         The proportion of training data to set aside as validation set for
1852         early stopping. Must be between 0 and 1.
1853         Only used if ``n_iter_no_change`` is set to an integer.
1854 
1855         .. versionadded:: 0.20
1856 
1857     n_iter_no_change : int, default None
1858         ``n_iter_no_change`` is used to decide if early stopping will be used
1859         to terminate training when validation score is not improving. By
1860         default it is set to None to disable early stopping. If set to a
1861         number, it will set aside ``validation_fraction`` size of the training
1862         data as validation and terminate training when validation score is not
1863         improving in all of the previous ``n_iter_no_change`` numbers of
1864         iterations.
1865 
1866         .. versionadded:: 0.20
1867 
1868     tol : float, optional, default 1e-4
1869         Tolerance for the early stopping. When the loss is not improving
1870         by at least tol for ``n_iter_no_change`` iterations (if set to a
1871         number), the training stops.
1872 
1873         .. versionadded:: 0.20
1874 
1875     Attributes
1876     ----------
1877     n_estimators_ : int
1878         The number of estimators as selected by early stopping (if
1879         ``n_iter_no_change`` is specified). Otherwise it is set to
1880         ``n_estimators``.
1881 
1882         .. versionadded:: 0.20
1883 
1884     feature_importances_ : array, shape (n_features,)
1885         The feature importances (the higher, the more important the feature).
1886 
1887     oob_improvement_ : array, shape (n_estimators,)
1888         The improvement in loss (= deviance) on the out-of-bag samples
1889         relative to the previous iteration.
1890         ``oob_improvement_[0]`` is the improvement in
1891         loss of the first stage over the ``init`` estimator.
1892 
1893     train_score_ : array, shape (n_estimators,)
1894         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
1895         model at iteration ``i`` on the in-bag sample.
1896         If ``subsample == 1`` this is the deviance on the training data.
1897 
1898     loss_ : LossFunction
1899         The concrete ``LossFunction`` object.
1900 
1901     init_ : estimator
1902         The estimator that provides the initial predictions.
1903         Set via the ``init`` argument or ``loss.init_estimator``.
1904 
1905     estimators_ : ndarray of DecisionTreeRegressor,\
1906 shape (n_estimators, ``loss_.K``)
1907         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary
1908         classification, otherwise n_classes.
1909 
1910     Notes
1911     -----
1912     The features are always randomly permuted at each split. Therefore,
1913     the best found split may vary, even with the same training data and
1914     ``max_features=n_features``, if the improvement of the criterion is
1915     identical for several splits enumerated during the search of the best
1916     split. To obtain a deterministic behaviour during fitting,
1917     ``random_state`` has to be fixed.
1918 
1919     See also
1920     --------
1921     sklearn.tree.DecisionTreeClassifier, RandomForestClassifier
1922     AdaBoostClassifier
1923 
1924     References
1925     ----------
1926     J. Friedman, Greedy Function Approximation: A Gradient Boosting
1927     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
1928 
1929     J. Friedman, Stochastic Gradient Boosting, 1999
1930 
1931     T. Hastie, R. Tibshirani and J. Friedman.
1932     Elements of Statistical Learning Ed. 2, Springer, 2009.
1933     """
1934 
1935     _SUPPORTED_LOSS = ('deviance', 'exponential')
1936 
1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
1940                  max_depth=3, min_impurity_decrease=0.,
1941                  min_impurity_split=None, init=None,
1942                  random_state=None, max_features=None, verbose=0,
1943                  max_leaf_nodes=None, warm_start=False,
1944                  presort='auto', validation_fraction=0.1,
1945                  n_iter_no_change=None, tol=1e-4):
1946 
1947         super().__init__(
1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
1949             criterion=criterion, min_samples_split=min_samples_split,
1950             min_samples_leaf=min_samples_leaf,
1951             min_weight_fraction_leaf=min_weight_fraction_leaf,
1952             max_depth=max_depth, init=init, subsample=subsample,
1953             max_features=max_features,
1954             random_state=random_state, verbose=verbose,
1955             max_leaf_nodes=max_leaf_nodes,
1956             min_impurity_decrease=min_impurity_decrease,
1957             min_impurity_split=min_impurity_split,
1958             warm_start=warm_start, presort=presort,
1959             validation_fraction=validation_fraction,
1960             n_iter_no_change=n_iter_no_change, tol=tol)
1961 
1962     def _validate_y(self, y, sample_weight):
1963         check_classification_targets(y)
1964         self.classes_, y = np.unique(y, return_inverse=True)
1965         n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))
1966         if n_trim_classes < 2:
1967             raise ValueError("y contains %d class after sample_weight "
1968                              "trimmed classes with zero weights, while a "
1969                              "minimum of 2 classes are required."
1970                              % n_trim_classes)
1971         self.n_classes_ = len(self.classes_)
1972         return y
1973 
1974     def decision_function(self, X):
1975         """Compute the decision function of ``X``.
1976 
1977         Parameters
1978         ----------
1979         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1980             The input samples. Internally, it will be converted to
1981             ``dtype=np.float32`` and if a sparse matrix is provided
1982             to a sparse ``csr_matrix``.
1983 
1984         Returns
1985         -------
1986         score : array, shape (n_samples, n_classes) or (n_samples,)
1987             The decision function of the input samples. The order of the
1988             classes corresponds to that in the attribute `classes_`.
1989             Regression and binary classification produce an array of shape
1990             [n_samples].
1991         """
1992         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1993         score = self._decision_function(X)
1994         if score.shape[1] == 1:
1995             return score.ravel()
1996         return score
1997 
1998     def staged_decision_function(self, X):
1999         """Compute decision function of ``X`` for each iteration.
2000 
2001         This method allows monitoring (i.e. determine error on testing set)
2002         after each stage.
2003 
2004         Parameters
2005         ----------
2006         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2007             The input samples. Internally, it will be converted to
2008             ``dtype=np.float32`` and if a sparse matrix is provided
2009             to a sparse ``csr_matrix``.
2010 
2011         Returns
2012         -------
2013         score : generator of array, shape (n_samples, k)
2014             The decision function of the input samples. The order of the
2015             classes corresponds to that in the attribute `classes_`.
2016             Regression and binary classification are special cases with
2017             ``k == 1``, otherwise ``k==n_classes``.
2018         """
2019         yield from self._staged_decision_function(X)
2020 
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)
2039 
2040     def staged_predict(self, X):
2041         """Predict class at each stage for X.
2042 
2043         This method allows monitoring (i.e. determine error on testing set)
2044         after each stage.
2045 
2046         Parameters
2047         ----------
2048         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2049             The input samples. Internally, it will be converted to
2050             ``dtype=np.float32`` and if a sparse matrix is provided
2051             to a sparse ``csr_matrix``.
2052 
2053         Returns
2054         -------
2055         y : generator of array of shape (n_samples,)
2056             The predicted value of the input samples.
2057         """
2058         for score in self._staged_decision_function(X):
2059             decisions = self.loss_._score_to_decision(score)
2060             yield self.classes_.take(decisions, axis=0)
2061 
2062     def predict_proba(self, X):
2063         """Predict class probabilities for X.
2064 
2065         Parameters
2066         ----------
2067         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2068             The input samples. Internally, it will be converted to
2069             ``dtype=np.float32`` and if a sparse matrix is provided
2070             to a sparse ``csr_matrix``.
2071 
2072         Raises
2073         ------
2074         AttributeError
2075             If the ``loss`` does not support probabilities.
2076 
2077         Returns
2078         -------
2079         p : array, shape (n_samples, n_classes)
2080             The class probabilities of the input samples. The order of the
2081             classes corresponds to that in the attribute `classes_`.
2082         """
2083         score = self.decision_function(X)
2084         try:
2085             return self.loss_._score_to_proba(score)
2086         except NotFittedError:
2087             raise
2088         except AttributeError:
2089             raise AttributeError('loss=%r does not support predict_proba' %
2090                                  self.loss)
2091 
2092     def predict_log_proba(self, X):
2093         """Predict class log-probabilities for X.
2094 
2095         Parameters
2096         ----------
2097         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2098             The input samples. Internally, it will be converted to
2099             ``dtype=np.float32`` and if a sparse matrix is provided
2100             to a sparse ``csr_matrix``.
2101 
2102         Raises
2103         ------
2104         AttributeError
2105             If the ``loss`` does not support probabilities.
2106 
2107         Returns
2108         -------
2109         p : array, shape (n_samples, n_classes)
2110             The class log-probabilities of the input samples. The order of the
2111             classes corresponds to that in the attribute `classes_`.
2112         """
2113         proba = self.predict_proba(X)
2114         return np.log(proba)
2115 
2116     def staged_predict_proba(self, X):
2117         """Predict class probabilities at each stage for X.
2118 
2119         This method allows monitoring (i.e. determine error on testing set)
2120         after each stage.
2121 
2122         Parameters
2123         ----------
2124         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2125             The input samples. Internally, it will be converted to
2126             ``dtype=np.float32`` and if a sparse matrix is provided
2127             to a sparse ``csr_matrix``.
2128 
2129         Returns
2130         -------
2131         y : generator of array of shape (n_samples,)
2132             The predicted value of the input samples.
2133         """
2134         try:
2135             for score in self._staged_decision_function(X):
2136                 yield self.loss_._score_to_proba(score)
2137         except NotFittedError:
2138             raise
2139         except AttributeError:
2140             raise AttributeError('loss=%r does not support predict_proba' %
2141                                  self.loss)
2142 
2143 
2144 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
2145     """Gradient Boosting for regression.
2146 
2147     GB builds an additive model in a forward stage-wise fashion;
2148     it allows for the optimization of arbitrary differentiable loss functions.
2149     In each stage a regression tree is fit on the negative gradient of the
2150     given loss function.
2151 
2152     Read more in the :ref:`User Guide <gradient_boosting>`.
2153 
2154     Parameters
2155     ----------
2156     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
2157         loss function to be optimized. 'ls' refers to least squares
2158         regression. 'lad' (least absolute deviation) is a highly robust
2159         loss function solely based on order information of the input
2160         variables. 'huber' is a combination of the two. 'quantile'
2161         allows quantile regression (use `alpha` to specify the quantile).
2162 
2163     learning_rate : float, optional (default=0.1)
2164         learning rate shrinks the contribution of each tree by `learning_rate`.
2165         There is a trade-off between learning_rate and n_estimators.
2166 
2167     n_estimators : int (default=100)
2168         The number of boosting stages to perform. Gradient boosting
2169         is fairly robust to over-fitting so a large number usually
2170         results in better performance.
2171 
2172     subsample : float, optional (default=1.0)
2173         The fraction of samples to be used for fitting the individual base
2174         learners. If smaller than 1.0 this results in Stochastic Gradient
2175         Boosting. `subsample` interacts with the parameter `n_estimators`.
2176         Choosing `subsample < 1.0` leads to a reduction of variance
2177         and an increase in bias.
2178 
2179     criterion : string, optional (default="friedman_mse")
2180         The function to measure the quality of a split. Supported criteria
2181         are "friedman_mse" for the mean squared error with improvement
2182         score by Friedman, "mse" for mean squared error, and "mae" for
2183         the mean absolute error. The default value of "friedman_mse" is
2184         generally the best as it can provide a better approximation in
2185         some cases.
2186 
2187         .. versionadded:: 0.18
2188 
2189     min_samples_split : int, float, optional (default=2)
2190         The minimum number of samples required to split an internal node:
2191 
2192         - If int, then consider `min_samples_split` as the minimum number.
2193         - If float, then `min_samples_split` is a fraction and
2194           `ceil(min_samples_split * n_samples)` are the minimum
2195           number of samples for each split.
2196 
2197         .. versionchanged:: 0.18
2198            Added float values for fractions.
2199 
2200     min_samples_leaf : int, float, optional (default=1)
2201         The minimum number of samples required to be at a leaf node.
2202         A split point at any depth will only be considered if it leaves at
2203         least ``min_samples_leaf`` training samples in each of the left and
2204         right branches.  This may have the effect of smoothing the model,
2205         especially in regression.
2206 
2207         - If int, then consider `min_samples_leaf` as the minimum number.
2208         - If float, then `min_samples_leaf` is a fraction and
2209           `ceil(min_samples_leaf * n_samples)` are the minimum
2210           number of samples for each node.
2211 
2212         .. versionchanged:: 0.18
2213            Added float values for fractions.
2214 
2215     min_weight_fraction_leaf : float, optional (default=0.)
2216         The minimum weighted fraction of the sum total of weights (of all
2217         the input samples) required to be at a leaf node. Samples have
2218         equal weight when sample_weight is not provided.
2219 
2220     max_depth : integer, optional (default=3)
2221         maximum depth of the individual regression estimators. The maximum
2222         depth limits the number of nodes in the tree. Tune this parameter
2223         for best performance; the best value depends on the interaction
2224         of the input variables.
2225 
2226     min_impurity_decrease : float, optional (default=0.)
2227         A node will be split if this split induces a decrease of the impurity
2228         greater than or equal to this value.
2229 
2230         The weighted impurity decrease equation is the following::
2231 
2232             N_t / N * (impurity - N_t_R / N_t * right_impurity
2233                                 - N_t_L / N_t * left_impurity)
2234 
2235         where ``N`` is the total number of samples, ``N_t`` is the number of
2236         samples at the current node, ``N_t_L`` is the number of samples in the
2237         left child, and ``N_t_R`` is the number of samples in the right child.
2238 
2239         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2240         if ``sample_weight`` is passed.
2241 
2242         .. versionadded:: 0.19
2243 
2244     min_impurity_split : float, (default=1e-7)
2245         Threshold for early stopping in tree growth. A node will split
2246         if its impurity is above the threshold, otherwise it is a leaf.
2247 
2248         .. deprecated:: 0.19
2249            ``min_impurity_split`` has been deprecated in favor of
2250            ``min_impurity_decrease`` in 0.19. The default value of
2251            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
2252            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
2253 
2254     init : estimator, optional (default=None)
2255         An estimator object that is used to compute the initial
2256         predictions. ``init`` has to provide ``fit`` and ``predict``.
2257         If None it uses ``loss.init_estimator``.
2258 
2259     random_state : int, RandomState instance or None, optional (default=None)
2260         If int, random_state is the seed used by the random number generator;
2261         If RandomState instance, random_state is the random number generator;
2262         If None, the random number generator is the RandomState instance used
2263         by `np.random`.
2264 
2265     max_features : int, float, string or None, optional (default=None)
2266         The number of features to consider when looking for the best split:
2267 
2268         - If int, then consider `max_features` features at each split.
2269         - If float, then `max_features` is a fraction and
2270           `int(max_features * n_features)` features are considered at each
2271           split.
2272         - If "auto", then `max_features=n_features`.
2273         - If "sqrt", then `max_features=sqrt(n_features)`.
2274         - If "log2", then `max_features=log2(n_features)`.
2275         - If None, then `max_features=n_features`.
2276 
2277         Choosing `max_features < n_features` leads to a reduction of variance
2278         and an increase in bias.
2279 
2280         Note: the search for a split does not stop until at least one
2281         valid partition of the node samples is found, even if it requires to
2282         effectively inspect more than ``max_features`` features.
2283 
2284     alpha : float (default=0.9)
2285         The alpha-quantile of the huber loss function and the quantile
2286         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
2287 
2288     verbose : int, default: 0
2289         Enable verbose output. If 1 then it prints progress and performance
2290         once in a while (the more trees the lower the frequency). If greater
2291         than 1 then it prints progress and performance for every tree.
2292 
2293     max_leaf_nodes : int or None, optional (default=None)
2294         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2295         Best nodes are defined as relative reduction in impurity.
2296         If None then unlimited number of leaf nodes.
2297 
2298     warm_start : bool, default: False
2299         When set to ``True``, reuse the solution of the previous call to fit
2300         and add more estimators to the ensemble, otherwise, just erase the
2301         previous solution. See :term:`the Glossary <warm_start>`.
2302 
2303     presort : bool or 'auto', optional (default='auto')
2304         Whether to presort the data to speed up the finding of best splits in
2305         fitting. Auto mode by default will use presorting on dense data and
2306         default to normal sorting on sparse data. Setting presort to true on
2307         sparse data will raise an error.
2308 
2309         .. versionadded:: 0.17
2310            optional parameter *presort*.
2311 
2312     validation_fraction : float, optional, default 0.1
2313         The proportion of training data to set aside as validation set for
2314         early stopping. Must be between 0 and 1.
2315         Only used if ``n_iter_no_change`` is set to an integer.
2316 
2317         .. versionadded:: 0.20
2318 
2319     n_iter_no_change : int, default None
2320         ``n_iter_no_change`` is used to decide if early stopping will be used
2321         to terminate training when validation score is not improving. By
2322         default it is set to None to disable early stopping. If set to a
2323         number, it will set aside ``validation_fraction`` size of the training
2324         data as validation and terminate training when validation score is not
2325         improving in all of the previous ``n_iter_no_change`` numbers of
2326         iterations.
2327 
2328         .. versionadded:: 0.20
2329 
2330     tol : float, optional, default 1e-4
2331         Tolerance for the early stopping. When the loss is not improving
2332         by at least tol for ``n_iter_no_change`` iterations (if set to a
2333         number), the training stops.
2334 
2335         .. versionadded:: 0.20
2336 
2337 
2338     Attributes
2339     ----------
2340     feature_importances_ : array, shape (n_features,)
2341         The feature importances (the higher, the more important the feature).
2342 
2343     oob_improvement_ : array, shape (n_estimators,)
2344         The improvement in loss (= deviance) on the out-of-bag samples
2345         relative to the previous iteration.
2346         ``oob_improvement_[0]`` is the improvement in
2347         loss of the first stage over the ``init`` estimator.
2348 
2349     train_score_ : array, shape (n_estimators,)
2350         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
2351         model at iteration ``i`` on the in-bag sample.
2352         If ``subsample == 1`` this is the deviance on the training data.
2353 
2354     loss_ : LossFunction
2355         The concrete ``LossFunction`` object.
2356 
2357     init_ : estimator
2358         The estimator that provides the initial predictions.
2359         Set via the ``init`` argument or ``loss.init_estimator``.
2360 
2361     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
2362         The collection of fitted sub-estimators.
2363 
2364     Notes
2365     -----
2366     The features are always randomly permuted at each split. Therefore,
2367     the best found split may vary, even with the same training data and
2368     ``max_features=n_features``, if the improvement of the criterion is
2369     identical for several splits enumerated during the search of the best
2370     split. To obtain a deterministic behaviour during fitting,
2371     ``random_state`` has to be fixed.
2372 
2373     See also
2374     --------
2375     DecisionTreeRegressor, RandomForestRegressor
2376 
2377     References
2378     ----------
2379     J. Friedman, Greedy Function Approximation: A Gradient Boosting
2380     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
2381 
2382     J. Friedman, Stochastic Gradient Boosting, 1999
2383 
2384     T. Hastie, R. Tibshirani and J. Friedman.
2385     Elements of Statistical Learning Ed. 2, Springer, 2009.
2386     """
2387 
2388     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')
2389 
2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
2393                  max_depth=3, min_impurity_decrease=0.,
2394                  min_impurity_split=None, init=None, random_state=None,
2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
2396                  warm_start=False, presort='auto', validation_fraction=0.1,
2397                  n_iter_no_change=None, tol=1e-4):
2398 
2399         super().__init__(
2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
2401             criterion=criterion, min_samples_split=min_samples_split,
2402             min_samples_leaf=min_samples_leaf,
2403             min_weight_fraction_leaf=min_weight_fraction_leaf,
2404             max_depth=max_depth, init=init, subsample=subsample,
2405             max_features=max_features,
2406             min_impurity_decrease=min_impurity_decrease,
2407             min_impurity_split=min_impurity_split,
2408             random_state=random_state, alpha=alpha, verbose=verbose,
2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
2410             presort=presort, validation_fraction=validation_fraction,
2411             n_iter_no_change=n_iter_no_change, tol=tol)
2412 
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()
2430 
2431     def staged_predict(self, X):
2432         """Predict regression target at each stage for X.
2433 
2434         This method allows monitoring (i.e. determine error on testing set)
2435         after each stage.
2436 
2437         Parameters
2438         ----------
2439         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2440             The input samples. Internally, it will be converted to
2441             ``dtype=np.float32`` and if a sparse matrix is provided
2442             to a sparse ``csr_matrix``.
2443 
2444         Returns
2445         -------
2446         y : generator of array of shape (n_samples,)
2447             The predicted value of the input samples.
2448         """
2449         for y in self._staged_decision_function(X):
2450             yield y.ravel()
2451 
2452     def apply(self, X):
2453         """Apply trees in the ensemble to X, return leaf indices.
2454 
2455         .. versionadded:: 0.17
2456 
2457         Parameters
2458         ----------
2459         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2460             The input samples. Internally, its dtype will be converted to
2461             ``dtype=np.float32``. If a sparse matrix is provided, it will
2462             be converted to a sparse ``csr_matrix``.
2463 
2464         Returns
2465         -------
2466         X_leaves : array-like, shape (n_samples, n_estimators)
2467             For each datapoint x in X and for each tree in the ensemble,
2468             return the index of the leaf x ends up in each estimator.
2469         """
2470 
2471         leaves = super().apply(X)
2472         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])
2473         return leaves

</code><intended_behavior>If the issue involves the handling of sample weights with initial estimators that do not support them, the implementation needs to ensure that these weights are only applied where applicable or provide a fallback mechanism. This could mean introducing a check or compatibility layer within the gradient boosting initialization process to verify if the `init` estimator supports sample weights and handle cases where it doesn't, possibly by ignoring the weights or applying a default weighting mechanism that ensures the initial model fitting process proceeds without errors.</intended_behavior>]
2025-10-18 16:58:48.064 | INFO     | Search completed. Bug locations: [<file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>predict</method>
<code>
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>QuantileEstimator</class>
<method>predict</method>
<code>
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>MeanEstimator</class>
<method>predict</method>
<code>
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>LogOddsEstimator</class>
<method>predict</method>
<code>
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>PriorProbabilityEstimator</class>
<method>predict</method>
<code>
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>ZeroEstimator</class>
<method>predict</method>
<code>
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>GradientBoostingClassifier</class>
<method>predict</method>
<code>
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>GradientBoostingRegressor</class>
<method>predict</method>
<code>
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()

</code><intended_behavior>The predict method should correctly handle the output shape from the `init` estimator, ensuring it matches the expected format for proceeding prediction steps in the gradient boosting process. This might involve adding a reshape or validation step specifically to accommodate `init` estimators that are scikit-learn models but have differing output formats.</intended_behavior>, <file>sklearn/ensemble/gradient_boosting.py</file>
<class>None</class>
<method>None</method>
<code>
1 """Gradient Boosted Regression Trees
2 
3 This module contains methods for fitting gradient boosted regression trees for
4 both classification and regression.
5 
6 The module structure is the following:
7 
8 - The ``BaseGradientBoosting`` base class implements a common ``fit`` method
9   for all the estimators in the module. Regression and classification
10   only differ in the concrete ``LossFunction`` used.
11 
12 - ``GradientBoostingClassifier`` implements gradient boosting for
13   classification problems.
14 
15 - ``GradientBoostingRegressor`` implements gradient boosting for
16   regression problems.
17 """
18 
19 # Authors: Peter Prettenhofer, Scott White, Gilles Louppe, Emanuele Olivetti,
20 #          Arnaud Joly, Jacob Schreiber
21 # License: BSD 3 clause
22 
23 from abc import ABCMeta
24 from abc import abstractmethod
25 
26 from .base import BaseEnsemble
27 from ..base import ClassifierMixin
28 from ..base import RegressorMixin
29 
30 from ._gradient_boosting import predict_stages
31 from ._gradient_boosting import predict_stage
32 from ._gradient_boosting import _random_sample_mask
33 
34 import numbers
35 import numpy as np
36 
37 from scipy.sparse import csc_matrix
38 from scipy.sparse import csr_matrix
39 from scipy.sparse import issparse
40 from scipy.special import expit
41 
42 from time import time
43 from ..model_selection import train_test_split
44 from ..tree.tree import DecisionTreeRegressor
45 from ..tree._tree import DTYPE
46 from ..tree._tree import TREE_LEAF
47 
48 from ..utils import check_random_state
49 from ..utils import check_array
50 from ..utils import check_X_y
51 from ..utils import column_or_1d
52 from ..utils import check_consistent_length
53 from ..utils import deprecated
54 from ..utils.fixes import logsumexp
55 from ..utils.stats import _weighted_percentile
56 from ..utils.validation import check_is_fitted
57 from ..utils.multiclass import check_classification_targets
58 from ..exceptions import NotFittedError
59 
60 
61 class QuantileEstimator:
62     """An estimator predicting the alpha-quantile of the training targets.
63 
64     Parameters
65     ----------
66     alpha : float
67         The quantile
68     """
69     def __init__(self, alpha=0.9):
70         if not 0 < alpha < 1.0:
71             raise ValueError("`alpha` must be in (0, 1.0) but was %r" % alpha)
72         self.alpha = alpha
73 
74     def fit(self, X, y, sample_weight=None):
75         """Fit the estimator.
76 
77         Parameters
78         ----------
79         X : {array-like, sparse matrix}, shape (n_samples, n_features)
80             Training data
81 
82         y : array, shape (n_samples, n_targets)
83             Target values. Will be cast to X's dtype if necessary
84 
85         sample_weight : numpy array of shape (n_samples,)
86             Individual weights for each sample
87         """
88         if sample_weight is None:
89             self.quantile = np.percentile(y, self.alpha * 100.0)
90         else:
91             self.quantile = _weighted_percentile(y, sample_weight,
92                                                  self.alpha * 100.0)
93 
94     def predict(self, X):
95         """Predict labels
96 
97         Parameters
98         ----------
99         X : {array-like, sparse matrix}, shape (n_samples, n_features)
100             Samples.
101 
102         Returns
103         -------
104         y : array, shape (n_samples,)
105             Returns predicted values.
106         """
107         check_is_fitted(self, 'quantile')
108 
109         y = np.empty((X.shape[0], 1), dtype=np.float64)
110         y.fill(self.quantile)
111         return y
112 
113 
114 class MeanEstimator:
115     """An estimator predicting the mean of the training targets."""
116     def fit(self, X, y, sample_weight=None):
117         """Fit the estimator.
118 
119         Parameters
120         ----------
121         X : {array-like, sparse matrix}, shape (n_samples, n_features)
122             Training data
123 
124         y : array, shape (n_samples, n_targets)
125             Target values. Will be cast to X's dtype if necessary
126 
127         sample_weight : numpy array of shape (n_samples,)
128             Individual weights for each sample
129         """
130         if sample_weight is None:
131             self.mean = np.mean(y)
132         else:
133             self.mean = np.average(y, weights=sample_weight)
134 
135     def predict(self, X):
136         """Predict labels
137 
138         Parameters
139         ----------
140         X : {array-like, sparse matrix}, shape (n_samples, n_features)
141             Samples.
142 
143         Returns
144         -------
145         y : array, shape (n_samples,)
146             Returns predicted values.
147         """
148         check_is_fitted(self, 'mean')
149 
150         y = np.empty((X.shape[0], 1), dtype=np.float64)
151         y.fill(self.mean)
152         return y
153 
154 
155 class LogOddsEstimator:
156     """An estimator predicting the log odds ratio."""
157     scale = 1.0
158 
159     def fit(self, X, y, sample_weight=None):
160         """Fit the estimator.
161 
162         Parameters
163         ----------
164         X : {array-like, sparse matrix}, shape (n_samples, n_features)
165             Training data
166 
167         y : array, shape (n_samples, n_targets)
168             Target values. Will be cast to X's dtype if necessary
169 
170         sample_weight : numpy array of shape (n_samples,)
171             Individual weights for each sample
172         """
173         # pre-cond: pos, neg are encoded as 1, 0
174         if sample_weight is None:
175             pos = np.sum(y)
176             neg = y.shape[0] - pos
177         else:
178             pos = np.sum(sample_weight * y)
179             neg = np.sum(sample_weight * (1 - y))
180 
181         if neg == 0 or pos == 0:
182             raise ValueError('y contains non binary labels.')
183         self.prior = self.scale * np.log(pos / neg)
184 
185     def predict(self, X):
186         """Predict labels
187 
188         Parameters
189         ----------
190         X : {array-like, sparse matrix}, shape (n_samples, n_features)
191             Samples.
192 
193         Returns
194         -------
195         y : array, shape (n_samples,)
196             Returns predicted values.
197         """
198         check_is_fitted(self, 'prior')
199 
200         y = np.empty((X.shape[0], 1), dtype=np.float64)
201         y.fill(self.prior)
202         return y
203 
204 
205 class ScaledLogOddsEstimator(LogOddsEstimator):
206     """Log odds ratio scaled by 0.5 -- for exponential loss. """
207     scale = 0.5
208 
209 
210 class PriorProbabilityEstimator:
211     """An estimator predicting the probability of each
212     class in the training data.
213     """
214     def fit(self, X, y, sample_weight=None):
215         """Fit the estimator.
216 
217         Parameters
218         ----------
219         X : {array-like, sparse matrix}, shape (n_samples, n_features)
220             Training data
221 
222         y : array, shape (n_samples, n_targets)
223             Target values. Will be cast to X's dtype if necessary
224 
225         sample_weight : array, shape (n_samples,)
226             Individual weights for each sample
227         """
228         if sample_weight is None:
229             sample_weight = np.ones_like(y, dtype=np.float64)
230         class_counts = np.bincount(y, weights=sample_weight)
231         self.priors = class_counts / class_counts.sum()
232 
233     def predict(self, X):
234         """Predict labels
235 
236         Parameters
237         ----------
238         X : {array-like, sparse matrix}, shape (n_samples, n_features)
239             Samples.
240 
241         Returns
242         -------
243         y : array, shape (n_samples,)
244             Returns predicted values.
245         """
246         check_is_fitted(self, 'priors')
247 
248         y = np.empty((X.shape[0], self.priors.shape[0]), dtype=np.float64)
249         y[:] = self.priors
250         return y
251 
252 
253 class ZeroEstimator:
254     """An estimator that simply predicts zero. """
255 
256     def fit(self, X, y, sample_weight=None):
257         """Fit the estimator.
258 
259         Parameters
260         ----------
261         X : {array-like, sparse matrix}, shape (n_samples, n_features)
262             Training data
263 
264         y : numpy, shape (n_samples, n_targets)
265             Target values. Will be cast to X's dtype if necessary
266 
267         sample_weight : array, shape (n_samples,)
268             Individual weights for each sample
269         """
270         if np.issubdtype(y.dtype, np.signedinteger):
271             # classification
272             self.n_classes = np.unique(y).shape[0]
273             if self.n_classes == 2:
274                 self.n_classes = 1
275         else:
276             # regression
277             self.n_classes = 1
278 
279     def predict(self, X):
280         """Predict labels
281 
282         Parameters
283         ----------
284         X : {array-like, sparse matrix}, shape (n_samples, n_features)
285             Samples.
286 
287         Returns
288         -------
289         y : array, shape (n_samples,)
290             Returns predicted values.
291         """
292         check_is_fitted(self, 'n_classes')
293 
294         y = np.empty((X.shape[0], self.n_classes), dtype=np.float64)
295         y.fill(0.0)
296         return y
297 
298 
299 class LossFunction(metaclass=ABCMeta):
300     """Abstract base class for various loss functions.
301 
302     Parameters
303     ----------
304     n_classes : int
305         Number of classes
306 
307     Attributes
308     ----------
309     K : int
310         The number of regression trees to be induced;
311         1 for regression and binary classification;
312         ``n_classes`` for multi-class classification.
313     """
314 
315     is_multi_class = False
316 
317     def __init__(self, n_classes):
318         self.K = n_classes
319 
320     def init_estimator(self):
321         """Default ``init`` estimator for loss function. """
322         raise NotImplementedError()
323 
324     @abstractmethod
325     def __call__(self, y, pred, sample_weight=None):
326         """Compute the loss.
327 
328         Parameters
329         ----------
330         y : array, shape (n_samples,)
331             True labels
332 
333         pred : array, shape (n_samples,)
334             Predicted labels
335 
336         sample_weight : array-like, shape (n_samples,), optional
337             Sample weights.
338         """
339 
340     @abstractmethod
341     def negative_gradient(self, y, y_pred, **kargs):
342         """Compute the negative gradient.
343 
344         Parameters
345         ----------
346         y : array, shape (n_samples,)
347             The target labels.
348 
349         y_pred : array, shape (n_samples,)
350             The predictions.
351         """
352 
353     def update_terminal_regions(self, tree, X, y, residual, y_pred,
354                                 sample_weight, sample_mask,
355                                 learning_rate=0.1, k=0):
356         """Update the terminal regions (=leaves) of the given tree and
357         updates the current predictions of the model. Traverses tree
358         and invokes template method `_update_terminal_region`.
359 
360         Parameters
361         ----------
362         tree : tree.Tree
363             The tree object.
364         X : array, shape (n, m)
365             The data array.
366         y : array, shape (n,)
367             The target labels.
368         residual : array, shape (n,)
369             The residuals (usually the negative gradient).
370         y_pred : array, shape (n,)
371             The predictions.
372         sample_weight : array, shape (n,)
373             The weight of each sample.
374         sample_mask : array, shape (n,)
375             The sample mask to be used.
376         learning_rate : float, default=0.1
377             learning rate shrinks the contribution of each tree by
378              ``learning_rate``.
379         k : int, default 0
380             The index of the estimator being updated.
381 
382         """
383         # compute leaf for each sample in ``X``.
384         terminal_regions = tree.apply(X)
385 
386         # mask all which are not in sample mask.
387         masked_terminal_regions = terminal_regions.copy()
388         masked_terminal_regions[~sample_mask] = -1
389 
390         # update each leaf (= perform line search)
391         for leaf in np.where(tree.children_left == TREE_LEAF)[0]:
392             self._update_terminal_region(tree, masked_terminal_regions,
393                                          leaf, X, y, residual,
394                                          y_pred[:, k], sample_weight)
395 
396         # update predictions (both in-bag and out-of-bag)
397         y_pred[:, k] += (learning_rate
398                          * tree.value[:, 0, 0].take(terminal_regions, axis=0))
399 
400     @abstractmethod
401     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
402                                 residual, pred, sample_weight):
403         """Template method for updating terminal regions (=leaves). """
404 
405 
406 class RegressionLossFunction(LossFunction, metaclass=ABCMeta):
407     """Base class for regression loss functions.
408 
409     Parameters
410     ----------
411     n_classes : int
412         Number of classes
413     """
414     def __init__(self, n_classes):
415         if n_classes != 1:
416             raise ValueError("``n_classes`` must be 1 for regression but "
417                              "was %r" % n_classes)
418         super().__init__(n_classes)
419 
420 
421 class LeastSquaresError(RegressionLossFunction):
422     """Loss function for least squares (LS) estimation.
423     Terminal regions need not to be updated for least squares.
424 
425     Parameters
426     ----------
427     n_classes : int
428         Number of classes
429     """
430 
431     def init_estimator(self):
432         return MeanEstimator()
433 
434     def __call__(self, y, pred, sample_weight=None):
435         """Compute the least squares loss.
436 
437         Parameters
438         ----------
439         y : array, shape (n_samples,)
440             True labels
441 
442         pred : array, shape (n_samples,)
443             Predicted labels
444 
445         sample_weight : array-like, shape (n_samples,), optional
446             Sample weights.
447         """
448         if sample_weight is None:
449             return np.mean((y - pred.ravel()) ** 2.0)
450         else:
451             return (1.0 / sample_weight.sum() *
452                     np.sum(sample_weight * ((y - pred.ravel()) ** 2.0)))
453 
454     def negative_gradient(self, y, pred, **kargs):
455         """Compute the negative gradient.
456 
457         Parameters
458         ----------
459         y : array, shape (n_samples,)
460             The target labels.
461 
462         pred : array, shape (n_samples,)
463             The predictions.
464         """
465         return y - pred.ravel()
466 
467     def update_terminal_regions(self, tree, X, y, residual, y_pred,
468                                 sample_weight, sample_mask,
469                                 learning_rate=0.1, k=0):
470         """Least squares does not need to update terminal regions.
471 
472         But it has to update the predictions.
473 
474         Parameters
475         ----------
476         tree : tree.Tree
477             The tree object.
478         X : array, shape (n, m)
479             The data array.
480         y : array, shape (n,)
481             The target labels.
482         residual : array, shape (n,)
483             The residuals (usually the negative gradient).
484         y_pred : array, shape (n,)
485             The predictions.
486         sample_weight : array, shape (n,)
487             The weight of each sample.
488         sample_mask : array, shape (n,)
489             The sample mask to be used.
490         learning_rate : float, default=0.1
491             learning rate shrinks the contribution of each tree by
492              ``learning_rate``.
493         k : int, default 0
494             The index of the estimator being updated.
495         """
496         # update predictions
497         y_pred[:, k] += learning_rate * tree.predict(X).ravel()
498 
499     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
500                                 residual, pred, sample_weight):
501         pass
502 
503 
504 class LeastAbsoluteError(RegressionLossFunction):
505     """Loss function for least absolute deviation (LAD) regression.
506 
507     Parameters
508     ----------
509     n_classes : int
510         Number of classes
511     """
512     def init_estimator(self):
513         return QuantileEstimator(alpha=0.5)
514 
515     def __call__(self, y, pred, sample_weight=None):
516         """Compute the least absolute error.
517 
518         Parameters
519         ----------
520         y : array, shape (n_samples,)
521             True labels
522 
523         pred : array, shape (n_samples,)
524             Predicted labels
525 
526         sample_weight : array-like, shape (n_samples,), optional
527             Sample weights.
528         """
529         if sample_weight is None:
530             return np.abs(y - pred.ravel()).mean()
531         else:
532             return (1.0 / sample_weight.sum() *
533                     np.sum(sample_weight * np.abs(y - pred.ravel())))
534 
535     def negative_gradient(self, y, pred, **kargs):
536         """Compute the negative gradient.
537 
538         1.0 if y - pred > 0.0 else -1.0
539 
540         Parameters
541         ----------
542         y : array, shape (n_samples,)
543             The target labels.
544 
545         pred : array, shape (n_samples,)
546             The predictions.
547         """
548         pred = pred.ravel()
549         return 2.0 * (y - pred > 0.0) - 1.0
550 
551     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
552                                 residual, pred, sample_weight):
553         """LAD updates terminal regions to median estimates. """
554         terminal_region = np.where(terminal_regions == leaf)[0]
555         sample_weight = sample_weight.take(terminal_region, axis=0)
556         diff = y.take(terminal_region, axis=0) - pred.take(terminal_region, axis=0)
557         tree.value[leaf, 0, 0] = _weighted_percentile(diff, sample_weight, percentile=50)
558 
559 
560 class HuberLossFunction(RegressionLossFunction):
561     """Huber loss function for robust regression.
562 
563     M-Regression proposed in Friedman 2001.
564 
565     References
566     ----------
567     J. Friedman, Greedy Function Approximation: A Gradient Boosting
568     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
569 
570     Parameters
571     ----------
572     n_classes : int
573         Number of classes
574 
575     alpha : float
576         Percentile at which to extract score
577     """
578 
579     def __init__(self, n_classes, alpha=0.9):
580         super().__init__(n_classes)
581         self.alpha = alpha
582         self.gamma = None
583 
584     def init_estimator(self):
585         return QuantileEstimator(alpha=0.5)
586 
587     def __call__(self, y, pred, sample_weight=None):
588         """Compute the Huber loss.
589 
590         Parameters
591         ----------
592         y : array, shape (n_samples,)
593             True labels
594 
595         pred : array, shape (n_samples,)
596             Predicted labels
597 
598         sample_weight : array-like, shape (n_samples,), optional
599             Sample weights.
600         """
601         pred = pred.ravel()
602         diff = y - pred
603         gamma = self.gamma
604         if gamma is None:
605             if sample_weight is None:
606                 gamma = np.percentile(np.abs(diff), self.alpha * 100)
607             else:
608                 gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
609 
610         gamma_mask = np.abs(diff) <= gamma
611         if sample_weight is None:
612             sq_loss = np.sum(0.5 * diff[gamma_mask] ** 2.0)
613             lin_loss = np.sum(gamma * (np.abs(diff[~gamma_mask]) - gamma / 2.0))
614             loss = (sq_loss + lin_loss) / y.shape[0]
615         else:
616             sq_loss = np.sum(0.5 * sample_weight[gamma_mask] * diff[gamma_mask] ** 2.0)
617             lin_loss = np.sum(gamma * sample_weight[~gamma_mask] *
618                               (np.abs(diff[~gamma_mask]) - gamma / 2.0))
619             loss = (sq_loss + lin_loss) / sample_weight.sum()
620         return loss
621 
622     def negative_gradient(self, y, pred, sample_weight=None, **kargs):
623         """Compute the negative gradient.
624 
625         Parameters
626         ----------
627         y : array, shape (n_samples,)
628             The target labels.
629 
630         pred : array, shape (n_samples,)
631             The predictions.
632 
633         sample_weight : array-like, shape (n_samples,), optional
634             Sample weights.
635         """
636         pred = pred.ravel()
637         diff = y - pred
638         if sample_weight is None:
639             gamma = np.percentile(np.abs(diff), self.alpha * 100)
640         else:
641             gamma = _weighted_percentile(np.abs(diff), sample_weight, self.alpha * 100)
642         gamma_mask = np.abs(diff) <= gamma
643         residual = np.zeros((y.shape[0],), dtype=np.float64)
644         residual[gamma_mask] = diff[gamma_mask]
645         residual[~gamma_mask] = gamma * np.sign(diff[~gamma_mask])
646         self.gamma = gamma
647         return residual
648 
649     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
650                                 residual, pred, sample_weight):
651         terminal_region = np.where(terminal_regions == leaf)[0]
652         sample_weight = sample_weight.take(terminal_region, axis=0)
653         gamma = self.gamma
654         diff = (y.take(terminal_region, axis=0)
655                 - pred.take(terminal_region, axis=0))
656         median = _weighted_percentile(diff, sample_weight, percentile=50)
657         diff_minus_median = diff - median
658         tree.value[leaf, 0] = median + np.mean(
659             np.sign(diff_minus_median) *
660             np.minimum(np.abs(diff_minus_median), gamma))
661 
662 
663 class QuantileLossFunction(RegressionLossFunction):
664     """Loss function for quantile regression.
665 
666     Quantile regression allows to estimate the percentiles
667     of the conditional distribution of the target.
668 
669     Parameters
670     ----------
671     n_classes : int
672         Number of classes.
673 
674     alpha : float, optional (default = 0.9)
675         The percentile
676     """
677     def __init__(self, n_classes, alpha=0.9):
678         super().__init__(n_classes)
679         self.alpha = alpha
680         self.percentile = alpha * 100.0
681 
682     def init_estimator(self):
683         return QuantileEstimator(self.alpha)
684 
685     def __call__(self, y, pred, sample_weight=None):
686         """Compute the Quantile loss.
687 
688         Parameters
689         ----------
690         y : array, shape (n_samples,)
691             True labels
692 
693         pred : array, shape (n_samples,)
694             Predicted labels
695 
696         sample_weight : array-like, shape (n_samples,), optional
697             Sample weights.
698         """
699         pred = pred.ravel()
700         diff = y - pred
701         alpha = self.alpha
702 
703         mask = y > pred
704         if sample_weight is None:
705             loss = (alpha * diff[mask].sum() -
706                     (1.0 - alpha) * diff[~mask].sum()) / y.shape[0]
707         else:
708             loss = ((alpha * np.sum(sample_weight[mask] * diff[mask]) -
709                     (1.0 - alpha) * np.sum(sample_weight[~mask] * diff[~mask])) /
710                     sample_weight.sum())
711         return loss
712 
713     def negative_gradient(self, y, pred, **kargs):
714         """Compute the negative gradient.
715 
716         Parameters
717         ----------
718         y : array, shape (n_samples,)
719             The target labels.
720 
721         pred : array, shape (n_samples,)
722             The predictions.
723         """
724         alpha = self.alpha
725         pred = pred.ravel()
726         mask = y > pred
727         return (alpha * mask) - ((1.0 - alpha) * ~mask)
728 
729     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
730                                 residual, pred, sample_weight):
731         terminal_region = np.where(terminal_regions == leaf)[0]
732         diff = (y.take(terminal_region, axis=0)
733                 - pred.take(terminal_region, axis=0))
734         sample_weight = sample_weight.take(terminal_region, axis=0)
735 
736         val = _weighted_percentile(diff, sample_weight, self.percentile)
737         tree.value[leaf, 0] = val
738 
739 
740 class ClassificationLossFunction(LossFunction, metaclass=ABCMeta):
741     """Base class for classification loss functions. """
742 
743     def _score_to_proba(self, score):
744         """Template method to convert scores to probabilities.
745 
746          the does not support probabilities raises AttributeError.
747         """
748         raise TypeError('%s does not support predict_proba' % type(self).__name__)
749 
750     @abstractmethod
751     def _score_to_decision(self, score):
752         """Template method to convert scores to decisions.
753 
754         Returns int arrays.
755         """
756 
757 
758 class BinomialDeviance(ClassificationLossFunction):
759     """Binomial deviance loss function for binary classification.
760 
761     Binary classification is a special case; here, we only need to
762     fit one tree instead of ``n_classes`` trees.
763 
764     Parameters
765     ----------
766     n_classes : int
767         Number of classes.
768     """
769     def __init__(self, n_classes):
770         if n_classes != 2:
771             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
772                              .format(self.__class__.__name__, n_classes))
773         # we only need to fit one tree for binary clf.
774         super().__init__(1)
775 
776     def init_estimator(self):
777         return LogOddsEstimator()
778 
779     def __call__(self, y, pred, sample_weight=None):
780         """Compute the deviance (= 2 * negative log-likelihood).
781 
782         Parameters
783         ----------
784         y : array, shape (n_samples,)
785             True labels
786 
787         pred : array, shape (n_samples,)
788             Predicted labels
789 
790         sample_weight : array-like, shape (n_samples,), optional
791             Sample weights.
792         """
793         # logaddexp(0, v) == log(1.0 + exp(v))
794         pred = pred.ravel()
795         if sample_weight is None:
796             return -2.0 * np.mean((y * pred) - np.logaddexp(0.0, pred))
797         else:
798             return (-2.0 / sample_weight.sum() *
799                     np.sum(sample_weight * ((y * pred) - np.logaddexp(0.0, pred))))
800 
801     def negative_gradient(self, y, pred, **kargs):
802         """Compute the residual (= negative gradient).
803 
804         Parameters
805         ----------
806         y : array, shape (n_samples,)
807             True labels
808 
809         pred : array, shape (n_samples,)
810             Predicted labels
811         """
812         return y - expit(pred.ravel())
813 
814     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
815                                 residual, pred, sample_weight):
816         """Make a single Newton-Raphson step.
817 
818         our node estimate is given by:
819 
820             sum(w * (y - prob)) / sum(w * prob * (1 - prob))
821 
822         we take advantage that: y - prob = residual
823         """
824         terminal_region = np.where(terminal_regions == leaf)[0]
825         residual = residual.take(terminal_region, axis=0)
826         y = y.take(terminal_region, axis=0)
827         sample_weight = sample_weight.take(terminal_region, axis=0)
828 
829         numerator = np.sum(sample_weight * residual)
830         denominator = np.sum(sample_weight * (y - residual) * (1 - y + residual))
831 
832         # prevents overflow and division by zero
833         if abs(denominator) < 1e-150:
834             tree.value[leaf, 0, 0] = 0.0
835         else:
836             tree.value[leaf, 0, 0] = numerator / denominator
837 
838     def _score_to_proba(self, score):
839         proba = np.ones((score.shape[0], 2), dtype=np.float64)
840         proba[:, 1] = expit(score.ravel())
841         proba[:, 0] -= proba[:, 1]
842         return proba
843 
844     def _score_to_decision(self, score):
845         proba = self._score_to_proba(score)
846         return np.argmax(proba, axis=1)
847 
848 
849 class MultinomialDeviance(ClassificationLossFunction):
850     """Multinomial deviance loss function for multi-class classification.
851 
852     For multi-class classification we need to fit ``n_classes`` trees at
853     each stage.
854 
855     Parameters
856     ----------
857     n_classes : int
858         Number of classes
859     """
860 
861     is_multi_class = True
862 
863     def __init__(self, n_classes):
864         if n_classes < 3:
865             raise ValueError("{0:s} requires more than 2 classes.".format(
866                 self.__class__.__name__))
867         super().__init__(n_classes)
868 
869     def init_estimator(self):
870         return PriorProbabilityEstimator()
871 
872     def __call__(self, y, pred, sample_weight=None):
873         """Compute the Multinomial deviance.
874 
875         Parameters
876         ----------
877         y : array, shape (n_samples,)
878             True labels
879 
880         pred : array, shape (n_samples,)
881             Predicted labels
882 
883         sample_weight : array-like, shape (n_samples,), optional
884             Sample weights.
885         """
886         # create one-hot label encoding
887         Y = np.zeros((y.shape[0], self.K), dtype=np.float64)
888         for k in range(self.K):
889             Y[:, k] = y == k
890 
891         if sample_weight is None:
892             return np.sum(-1 * (Y * pred).sum(axis=1) +
893                           logsumexp(pred, axis=1))
894         else:
895             return np.sum(-1 * sample_weight * (Y * pred).sum(axis=1) +
896                           logsumexp(pred, axis=1))
897 
898     def negative_gradient(self, y, pred, k=0, **kwargs):
899         """Compute negative gradient for the ``k``-th class.
900 
901         Parameters
902         ----------
903         y : array, shape (n_samples,)
904             The target labels.
905 
906         pred : array, shape (n_samples,)
907             The predictions.
908 
909         k : int, optional (default=0)
910             The index of the class
911         """
912         return y - np.nan_to_num(np.exp(pred[:, k] -
913                                         logsumexp(pred, axis=1)))
914 
915     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
916                                 residual, pred, sample_weight):
917         """Make a single Newton-Raphson step. """
918         terminal_region = np.where(terminal_regions == leaf)[0]
919         residual = residual.take(terminal_region, axis=0)
920         y = y.take(terminal_region, axis=0)
921         sample_weight = sample_weight.take(terminal_region, axis=0)
922 
923         numerator = np.sum(sample_weight * residual)
924         numerator *= (self.K - 1) / self.K
925 
926         denominator = np.sum(sample_weight * (y - residual) *
927                              (1.0 - y + residual))
928 
929         # prevents overflow and division by zero
930         if abs(denominator) < 1e-150:
931             tree.value[leaf, 0, 0] = 0.0
932         else:
933             tree.value[leaf, 0, 0] = numerator / denominator
934 
935     def _score_to_proba(self, score):
936         return np.nan_to_num(
937             np.exp(score - (logsumexp(score, axis=1)[:, np.newaxis])))
938 
939     def _score_to_decision(self, score):
940         proba = self._score_to_proba(score)
941         return np.argmax(proba, axis=1)
942 
943 
944 class ExponentialLoss(ClassificationLossFunction):
945     """Exponential loss function for binary classification.
946 
947     Same loss as AdaBoost.
948 
949     References
950     ----------
951     Greg Ridgeway, Generalized Boosted Models: A guide to the gbm package, 2007
952 
953     Parameters
954     ----------
955     n_classes : int
956         Number of classes.
957     """
958     def __init__(self, n_classes):
959         if n_classes != 2:
960             raise ValueError("{0:s} requires 2 classes; got {1:d} class(es)"
961                              .format(self.__class__.__name__, n_classes))
962         # we only need to fit one tree for binary clf.
963         super().__init__(1)
964 
965     def init_estimator(self):
966         return ScaledLogOddsEstimator()
967 
968     def __call__(self, y, pred, sample_weight=None):
969         """Compute the exponential loss
970 
971         Parameters
972         ----------
973         y : array, shape (n_samples,)
974             True labels
975 
976         pred : array, shape (n_samples,)
977             Predicted labels
978 
979         sample_weight : array-like, shape (n_samples,), optional
980             Sample weights.
981         """
982         pred = pred.ravel()
983         if sample_weight is None:
984             return np.mean(np.exp(-(2. * y - 1.) * pred))
985         else:
986             return (1.0 / sample_weight.sum() *
987                     np.sum(sample_weight * np.exp(-(2 * y - 1) * pred)))
988 
989     def negative_gradient(self, y, pred, **kargs):
990         """Compute the residual (= negative gradient).
991 
992         Parameters
993         ----------
994         y : array, shape (n_samples,)
995             True labels
996 
997         pred : array, shape (n_samples,)
998             Predicted labels
999         """
1000         y_ = -(2. * y - 1.)
1001         return y_ * np.exp(y_ * pred.ravel())
1002 
1003     def _update_terminal_region(self, tree, terminal_regions, leaf, X, y,
1004                                 residual, pred, sample_weight):
1005         terminal_region = np.where(terminal_regions == leaf)[0]
1006         pred = pred.take(terminal_region, axis=0)
1007         y = y.take(terminal_region, axis=0)
1008         sample_weight = sample_weight.take(terminal_region, axis=0)
1009 
1010         y_ = 2. * y - 1.
1011 
1012         numerator = np.sum(y_ * sample_weight * np.exp(-y_ * pred))
1013         denominator = np.sum(sample_weight * np.exp(-y_ * pred))
1014 
1015         # prevents overflow and division by zero
1016         if abs(denominator) < 1e-150:
1017             tree.value[leaf, 0, 0] = 0.0
1018         else:
1019             tree.value[leaf, 0, 0] = numerator / denominator
1020 
1021     def _score_to_proba(self, score):
1022         proba = np.ones((score.shape[0], 2), dtype=np.float64)
1023         proba[:, 1] = expit(2.0 * score.ravel())
1024         proba[:, 0] -= proba[:, 1]
1025         return proba
1026 
1027     def _score_to_decision(self, score):
1028         return (score.ravel() >= 0.0).astype(np.int)
1029 
1030 
1031 LOSS_FUNCTIONS = {'ls': LeastSquaresError,
1032                   'lad': LeastAbsoluteError,
1033                   'huber': HuberLossFunction,
1034                   'quantile': QuantileLossFunction,
1035                   'deviance': None,    # for both, multinomial and binomial
1036                   'exponential': ExponentialLoss,
1037                   }
1038 
1039 
1040 INIT_ESTIMATORS = {'zero': ZeroEstimator}
1041 
1042 
1043 class VerboseReporter:
1044     """Reports verbose output to stdout.
1045 
1046     Parameters
1047     ----------
1048     verbose : int
1049         Verbosity level. If ``verbose==1`` output is printed once in a while
1050         (when iteration mod verbose_mod is zero).; if larger than 1 then output
1051         is printed for each update.
1052     """
1053 
1054     def __init__(self, verbose):
1055         self.verbose = verbose
1056 
1057     def init(self, est, begin_at_stage=0):
1058         """Initialize reporter
1059 
1060         Parameters
1061         ----------
1062         est : Estimator
1063             The estimator
1064 
1065         begin_at_stage : int
1066             stage at which to begin reporting
1067         """
1068         # header fields and line format str
1069         header_fields = ['Iter', 'Train Loss']
1070         verbose_fmt = ['{iter:>10d}', '{train_score:>16.4f}']
1071         # do oob?
1072         if est.subsample < 1:
1073             header_fields.append('OOB Improve')
1074             verbose_fmt.append('{oob_impr:>16.4f}')
1075         header_fields.append('Remaining Time')
1076         verbose_fmt.append('{remaining_time:>16s}')
1077 
1078         # print the header line
1079         print(('%10s ' + '%16s ' *
1080                (len(header_fields) - 1)) % tuple(header_fields))
1081 
1082         self.verbose_fmt = ' '.join(verbose_fmt)
1083         # plot verbose info each time i % verbose_mod == 0
1084         self.verbose_mod = 1
1085         self.start_time = time()
1086         self.begin_at_stage = begin_at_stage
1087 
1088     def update(self, j, est):
1089         """Update reporter with new iteration.
1090 
1091         Parameters
1092         ----------
1093         j : int
1094             The new iteration
1095         est : Estimator
1096             The estimator
1097         """
1098         do_oob = est.subsample < 1
1099         # we need to take into account if we fit additional estimators.
1100         i = j - self.begin_at_stage  # iteration relative to the start iter
1101         if (i + 1) % self.verbose_mod == 0:
1102             oob_impr = est.oob_improvement_[j] if do_oob else 0
1103             remaining_time = ((est.n_estimators - (j + 1)) *
1104                               (time() - self.start_time) / float(i + 1))
1105             if remaining_time > 60:
1106                 remaining_time = '{0:.2f}m'.format(remaining_time / 60.0)
1107             else:
1108                 remaining_time = '{0:.2f}s'.format(remaining_time)
1109             print(self.verbose_fmt.format(iter=j + 1,
1110                                           train_score=est.train_score_[j],
1111                                           oob_impr=oob_impr,
1112                                           remaining_time=remaining_time))
1113             if self.verbose == 1 and ((i + 1) // (self.verbose_mod * 10) > 0):
1114                 # adjust verbose frequency (powers of 10)
1115                 self.verbose_mod *= 10
1116 
1117 
1118 class BaseGradientBoosting(BaseEnsemble, metaclass=ABCMeta):
1119     """Abstract base class for Gradient Boosting. """
1120 
1121     @abstractmethod
1122     def __init__(self, loss, learning_rate, n_estimators, criterion,
1123                  min_samples_split, min_samples_leaf, min_weight_fraction_leaf,
1124                  max_depth, min_impurity_decrease, min_impurity_split,
1125                  init, subsample, max_features,
1126                  random_state, alpha=0.9, verbose=0, max_leaf_nodes=None,
1127                  warm_start=False, presort='auto',
1128                  validation_fraction=0.1, n_iter_no_change=None,
1129                  tol=1e-4):
1130 
1131         self.n_estimators = n_estimators
1132         self.learning_rate = learning_rate
1133         self.loss = loss
1134         self.criterion = criterion
1135         self.min_samples_split = min_samples_split
1136         self.min_samples_leaf = min_samples_leaf
1137         self.min_weight_fraction_leaf = min_weight_fraction_leaf
1138         self.subsample = subsample
1139         self.max_features = max_features
1140         self.max_depth = max_depth
1141         self.min_impurity_decrease = min_impurity_decrease
1142         self.min_impurity_split = min_impurity_split
1143         self.init = init
1144         self.random_state = random_state
1145         self.alpha = alpha
1146         self.verbose = verbose
1147         self.max_leaf_nodes = max_leaf_nodes
1148         self.warm_start = warm_start
1149         self.presort = presort
1150         self.validation_fraction = validation_fraction
1151         self.n_iter_no_change = n_iter_no_change
1152         self.tol = tol
1153 
1154     def _fit_stage(self, i, X, y, y_pred, sample_weight, sample_mask,
1155                    random_state, X_idx_sorted, X_csc=None, X_csr=None):
1156         """Fit another stage of ``n_classes_`` trees to the boosting model. """
1157 
1158         assert sample_mask.dtype == np.bool
1159         loss = self.loss_
1160         original_y = y
1161 
1162         # Need to pass a copy of y_pred to negative_gradient() because y_pred
1163         # is partially updated at the end of the loop in
1164         # update_terminal_regions(), and gradients need to be evaluated at
1165         # iteration i - 1.
1166         y_pred_copy = y_pred.copy()
1167 
1168         for k in range(loss.K):
1169             if loss.is_multi_class:
1170                 y = np.array(original_y == k, dtype=np.float64)
1171 
1172             residual = loss.negative_gradient(y, y_pred_copy, k=k,
1173                                               sample_weight=sample_weight)
1174 
1175             # induce regression tree on residuals
1176             tree = DecisionTreeRegressor(
1177                 criterion=self.criterion,
1178                 splitter='best',
1179                 max_depth=self.max_depth,
1180                 min_samples_split=self.min_samples_split,
1181                 min_samples_leaf=self.min_samples_leaf,
1182                 min_weight_fraction_leaf=self.min_weight_fraction_leaf,
1183                 min_impurity_decrease=self.min_impurity_decrease,
1184                 min_impurity_split=self.min_impurity_split,
1185                 max_features=self.max_features,
1186                 max_leaf_nodes=self.max_leaf_nodes,
1187                 random_state=random_state,
1188                 presort=self.presort)
1189 
1190             if self.subsample < 1.0:
1191                 # no inplace multiplication!
1192                 sample_weight = sample_weight * sample_mask.astype(np.float64)
1193 
1194             X = X_csr if X_csr is not None else X
1195             tree.fit(X, residual, sample_weight=sample_weight,
1196                      check_input=False, X_idx_sorted=X_idx_sorted)
1197 
1198             # update tree leaves
1199             loss.update_terminal_regions(tree.tree_, X, y, residual, y_pred,
1200                                          sample_weight, sample_mask,
1201                                          learning_rate=self.learning_rate, k=k)
1202 
1203             # add tree to ensemble
1204             self.estimators_[i, k] = tree
1205 
1206         return y_pred
1207 
1208     def _check_params(self):
1209         """Check validity of parameters and raise ValueError if not valid. """
1210         if self.n_estimators <= 0:
1211             raise ValueError("n_estimators must be greater than 0 but "
1212                              "was %r" % self.n_estimators)
1213 
1214         if self.learning_rate <= 0.0:
1215             raise ValueError("learning_rate must be greater than 0 but "
1216                              "was %r" % self.learning_rate)
1217 
1218         if (self.loss not in self._SUPPORTED_LOSS
1219                 or self.loss not in LOSS_FUNCTIONS):
1220             raise ValueError("Loss '{0:s}' not supported. ".format(self.loss))
1221 
1222         if self.loss == 'deviance':
1223             loss_class = (MultinomialDeviance
1224                           if len(self.classes_) > 2
1225                           else BinomialDeviance)
1226         else:
1227             loss_class = LOSS_FUNCTIONS[self.loss]
1228 
1229         if self.loss in ('huber', 'quantile'):
1230             self.loss_ = loss_class(self.n_classes_, self.alpha)
1231         else:
1232             self.loss_ = loss_class(self.n_classes_)
1233 
1234         if not (0.0 < self.subsample <= 1.0):
1235             raise ValueError("subsample must be in (0,1] but "
1236                              "was %r" % self.subsample)
1237 
1238         if self.init is not None:
1239             if isinstance(self.init, str):
1240                 if self.init not in INIT_ESTIMATORS:
1241                     raise ValueError('init="%s" is not supported' % self.init)
1242             else:
1243                 if (not hasattr(self.init, 'fit')
1244                         or not hasattr(self.init, 'predict')):
1245                     raise ValueError("init=%r must be valid BaseEstimator "
1246                                      "and support both fit and "
1247                                      "predict" % self.init)
1248 
1249         if not (0.0 < self.alpha < 1.0):
1250             raise ValueError("alpha must be in (0.0, 1.0) but "
1251                              "was %r" % self.alpha)
1252 
1253         if isinstance(self.max_features, str):
1254             if self.max_features == "auto":
1255                 # if is_classification
1256                 if self.n_classes_ > 1:
1257                     max_features = max(1, int(np.sqrt(self.n_features_)))
1258                 else:
1259                     # is regression
1260                     max_features = self.n_features_
1261             elif self.max_features == "sqrt":
1262                 max_features = max(1, int(np.sqrt(self.n_features_)))
1263             elif self.max_features == "log2":
1264                 max_features = max(1, int(np.log2(self.n_features_)))
1265             else:
1266                 raise ValueError("Invalid value for max_features: %r. "
1267                                  "Allowed string values are 'auto', 'sqrt' "
1268                                  "or 'log2'." % self.max_features)
1269         elif self.max_features is None:
1270             max_features = self.n_features_
1271         elif isinstance(self.max_features, (numbers.Integral, np.integer)):
1272             max_features = self.max_features
1273         else:  # float
1274             if 0. < self.max_features <= 1.:
1275                 max_features = max(int(self.max_features *
1276                                        self.n_features_), 1)
1277             else:
1278                 raise ValueError("max_features must be in (0, n_features]")
1279 
1280         self.max_features_ = max_features
1281 
1282         if not isinstance(self.n_iter_no_change,
1283                           (numbers.Integral, np.integer, type(None))):
1284             raise ValueError("n_iter_no_change should either be None or an "
1285                              "integer. %r was passed"
1286                              % self.n_iter_no_change)
1287 
1288         allowed_presort = ('auto', True, False)
1289         if self.presort not in allowed_presort:
1290             raise ValueError("'presort' should be in {}. Got {!r} instead."
1291                              .format(allowed_presort, self.presort))
1292 
1293     def _init_state(self):
1294         """Initialize model state and allocate model state data structures. """
1295 
1296         if self.init is None:
1297             self.init_ = self.loss_.init_estimator()
1298         elif isinstance(self.init, str):
1299             self.init_ = INIT_ESTIMATORS[self.init]()
1300         else:
1301             self.init_ = self.init
1302 
1303         self.estimators_ = np.empty((self.n_estimators, self.loss_.K),
1304                                     dtype=np.object)
1305         self.train_score_ = np.zeros((self.n_estimators,), dtype=np.float64)
1306         # do oob?
1307         if self.subsample < 1.0:
1308             self.oob_improvement_ = np.zeros((self.n_estimators),
1309                                              dtype=np.float64)
1310 
1311     def _clear_state(self):
1312         """Clear the state of the gradient boosting model. """
1313         if hasattr(self, 'estimators_'):
1314             self.estimators_ = np.empty((0, 0), dtype=np.object)
1315         if hasattr(self, 'train_score_'):
1316             del self.train_score_
1317         if hasattr(self, 'oob_improvement_'):
1318             del self.oob_improvement_
1319         if hasattr(self, 'init_'):
1320             del self.init_
1321         if hasattr(self, '_rng'):
1322             del self._rng
1323 
1324     def _resize_state(self):
1325         """Add additional ``n_estimators`` entries to all attributes. """
1326         # self.n_estimators is the number of additional est to fit
1327         total_n_estimators = self.n_estimators
1328         if total_n_estimators < self.estimators_.shape[0]:
1329             raise ValueError('resize with smaller n_estimators %d < %d' %
1330                              (total_n_estimators, self.estimators_[0]))
1331 
1332         self.estimators_ = np.resize(self.estimators_,
1333                                      (total_n_estimators, self.loss_.K))
1334         self.train_score_ = np.resize(self.train_score_, total_n_estimators)
1335         if (self.subsample < 1 or hasattr(self, 'oob_improvement_')):
1336             # if do oob resize arrays or create new if not available
1337             if hasattr(self, 'oob_improvement_'):
1338                 self.oob_improvement_ = np.resize(self.oob_improvement_,
1339                                                   total_n_estimators)
1340             else:
1341                 self.oob_improvement_ = np.zeros((total_n_estimators,),
1342                                                  dtype=np.float64)
1343 
1344     def _is_initialized(self):
1345         return len(getattr(self, 'estimators_', [])) > 0
1346 
1347     def _check_initialized(self):
1348         """Check that the estimator is initialized, raising an error if not."""
1349         check_is_fitted(self, 'estimators_')
1350 
1351     @property
1352     @deprecated("Attribute n_features was deprecated in version 0.19 and "
1353                 "will be removed in 0.21.")
1354     def n_features(self):
1355         return self.n_features_
1356 
1357     def fit(self, X, y, sample_weight=None, monitor=None):
1358         """Fit the gradient boosting model.
1359 
1360         Parameters
1361         ----------
1362         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1363             The input samples. Internally, it will be converted to
1364             ``dtype=np.float32`` and if a sparse matrix is provided
1365             to a sparse ``csr_matrix``.
1366 
1367         y : array-like, shape (n_samples,)
1368             Target values (strings or integers in classification, real numbers
1369             in regression)
1370             For classification, labels must correspond to classes.
1371 
1372         sample_weight : array-like, shape (n_samples,) or None
1373             Sample weights. If None, then samples are equally weighted. Splits
1374             that would create child nodes with net zero or negative weight are
1375             ignored while searching for a split in each node. In the case of
1376             classification, splits are also ignored if they would result in any
1377             single class carrying a negative weight in either child node.
1378 
1379         monitor : callable, optional
1380             The monitor is called after each iteration with the current
1381             iteration, a reference to the estimator and the local variables of
1382             ``_fit_stages`` as keyword arguments ``callable(i, self,
1383             locals())``. If the callable returns ``True`` the fitting procedure
1384             is stopped. The monitor can be used for various things such as
1385             computing held-out estimates, early stopping, model introspect, and
1386             snapshoting.
1387 
1388         Returns
1389         -------
1390         self : object
1391         """
1392         # if not warmstart - clear the estimator state
1393         if not self.warm_start:
1394             self._clear_state()
1395 
1396         # Check input
1397         X, y = check_X_y(X, y, accept_sparse=['csr', 'csc', 'coo'], dtype=DTYPE)
1398         n_samples, self.n_features_ = X.shape
1399         if sample_weight is None:
1400             sample_weight = np.ones(n_samples, dtype=np.float32)
1401         else:
1402             sample_weight = column_or_1d(sample_weight, warn=True)
1403 
1404         check_consistent_length(X, y, sample_weight)
1405 
1406         y = self._validate_y(y, sample_weight)
1407 
1408         if self.n_iter_no_change is not None:
1409             X, X_val, y, y_val, sample_weight, sample_weight_val = (
1410                 train_test_split(X, y, sample_weight,
1411                                  random_state=self.random_state,
1412                                  test_size=self.validation_fraction))
1413         else:
1414             X_val = y_val = sample_weight_val = None
1415 
1416         self._check_params()
1417 
1418         if not self._is_initialized():
1419             # init state
1420             self._init_state()
1421 
1422             # fit initial model - FIXME make sample_weight optional
1423             self.init_.fit(X, y, sample_weight)
1424 
1425             # init predictions
1426             y_pred = self.init_.predict(X)
1427             begin_at_stage = 0
1428 
1429             # The rng state must be preserved if warm_start is True
1430             self._rng = check_random_state(self.random_state)
1431 
1432         else:
1433             # add more estimators to fitted model
1434             # invariant: warm_start = True
1435             if self.n_estimators < self.estimators_.shape[0]:
1436                 raise ValueError('n_estimators=%d must be larger or equal to '
1437                                  'estimators_.shape[0]=%d when '
1438                                  'warm_start==True'
1439                                  % (self.n_estimators,
1440                                     self.estimators_.shape[0]))
1441             begin_at_stage = self.estimators_.shape[0]
1442             # The requirements of _decision_function (called in two lines
1443             # below) are more constrained than fit. It accepts only CSR
1444             # matrices.
1445             X = check_array(X, dtype=DTYPE, order="C", accept_sparse='csr')
1446             y_pred = self._decision_function(X)
1447             self._resize_state()
1448 
1449         if self.presort is True and issparse(X):
1450             raise ValueError(
1451                 "Presorting is not supported for sparse matrices.")
1452 
1453         presort = self.presort
1454         # Allow presort to be 'auto', which means True if the dataset is dense,
1455         # otherwise it will be False.
1456         if presort == 'auto':
1457             presort = not issparse(X)
1458 
1459         X_idx_sorted = None
1460         if presort:
1461             X_idx_sorted = np.asfortranarray(np.argsort(X, axis=0),
1462                                              dtype=np.int32)
1463 
1464         # fit the boosting stages
1465         n_stages = self._fit_stages(X, y, y_pred, sample_weight, self._rng,
1466                                     X_val, y_val, sample_weight_val,
1467                                     begin_at_stage, monitor, X_idx_sorted)
1468 
1469         # change shape of arrays after fit (early-stopping or additional ests)
1470         if n_stages != self.estimators_.shape[0]:
1471             self.estimators_ = self.estimators_[:n_stages]
1472             self.train_score_ = self.train_score_[:n_stages]
1473             if hasattr(self, 'oob_improvement_'):
1474                 self.oob_improvement_ = self.oob_improvement_[:n_stages]
1475 
1476         self.n_estimators_ = n_stages
1477         return self
1478 
1479     def _fit_stages(self, X, y, y_pred, sample_weight, random_state,
1480                     X_val, y_val, sample_weight_val,
1481                     begin_at_stage=0, monitor=None, X_idx_sorted=None):
1482         """Iteratively fits the stages.
1483 
1484         For each stage it computes the progress (OOB, train score)
1485         and delegates to ``_fit_stage``.
1486         Returns the number of stages fit; might differ from ``n_estimators``
1487         due to early stopping.
1488         """
1489         n_samples = X.shape[0]
1490         do_oob = self.subsample < 1.0
1491         sample_mask = np.ones((n_samples, ), dtype=np.bool)
1492         n_inbag = max(1, int(self.subsample * n_samples))
1493         loss_ = self.loss_
1494 
1495         # Set min_weight_leaf from min_weight_fraction_leaf
1496         if self.min_weight_fraction_leaf != 0. and sample_weight is not None:
1497             min_weight_leaf = (self.min_weight_fraction_leaf *
1498                                np.sum(sample_weight))
1499         else:
1500             min_weight_leaf = 0.
1501 
1502         if self.verbose:
1503             verbose_reporter = VerboseReporter(self.verbose)
1504             verbose_reporter.init(self, begin_at_stage)
1505 
1506         X_csc = csc_matrix(X) if issparse(X) else None
1507         X_csr = csr_matrix(X) if issparse(X) else None
1508 
1509         if self.n_iter_no_change is not None:
1510             loss_history = np.full(self.n_iter_no_change, np.inf)
1511             # We create a generator to get the predictions for X_val after
1512             # the addition of each successive stage
1513             y_val_pred_iter = self._staged_decision_function(X_val)
1514 
1515         # perform boosting iterations
1516         i = begin_at_stage
1517         for i in range(begin_at_stage, self.n_estimators):
1518 
1519             # subsampling
1520             if do_oob:
1521                 sample_mask = _random_sample_mask(n_samples, n_inbag,
1522                                                   random_state)
1523                 # OOB score before adding this stage
1524                 old_oob_score = loss_(y[~sample_mask],
1525                                       y_pred[~sample_mask],
1526                                       sample_weight[~sample_mask])
1527 
1528             # fit next stage of trees
1529             y_pred = self._fit_stage(i, X, y, y_pred, sample_weight,
1530                                      sample_mask, random_state, X_idx_sorted,
1531                                      X_csc, X_csr)
1532 
1533             # track deviance (= loss)
1534             if do_oob:
1535                 self.train_score_[i] = loss_(y[sample_mask],
1536                                              y_pred[sample_mask],
1537                                              sample_weight[sample_mask])
1538                 self.oob_improvement_[i] = (
1539                     old_oob_score - loss_(y[~sample_mask],
1540                                           y_pred[~sample_mask],
1541                                           sample_weight[~sample_mask]))
1542             else:
1543                 # no need to fancy index w/ no subsampling
1544                 self.train_score_[i] = loss_(y, y_pred, sample_weight)
1545 
1546             if self.verbose > 0:
1547                 verbose_reporter.update(i, self)
1548 
1549             if monitor is not None:
1550                 early_stopping = monitor(i, self, locals())
1551                 if early_stopping:
1552                     break
1553 
1554             # We also provide an early stopping based on the score from
1555             # validation set (X_val, y_val), if n_iter_no_change is set
1556             if self.n_iter_no_change is not None:
1557                 # By calling next(y_val_pred_iter), we get the predictions
1558                 # for X_val after the addition of the current stage
1559                 validation_loss = loss_(y_val, next(y_val_pred_iter),
1560                                         sample_weight_val)
1561 
1562                 # Require validation_score to be better (less) than at least
1563                 # one of the last n_iter_no_change evaluations
1564                 if np.any(validation_loss + self.tol < loss_history):
1565                     loss_history[i % len(loss_history)] = validation_loss
1566                 else:
1567                     break
1568 
1569         return i + 1
1570 
1571     def _make_estimator(self, append=True):
1572         # we don't need _make_estimator
1573         raise NotImplementedError()
1574 
1575     def _init_decision_function(self, X):
1576         """Check input and compute prediction of ``init``. """
1577         self._check_initialized()
1578         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1579         if X.shape[1] != self.n_features_:
1580             raise ValueError("X.shape[1] should be {0:d}, not {1:d}.".format(
1581                 self.n_features_, X.shape[1]))
1582         score = self.init_.predict(X).astype(np.float64)
1583         return score
1584 
1585     def _decision_function(self, X):
1586         # for use in inner loop, not raveling the output in single-class case,
1587         # not doing input validation.
1588         score = self._init_decision_function(X)
1589         predict_stages(self.estimators_, X, self.learning_rate, score)
1590         return score
1591 
1592 
1593     def _staged_decision_function(self, X):
1594         """Compute decision function of ``X`` for each iteration.
1595 
1596         This method allows monitoring (i.e. determine error on testing set)
1597         after each stage.
1598 
1599         Parameters
1600         ----------
1601         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1602             The input samples. Internally, it will be converted to
1603             ``dtype=np.float32`` and if a sparse matrix is provided
1604             to a sparse ``csr_matrix``.
1605 
1606         Returns
1607         -------
1608         score : generator of array, shape (n_samples, k)
1609             The decision function of the input samples. The order of the
1610             classes corresponds to that in the attribute `classes_`.
1611             Regression and binary classification are special cases with
1612             ``k == 1``, otherwise ``k==n_classes``.
1613         """
1614         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1615         score = self._init_decision_function(X)
1616         for i in range(self.estimators_.shape[0]):
1617             predict_stage(self.estimators_, i, X, self.learning_rate, score)
1618             yield score.copy()
1619 
1620     @property
1621     def feature_importances_(self):
1622         """Return the feature importances (the higher, the more important the
1623            feature).
1624 
1625         Returns
1626         -------
1627         feature_importances_ : array, shape (n_features,)
1628         """
1629         self._check_initialized()
1630 
1631         total_sum = np.zeros((self.n_features_, ), dtype=np.float64)
1632         for stage in self.estimators_:
1633             stage_sum = sum(tree.tree_.compute_feature_importances(
1634                 normalize=False) for tree in stage) / len(stage)
1635             total_sum += stage_sum
1636 
1637         importances = total_sum / total_sum.sum()
1638         return importances
1639 
1640     def _validate_y(self, y, sample_weight):
1641         # 'sample_weight' is not utilised but is used for
1642         # consistency with similar method _validate_y of GBC
1643         self.n_classes_ = 1
1644         if y.dtype.kind == 'O':
1645             y = y.astype(np.float64)
1646         # Default implementation
1647         return y
1648 
1649     def apply(self, X):
1650         """Apply trees in the ensemble to X, return leaf indices.
1651 
1652         .. versionadded:: 0.17
1653 
1654         Parameters
1655         ----------
1656         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1657             The input samples. Internally, its dtype will be converted to
1658             ``dtype=np.float32``. If a sparse matrix is provided, it will
1659             be converted to a sparse ``csr_matrix``.
1660 
1661         Returns
1662         -------
1663         X_leaves : array-like, shape (n_samples, n_estimators, n_classes)
1664             For each datapoint x in X and for each tree in the ensemble,
1665             return the index of the leaf x ends up in each estimator.
1666             In the case of binary classification n_classes is 1.
1667         """
1668 
1669         self._check_initialized()
1670         X = self.estimators_[0, 0]._validate_X_predict(X, check_input=True)
1671 
1672         # n_classes will be equal to 1 in the binary classification or the
1673         # regression case.
1674         n_estimators, n_classes = self.estimators_.shape
1675         leaves = np.zeros((X.shape[0], n_estimators, n_classes))
1676 
1677         for i in range(n_estimators):
1678             for j in range(n_classes):
1679                 estimator = self.estimators_[i, j]
1680                 leaves[:, i, j] = estimator.apply(X, check_input=False)
1681 
1682         return leaves
1683 
1684 
1685 class GradientBoostingClassifier(BaseGradientBoosting, ClassifierMixin):
1686     """Gradient Boosting for classification.
1687 
1688     GB builds an additive model in a
1689     forward stage-wise fashion; it allows for the optimization of
1690     arbitrary differentiable loss functions. In each stage ``n_classes_``
1691     regression trees are fit on the negative gradient of the
1692     binomial or multinomial deviance loss function. Binary classification
1693     is a special case where only a single regression tree is induced.
1694 
1695     Read more in the :ref:`User Guide <gradient_boosting>`.
1696 
1697     Parameters
1698     ----------
1699     loss : {'deviance', 'exponential'}, optional (default='deviance')
1700         loss function to be optimized. 'deviance' refers to
1701         deviance (= logistic regression) for classification
1702         with probabilistic outputs. For loss 'exponential' gradient
1703         boosting recovers the AdaBoost algorithm.
1704 
1705     learning_rate : float, optional (default=0.1)
1706         learning rate shrinks the contribution of each tree by `learning_rate`.
1707         There is a trade-off between learning_rate and n_estimators.
1708 
1709     n_estimators : int (default=100)
1710         The number of boosting stages to perform. Gradient boosting
1711         is fairly robust to over-fitting so a large number usually
1712         results in better performance.
1713 
1714     subsample : float, optional (default=1.0)
1715         The fraction of samples to be used for fitting the individual base
1716         learners. If smaller than 1.0 this results in Stochastic Gradient
1717         Boosting. `subsample` interacts with the parameter `n_estimators`.
1718         Choosing `subsample < 1.0` leads to a reduction of variance
1719         and an increase in bias.
1720 
1721     criterion : string, optional (default="friedman_mse")
1722         The function to measure the quality of a split. Supported criteria
1723         are "friedman_mse" for the mean squared error with improvement
1724         score by Friedman, "mse" for mean squared error, and "mae" for
1725         the mean absolute error. The default value of "friedman_mse" is
1726         generally the best as it can provide a better approximation in
1727         some cases.
1728 
1729         .. versionadded:: 0.18
1730 
1731     min_samples_split : int, float, optional (default=2)
1732         The minimum number of samples required to split an internal node:
1733 
1734         - If int, then consider `min_samples_split` as the minimum number.
1735         - If float, then `min_samples_split` is a fraction and
1736           `ceil(min_samples_split * n_samples)` are the minimum
1737           number of samples for each split.
1738 
1739         .. versionchanged:: 0.18
1740            Added float values for fractions.
1741 
1742     min_samples_leaf : int, float, optional (default=1)
1743         The minimum number of samples required to be at a leaf node.
1744         A split point at any depth will only be considered if it leaves at
1745         least ``min_samples_leaf`` training samples in each of the left and
1746         right branches.  This may have the effect of smoothing the model,
1747         especially in regression.
1748 
1749         - If int, then consider `min_samples_leaf` as the minimum number.
1750         - If float, then `min_samples_leaf` is a fraction and
1751           `ceil(min_samples_leaf * n_samples)` are the minimum
1752           number of samples for each node.
1753 
1754         .. versionchanged:: 0.18
1755            Added float values for fractions.
1756 
1757     min_weight_fraction_leaf : float, optional (default=0.)
1758         The minimum weighted fraction of the sum total of weights (of all
1759         the input samples) required to be at a leaf node. Samples have
1760         equal weight when sample_weight is not provided.
1761 
1762     max_depth : integer, optional (default=3)
1763         maximum depth of the individual regression estimators. The maximum
1764         depth limits the number of nodes in the tree. Tune this parameter
1765         for best performance; the best value depends on the interaction
1766         of the input variables.
1767 
1768     min_impurity_decrease : float, optional (default=0.)
1769         A node will be split if this split induces a decrease of the impurity
1770         greater than or equal to this value.
1771 
1772         The weighted impurity decrease equation is the following::
1773 
1774             N_t / N * (impurity - N_t_R / N_t * right_impurity
1775                                 - N_t_L / N_t * left_impurity)
1776 
1777         where ``N`` is the total number of samples, ``N_t`` is the number of
1778         samples at the current node, ``N_t_L`` is the number of samples in the
1779         left child, and ``N_t_R`` is the number of samples in the right child.
1780 
1781         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
1782         if ``sample_weight`` is passed.
1783 
1784         .. versionadded:: 0.19
1785 
1786     min_impurity_split : float, (default=1e-7)
1787         Threshold for early stopping in tree growth. A node will split
1788         if its impurity is above the threshold, otherwise it is a leaf.
1789 
1790         .. deprecated:: 0.19
1791            ``min_impurity_split`` has been deprecated in favor of
1792            ``min_impurity_decrease`` in 0.19. The default value of
1793            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
1794            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
1795 
1796     init : estimator, optional
1797         An estimator object that is used to compute the initial
1798         predictions. ``init`` has to provide ``fit`` and ``predict``.
1799         If None it uses ``loss.init_estimator``.
1800 
1801     random_state : int, RandomState instance or None, optional (default=None)
1802         If int, random_state is the seed used by the random number generator;
1803         If RandomState instance, random_state is the random number generator;
1804         If None, the random number generator is the RandomState instance used
1805         by `np.random`.
1806 
1807     max_features : int, float, string or None, optional (default=None)
1808         The number of features to consider when looking for the best split:
1809 
1810         - If int, then consider `max_features` features at each split.
1811         - If float, then `max_features` is a fraction and
1812           `int(max_features * n_features)` features are considered at each
1813           split.
1814         - If "auto", then `max_features=sqrt(n_features)`.
1815         - If "sqrt", then `max_features=sqrt(n_features)`.
1816         - If "log2", then `max_features=log2(n_features)`.
1817         - If None, then `max_features=n_features`.
1818 
1819         Choosing `max_features < n_features` leads to a reduction of variance
1820         and an increase in bias.
1821 
1822         Note: the search for a split does not stop until at least one
1823         valid partition of the node samples is found, even if it requires to
1824         effectively inspect more than ``max_features`` features.
1825 
1826     verbose : int, default: 0
1827         Enable verbose output. If 1 then it prints progress and performance
1828         once in a while (the more trees the lower the frequency). If greater
1829         than 1 then it prints progress and performance for every tree.
1830 
1831     max_leaf_nodes : int or None, optional (default=None)
1832         Grow trees with ``max_leaf_nodes`` in best-first fashion.
1833         Best nodes are defined as relative reduction in impurity.
1834         If None then unlimited number of leaf nodes.
1835 
1836     warm_start : bool, default: False
1837         When set to ``True``, reuse the solution of the previous call to fit
1838         and add more estimators to the ensemble, otherwise, just erase the
1839         previous solution. See :term:`the Glossary <warm_start>`.
1840 
1841     presort : bool or 'auto', optional (default='auto')
1842         Whether to presort the data to speed up the finding of best splits in
1843         fitting. Auto mode by default will use presorting on dense data and
1844         default to normal sorting on sparse data. Setting presort to true on
1845         sparse data will raise an error.
1846 
1847         .. versionadded:: 0.17
1848            *presort* parameter.
1849 
1850     validation_fraction : float, optional, default 0.1
1851         The proportion of training data to set aside as validation set for
1852         early stopping. Must be between 0 and 1.
1853         Only used if ``n_iter_no_change`` is set to an integer.
1854 
1855         .. versionadded:: 0.20
1856 
1857     n_iter_no_change : int, default None
1858         ``n_iter_no_change`` is used to decide if early stopping will be used
1859         to terminate training when validation score is not improving. By
1860         default it is set to None to disable early stopping. If set to a
1861         number, it will set aside ``validation_fraction`` size of the training
1862         data as validation and terminate training when validation score is not
1863         improving in all of the previous ``n_iter_no_change`` numbers of
1864         iterations.
1865 
1866         .. versionadded:: 0.20
1867 
1868     tol : float, optional, default 1e-4
1869         Tolerance for the early stopping. When the loss is not improving
1870         by at least tol for ``n_iter_no_change`` iterations (if set to a
1871         number), the training stops.
1872 
1873         .. versionadded:: 0.20
1874 
1875     Attributes
1876     ----------
1877     n_estimators_ : int
1878         The number of estimators as selected by early stopping (if
1879         ``n_iter_no_change`` is specified). Otherwise it is set to
1880         ``n_estimators``.
1881 
1882         .. versionadded:: 0.20
1883 
1884     feature_importances_ : array, shape (n_features,)
1885         The feature importances (the higher, the more important the feature).
1886 
1887     oob_improvement_ : array, shape (n_estimators,)
1888         The improvement in loss (= deviance) on the out-of-bag samples
1889         relative to the previous iteration.
1890         ``oob_improvement_[0]`` is the improvement in
1891         loss of the first stage over the ``init`` estimator.
1892 
1893     train_score_ : array, shape (n_estimators,)
1894         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
1895         model at iteration ``i`` on the in-bag sample.
1896         If ``subsample == 1`` this is the deviance on the training data.
1897 
1898     loss_ : LossFunction
1899         The concrete ``LossFunction`` object.
1900 
1901     init_ : estimator
1902         The estimator that provides the initial predictions.
1903         Set via the ``init`` argument or ``loss.init_estimator``.
1904 
1905     estimators_ : ndarray of DecisionTreeRegressor,\
1906 shape (n_estimators, ``loss_.K``)
1907         The collection of fitted sub-estimators. ``loss_.K`` is 1 for binary
1908         classification, otherwise n_classes.
1909 
1910     Notes
1911     -----
1912     The features are always randomly permuted at each split. Therefore,
1913     the best found split may vary, even with the same training data and
1914     ``max_features=n_features``, if the improvement of the criterion is
1915     identical for several splits enumerated during the search of the best
1916     split. To obtain a deterministic behaviour during fitting,
1917     ``random_state`` has to be fixed.
1918 
1919     See also
1920     --------
1921     sklearn.tree.DecisionTreeClassifier, RandomForestClassifier
1922     AdaBoostClassifier
1923 
1924     References
1925     ----------
1926     J. Friedman, Greedy Function Approximation: A Gradient Boosting
1927     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
1928 
1929     J. Friedman, Stochastic Gradient Boosting, 1999
1930 
1931     T. Hastie, R. Tibshirani and J. Friedman.
1932     Elements of Statistical Learning Ed. 2, Springer, 2009.
1933     """
1934 
1935     _SUPPORTED_LOSS = ('deviance', 'exponential')
1936 
1937     def __init__(self, loss='deviance', learning_rate=0.1, n_estimators=100,
1938                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
1939                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
1940                  max_depth=3, min_impurity_decrease=0.,
1941                  min_impurity_split=None, init=None,
1942                  random_state=None, max_features=None, verbose=0,
1943                  max_leaf_nodes=None, warm_start=False,
1944                  presort='auto', validation_fraction=0.1,
1945                  n_iter_no_change=None, tol=1e-4):
1946 
1947         super().__init__(
1948             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
1949             criterion=criterion, min_samples_split=min_samples_split,
1950             min_samples_leaf=min_samples_leaf,
1951             min_weight_fraction_leaf=min_weight_fraction_leaf,
1952             max_depth=max_depth, init=init, subsample=subsample,
1953             max_features=max_features,
1954             random_state=random_state, verbose=verbose,
1955             max_leaf_nodes=max_leaf_nodes,
1956             min_impurity_decrease=min_impurity_decrease,
1957             min_impurity_split=min_impurity_split,
1958             warm_start=warm_start, presort=presort,
1959             validation_fraction=validation_fraction,
1960             n_iter_no_change=n_iter_no_change, tol=tol)
1961 
1962     def _validate_y(self, y, sample_weight):
1963         check_classification_targets(y)
1964         self.classes_, y = np.unique(y, return_inverse=True)
1965         n_trim_classes = np.count_nonzero(np.bincount(y, sample_weight))
1966         if n_trim_classes < 2:
1967             raise ValueError("y contains %d class after sample_weight "
1968                              "trimmed classes with zero weights, while a "
1969                              "minimum of 2 classes are required."
1970                              % n_trim_classes)
1971         self.n_classes_ = len(self.classes_)
1972         return y
1973 
1974     def decision_function(self, X):
1975         """Compute the decision function of ``X``.
1976 
1977         Parameters
1978         ----------
1979         X : {array-like, sparse matrix}, shape (n_samples, n_features)
1980             The input samples. Internally, it will be converted to
1981             ``dtype=np.float32`` and if a sparse matrix is provided
1982             to a sparse ``csr_matrix``.
1983 
1984         Returns
1985         -------
1986         score : array, shape (n_samples, n_classes) or (n_samples,)
1987             The decision function of the input samples. The order of the
1988             classes corresponds to that in the attribute `classes_`.
1989             Regression and binary classification produce an array of shape
1990             [n_samples].
1991         """
1992         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
1993         score = self._decision_function(X)
1994         if score.shape[1] == 1:
1995             return score.ravel()
1996         return score
1997 
1998     def staged_decision_function(self, X):
1999         """Compute decision function of ``X`` for each iteration.
2000 
2001         This method allows monitoring (i.e. determine error on testing set)
2002         after each stage.
2003 
2004         Parameters
2005         ----------
2006         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2007             The input samples. Internally, it will be converted to
2008             ``dtype=np.float32`` and if a sparse matrix is provided
2009             to a sparse ``csr_matrix``.
2010 
2011         Returns
2012         -------
2013         score : generator of array, shape (n_samples, k)
2014             The decision function of the input samples. The order of the
2015             classes corresponds to that in the attribute `classes_`.
2016             Regression and binary classification are special cases with
2017             ``k == 1``, otherwise ``k==n_classes``.
2018         """
2019         yield from self._staged_decision_function(X)
2020 
2021     def predict(self, X):
2022         """Predict class for X.
2023 
2024         Parameters
2025         ----------
2026         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2027             The input samples. Internally, it will be converted to
2028             ``dtype=np.float32`` and if a sparse matrix is provided
2029             to a sparse ``csr_matrix``.
2030 
2031         Returns
2032         -------
2033         y : array, shape (n_samples,)
2034             The predicted values.
2035         """
2036         score = self.decision_function(X)
2037         decisions = self.loss_._score_to_decision(score)
2038         return self.classes_.take(decisions, axis=0)
2039 
2040     def staged_predict(self, X):
2041         """Predict class at each stage for X.
2042 
2043         This method allows monitoring (i.e. determine error on testing set)
2044         after each stage.
2045 
2046         Parameters
2047         ----------
2048         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2049             The input samples. Internally, it will be converted to
2050             ``dtype=np.float32`` and if a sparse matrix is provided
2051             to a sparse ``csr_matrix``.
2052 
2053         Returns
2054         -------
2055         y : generator of array of shape (n_samples,)
2056             The predicted value of the input samples.
2057         """
2058         for score in self._staged_decision_function(X):
2059             decisions = self.loss_._score_to_decision(score)
2060             yield self.classes_.take(decisions, axis=0)
2061 
2062     def predict_proba(self, X):
2063         """Predict class probabilities for X.
2064 
2065         Parameters
2066         ----------
2067         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2068             The input samples. Internally, it will be converted to
2069             ``dtype=np.float32`` and if a sparse matrix is provided
2070             to a sparse ``csr_matrix``.
2071 
2072         Raises
2073         ------
2074         AttributeError
2075             If the ``loss`` does not support probabilities.
2076 
2077         Returns
2078         -------
2079         p : array, shape (n_samples, n_classes)
2080             The class probabilities of the input samples. The order of the
2081             classes corresponds to that in the attribute `classes_`.
2082         """
2083         score = self.decision_function(X)
2084         try:
2085             return self.loss_._score_to_proba(score)
2086         except NotFittedError:
2087             raise
2088         except AttributeError:
2089             raise AttributeError('loss=%r does not support predict_proba' %
2090                                  self.loss)
2091 
2092     def predict_log_proba(self, X):
2093         """Predict class log-probabilities for X.
2094 
2095         Parameters
2096         ----------
2097         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2098             The input samples. Internally, it will be converted to
2099             ``dtype=np.float32`` and if a sparse matrix is provided
2100             to a sparse ``csr_matrix``.
2101 
2102         Raises
2103         ------
2104         AttributeError
2105             If the ``loss`` does not support probabilities.
2106 
2107         Returns
2108         -------
2109         p : array, shape (n_samples, n_classes)
2110             The class log-probabilities of the input samples. The order of the
2111             classes corresponds to that in the attribute `classes_`.
2112         """
2113         proba = self.predict_proba(X)
2114         return np.log(proba)
2115 
2116     def staged_predict_proba(self, X):
2117         """Predict class probabilities at each stage for X.
2118 
2119         This method allows monitoring (i.e. determine error on testing set)
2120         after each stage.
2121 
2122         Parameters
2123         ----------
2124         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2125             The input samples. Internally, it will be converted to
2126             ``dtype=np.float32`` and if a sparse matrix is provided
2127             to a sparse ``csr_matrix``.
2128 
2129         Returns
2130         -------
2131         y : generator of array of shape (n_samples,)
2132             The predicted value of the input samples.
2133         """
2134         try:
2135             for score in self._staged_decision_function(X):
2136                 yield self.loss_._score_to_proba(score)
2137         except NotFittedError:
2138             raise
2139         except AttributeError:
2140             raise AttributeError('loss=%r does not support predict_proba' %
2141                                  self.loss)
2142 
2143 
2144 class GradientBoostingRegressor(BaseGradientBoosting, RegressorMixin):
2145     """Gradient Boosting for regression.
2146 
2147     GB builds an additive model in a forward stage-wise fashion;
2148     it allows for the optimization of arbitrary differentiable loss functions.
2149     In each stage a regression tree is fit on the negative gradient of the
2150     given loss function.
2151 
2152     Read more in the :ref:`User Guide <gradient_boosting>`.
2153 
2154     Parameters
2155     ----------
2156     loss : {'ls', 'lad', 'huber', 'quantile'}, optional (default='ls')
2157         loss function to be optimized. 'ls' refers to least squares
2158         regression. 'lad' (least absolute deviation) is a highly robust
2159         loss function solely based on order information of the input
2160         variables. 'huber' is a combination of the two. 'quantile'
2161         allows quantile regression (use `alpha` to specify the quantile).
2162 
2163     learning_rate : float, optional (default=0.1)
2164         learning rate shrinks the contribution of each tree by `learning_rate`.
2165         There is a trade-off between learning_rate and n_estimators.
2166 
2167     n_estimators : int (default=100)
2168         The number of boosting stages to perform. Gradient boosting
2169         is fairly robust to over-fitting so a large number usually
2170         results in better performance.
2171 
2172     subsample : float, optional (default=1.0)
2173         The fraction of samples to be used for fitting the individual base
2174         learners. If smaller than 1.0 this results in Stochastic Gradient
2175         Boosting. `subsample` interacts with the parameter `n_estimators`.
2176         Choosing `subsample < 1.0` leads to a reduction of variance
2177         and an increase in bias.
2178 
2179     criterion : string, optional (default="friedman_mse")
2180         The function to measure the quality of a split. Supported criteria
2181         are "friedman_mse" for the mean squared error with improvement
2182         score by Friedman, "mse" for mean squared error, and "mae" for
2183         the mean absolute error. The default value of "friedman_mse" is
2184         generally the best as it can provide a better approximation in
2185         some cases.
2186 
2187         .. versionadded:: 0.18
2188 
2189     min_samples_split : int, float, optional (default=2)
2190         The minimum number of samples required to split an internal node:
2191 
2192         - If int, then consider `min_samples_split` as the minimum number.
2193         - If float, then `min_samples_split` is a fraction and
2194           `ceil(min_samples_split * n_samples)` are the minimum
2195           number of samples for each split.
2196 
2197         .. versionchanged:: 0.18
2198            Added float values for fractions.
2199 
2200     min_samples_leaf : int, float, optional (default=1)
2201         The minimum number of samples required to be at a leaf node.
2202         A split point at any depth will only be considered if it leaves at
2203         least ``min_samples_leaf`` training samples in each of the left and
2204         right branches.  This may have the effect of smoothing the model,
2205         especially in regression.
2206 
2207         - If int, then consider `min_samples_leaf` as the minimum number.
2208         - If float, then `min_samples_leaf` is a fraction and
2209           `ceil(min_samples_leaf * n_samples)` are the minimum
2210           number of samples for each node.
2211 
2212         .. versionchanged:: 0.18
2213            Added float values for fractions.
2214 
2215     min_weight_fraction_leaf : float, optional (default=0.)
2216         The minimum weighted fraction of the sum total of weights (of all
2217         the input samples) required to be at a leaf node. Samples have
2218         equal weight when sample_weight is not provided.
2219 
2220     max_depth : integer, optional (default=3)
2221         maximum depth of the individual regression estimators. The maximum
2222         depth limits the number of nodes in the tree. Tune this parameter
2223         for best performance; the best value depends on the interaction
2224         of the input variables.
2225 
2226     min_impurity_decrease : float, optional (default=0.)
2227         A node will be split if this split induces a decrease of the impurity
2228         greater than or equal to this value.
2229 
2230         The weighted impurity decrease equation is the following::
2231 
2232             N_t / N * (impurity - N_t_R / N_t * right_impurity
2233                                 - N_t_L / N_t * left_impurity)
2234 
2235         where ``N`` is the total number of samples, ``N_t`` is the number of
2236         samples at the current node, ``N_t_L`` is the number of samples in the
2237         left child, and ``N_t_R`` is the number of samples in the right child.
2238 
2239         ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,
2240         if ``sample_weight`` is passed.
2241 
2242         .. versionadded:: 0.19
2243 
2244     min_impurity_split : float, (default=1e-7)
2245         Threshold for early stopping in tree growth. A node will split
2246         if its impurity is above the threshold, otherwise it is a leaf.
2247 
2248         .. deprecated:: 0.19
2249            ``min_impurity_split`` has been deprecated in favor of
2250            ``min_impurity_decrease`` in 0.19. The default value of
2251            ``min_impurity_split`` will change from 1e-7 to 0 in 0.23 and it
2252            will be removed in 0.25. Use ``min_impurity_decrease`` instead.
2253 
2254     init : estimator, optional (default=None)
2255         An estimator object that is used to compute the initial
2256         predictions. ``init`` has to provide ``fit`` and ``predict``.
2257         If None it uses ``loss.init_estimator``.
2258 
2259     random_state : int, RandomState instance or None, optional (default=None)
2260         If int, random_state is the seed used by the random number generator;
2261         If RandomState instance, random_state is the random number generator;
2262         If None, the random number generator is the RandomState instance used
2263         by `np.random`.
2264 
2265     max_features : int, float, string or None, optional (default=None)
2266         The number of features to consider when looking for the best split:
2267 
2268         - If int, then consider `max_features` features at each split.
2269         - If float, then `max_features` is a fraction and
2270           `int(max_features * n_features)` features are considered at each
2271           split.
2272         - If "auto", then `max_features=n_features`.
2273         - If "sqrt", then `max_features=sqrt(n_features)`.
2274         - If "log2", then `max_features=log2(n_features)`.
2275         - If None, then `max_features=n_features`.
2276 
2277         Choosing `max_features < n_features` leads to a reduction of variance
2278         and an increase in bias.
2279 
2280         Note: the search for a split does not stop until at least one
2281         valid partition of the node samples is found, even if it requires to
2282         effectively inspect more than ``max_features`` features.
2283 
2284     alpha : float (default=0.9)
2285         The alpha-quantile of the huber loss function and the quantile
2286         loss function. Only if ``loss='huber'`` or ``loss='quantile'``.
2287 
2288     verbose : int, default: 0
2289         Enable verbose output. If 1 then it prints progress and performance
2290         once in a while (the more trees the lower the frequency). If greater
2291         than 1 then it prints progress and performance for every tree.
2292 
2293     max_leaf_nodes : int or None, optional (default=None)
2294         Grow trees with ``max_leaf_nodes`` in best-first fashion.
2295         Best nodes are defined as relative reduction in impurity.
2296         If None then unlimited number of leaf nodes.
2297 
2298     warm_start : bool, default: False
2299         When set to ``True``, reuse the solution of the previous call to fit
2300         and add more estimators to the ensemble, otherwise, just erase the
2301         previous solution. See :term:`the Glossary <warm_start>`.
2302 
2303     presort : bool or 'auto', optional (default='auto')
2304         Whether to presort the data to speed up the finding of best splits in
2305         fitting. Auto mode by default will use presorting on dense data and
2306         default to normal sorting on sparse data. Setting presort to true on
2307         sparse data will raise an error.
2308 
2309         .. versionadded:: 0.17
2310            optional parameter *presort*.
2311 
2312     validation_fraction : float, optional, default 0.1
2313         The proportion of training data to set aside as validation set for
2314         early stopping. Must be between 0 and 1.
2315         Only used if ``n_iter_no_change`` is set to an integer.
2316 
2317         .. versionadded:: 0.20
2318 
2319     n_iter_no_change : int, default None
2320         ``n_iter_no_change`` is used to decide if early stopping will be used
2321         to terminate training when validation score is not improving. By
2322         default it is set to None to disable early stopping. If set to a
2323         number, it will set aside ``validation_fraction`` size of the training
2324         data as validation and terminate training when validation score is not
2325         improving in all of the previous ``n_iter_no_change`` numbers of
2326         iterations.
2327 
2328         .. versionadded:: 0.20
2329 
2330     tol : float, optional, default 1e-4
2331         Tolerance for the early stopping. When the loss is not improving
2332         by at least tol for ``n_iter_no_change`` iterations (if set to a
2333         number), the training stops.
2334 
2335         .. versionadded:: 0.20
2336 
2337 
2338     Attributes
2339     ----------
2340     feature_importances_ : array, shape (n_features,)
2341         The feature importances (the higher, the more important the feature).
2342 
2343     oob_improvement_ : array, shape (n_estimators,)
2344         The improvement in loss (= deviance) on the out-of-bag samples
2345         relative to the previous iteration.
2346         ``oob_improvement_[0]`` is the improvement in
2347         loss of the first stage over the ``init`` estimator.
2348 
2349     train_score_ : array, shape (n_estimators,)
2350         The i-th score ``train_score_[i]`` is the deviance (= loss) of the
2351         model at iteration ``i`` on the in-bag sample.
2352         If ``subsample == 1`` this is the deviance on the training data.
2353 
2354     loss_ : LossFunction
2355         The concrete ``LossFunction`` object.
2356 
2357     init_ : estimator
2358         The estimator that provides the initial predictions.
2359         Set via the ``init`` argument or ``loss.init_estimator``.
2360 
2361     estimators_ : array of DecisionTreeRegressor, shape (n_estimators, 1)
2362         The collection of fitted sub-estimators.
2363 
2364     Notes
2365     -----
2366     The features are always randomly permuted at each split. Therefore,
2367     the best found split may vary, even with the same training data and
2368     ``max_features=n_features``, if the improvement of the criterion is
2369     identical for several splits enumerated during the search of the best
2370     split. To obtain a deterministic behaviour during fitting,
2371     ``random_state`` has to be fixed.
2372 
2373     See also
2374     --------
2375     DecisionTreeRegressor, RandomForestRegressor
2376 
2377     References
2378     ----------
2379     J. Friedman, Greedy Function Approximation: A Gradient Boosting
2380     Machine, The Annals of Statistics, Vol. 29, No. 5, 2001.
2381 
2382     J. Friedman, Stochastic Gradient Boosting, 1999
2383 
2384     T. Hastie, R. Tibshirani and J. Friedman.
2385     Elements of Statistical Learning Ed. 2, Springer, 2009.
2386     """
2387 
2388     _SUPPORTED_LOSS = ('ls', 'lad', 'huber', 'quantile')
2389 
2390     def __init__(self, loss='ls', learning_rate=0.1, n_estimators=100,
2391                  subsample=1.0, criterion='friedman_mse', min_samples_split=2,
2392                  min_samples_leaf=1, min_weight_fraction_leaf=0.,
2393                  max_depth=3, min_impurity_decrease=0.,
2394                  min_impurity_split=None, init=None, random_state=None,
2395                  max_features=None, alpha=0.9, verbose=0, max_leaf_nodes=None,
2396                  warm_start=False, presort='auto', validation_fraction=0.1,
2397                  n_iter_no_change=None, tol=1e-4):
2398 
2399         super().__init__(
2400             loss=loss, learning_rate=learning_rate, n_estimators=n_estimators,
2401             criterion=criterion, min_samples_split=min_samples_split,
2402             min_samples_leaf=min_samples_leaf,
2403             min_weight_fraction_leaf=min_weight_fraction_leaf,
2404             max_depth=max_depth, init=init, subsample=subsample,
2405             max_features=max_features,
2406             min_impurity_decrease=min_impurity_decrease,
2407             min_impurity_split=min_impurity_split,
2408             random_state=random_state, alpha=alpha, verbose=verbose,
2409             max_leaf_nodes=max_leaf_nodes, warm_start=warm_start,
2410             presort=presort, validation_fraction=validation_fraction,
2411             n_iter_no_change=n_iter_no_change, tol=tol)
2412 
2413     def predict(self, X):
2414         """Predict regression target for X.
2415 
2416         Parameters
2417         ----------
2418         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2419             The input samples. Internally, it will be converted to
2420             ``dtype=np.float32`` and if a sparse matrix is provided
2421             to a sparse ``csr_matrix``.
2422 
2423         Returns
2424         -------
2425         y : array, shape (n_samples,)
2426             The predicted values.
2427         """
2428         X = check_array(X, dtype=DTYPE, order="C",  accept_sparse='csr')
2429         return self._decision_function(X).ravel()
2430 
2431     def staged_predict(self, X):
2432         """Predict regression target at each stage for X.
2433 
2434         This method allows monitoring (i.e. determine error on testing set)
2435         after each stage.
2436 
2437         Parameters
2438         ----------
2439         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2440             The input samples. Internally, it will be converted to
2441             ``dtype=np.float32`` and if a sparse matrix is provided
2442             to a sparse ``csr_matrix``.
2443 
2444         Returns
2445         -------
2446         y : generator of array of shape (n_samples,)
2447             The predicted value of the input samples.
2448         """
2449         for y in self._staged_decision_function(X):
2450             yield y.ravel()
2451 
2452     def apply(self, X):
2453         """Apply trees in the ensemble to X, return leaf indices.
2454 
2455         .. versionadded:: 0.17
2456 
2457         Parameters
2458         ----------
2459         X : {array-like, sparse matrix}, shape (n_samples, n_features)
2460             The input samples. Internally, its dtype will be converted to
2461             ``dtype=np.float32``. If a sparse matrix is provided, it will
2462             be converted to a sparse ``csr_matrix``.
2463 
2464         Returns
2465         -------
2466         X_leaves : array-like, shape (n_samples, n_estimators)
2467             For each datapoint x in X and for each tree in the ensemble,
2468             return the index of the leaf x ends up in each estimator.
2469         """
2470 
2471         leaves = super().apply(X)
2472         leaves = leaves.reshape(X.shape[0], self.estimators_.shape[0])
2473         return leaves

</code><intended_behavior>If the issue involves the handling of sample weights with initial estimators that do not support them, the implementation needs to ensure that these weights are only applied where applicable or provide a fallback mechanism. This could mean introducing a check or compatibility layer within the gradient boosting initialization process to verify if the `init` estimator supports sample weights and handle cases where it doesn't, possibly by ignoring the weights or applying a default weighting mechanism that ensures the initial model fitting process proceeds without errors.</intended_behavior>]
2025-10-18 16:58:48.065 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 16:58:48.065 | INFO     | Search-only: skipping patch selection
2025-10-18 16:58:48.184 | INFO     | Task scikit-learn__scikit-learn-12983 completed successfully.
2025-10-18 16:58:48.185 | INFO     | No patch generated. You can try running ACR again.
