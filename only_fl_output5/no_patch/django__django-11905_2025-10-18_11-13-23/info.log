2025-10-18 11:13:23.651 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 11:13:28.072 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=179, output_tokens=12, cost=0.002150
2025-10-18 11:13:28.073 | INFO     | Test agent decides that the issue statement does not contain reproduction steps; skipping reproducer tracing
2025-10-18 11:13:28.077 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 11:13:37.297 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=484, output_tokens=395, cost=0.016690
2025-10-18 11:13:37.307 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 11:13:40.319 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=749, output_tokens=61, cost=0.009320
2025-10-18 11:13:40.321 | DEBUG    | Extracted a valid json.
2025-10-18 11:13:40.321 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"__isnull\")",
        "search_class(\"QuerySet\")",
        "search_method(\"filter\")",
        "search_method(\"build_query\")",
        "search_class(\"SQLCompiler\")"
    ],
    "bug_locations": []
}
2025-10-18 11:13:40.409 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 11:13:40.409 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 11:13:56.221 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2668, output_tokens=666, cost=0.046660
2025-10-18 11:13:56.235 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 11:14:05.191 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3550, output_tokens=361, cost=0.046330
2025-10-18 11:14:05.200 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 11:14:13.760 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=715, output_tokens=339, cost=0.017320
2025-10-18 11:14:13.761 | DEBUG    | Extracted a valid json.
2025-10-18 11:14:13.761 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "django/db/models/query.py",
            "class": "QuerySet",
            "method": "filter, _filter_or_exclude",
            "intended_behavior": "Inside the `filter` and `_filter_or_exclude` methods, whenever a `__isnull` lookup is encountered, introduce a validation check to assert that the value associated with `__isnull` is a boolean (`True` or `False`). If a non-boolean value is detected, the method should raise a `ValueError` with a clear message indicating that `__isnull` only accepts boolean values. This preserves the existing functionality of allowing to filter querysets based on whether a field is `NULL` or not, but enforces correct usage and prevents the misuse of the lookup with unintended value types."
        },
        {
            "file": "django/db/models/sql/query.py",
            "class": "SQLCompiler",
            "method": "",
            "intended_behavior": "While not directly modifying this class for the boolean validation, ensuring that the `SQLCompiler` has access to the validation logic or error raised in `QuerySet`'s methods might help with broader error handling and messaging. The actual change would be in the queryset filtering logic, but attention should be given here to ensure that errors are gracefully handled and correctly propagate up to developers when incorrect `__isnull` usage is attempted. The intended behavior is not so much an alteration of `SQLCompiler` but an acknowledgment that changes in `QuerySet` logic may affect how SQL compilation errors are caught and reported."
        }
    ]
}
2025-10-18 11:14:13.777 | DEBUG    | Bug location extracted successfully: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "...(remaining elements truncated)..."
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError(
284                 'QuerySet indices must be integers or slices, not %s.'
285                 % type(k).__name__
286             )
287         assert ((not isinstance(k, slice) and (k >= 0)) or
288                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
289                  (k.stop is None or k.stop >= 0))), \
290             "Negative indexing is not supported."
291 
292         if self._result_cache is not None:
293             return self._result_cache[k]
294 
295         if isinstance(k, slice):
296             qs = self._chain()
297             if k.start is not None:
298                 start = int(k.start)
299             else:
300                 start = None
301             if k.stop is not None:
302                 stop = int(k.stop)
303             else:
304                 stop = None
305             qs.query.set_limits(start, stop)
306             return list(qs)[::k.step] if k.step else qs
307 
308         qs = self._chain()
309         qs.query.set_limits(k, k + 1)
310         qs._fetch_all()
311         return qs._result_cache[0]
312 
313     def __and__(self, other):
314         self._merge_sanity_check(other)
315         if isinstance(other, EmptyQuerySet):
316             return other
317         if isinstance(self, EmptyQuerySet):
318             return self
319         combined = self._chain()
320         combined._merge_known_related_objects(other)
321         combined.query.combine(other.query, sql.AND)
322         return combined
323 
324     def __or__(self, other):
325         self._merge_sanity_check(other)
326         if isinstance(self, EmptyQuerySet):
327             return other
328         if isinstance(other, EmptyQuerySet):
329             return self
330         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
331         combined = query._chain()
332         combined._merge_known_related_objects(other)
333         if not other.query.can_filter():
334             other = other.model._base_manager.filter(pk__in=other.values('pk'))
335         combined.query.combine(other.query, sql.OR)
336         return combined
337 
338     ####################################
339     # METHODS THAT DO DATABASE QUERIES #
340     ####################################
341 
342     def _iterator(self, use_chunked_fetch, chunk_size):
343         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
344 
345     def iterator(self, chunk_size=2000):
346         """
347         An iterator over the results from applying this QuerySet to the
348         database.
349         """
350         if chunk_size <= 0:
351             raise ValueError('Chunk size must be strictly positive.')
352         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
353         return self._iterator(use_chunked_fetch, chunk_size)
354 
355     def aggregate(self, *args, **kwargs):
356         """
357         Return a dictionary containing the calculations (aggregation)
358         over the current queryset.
359 
360         If args is present the expression is passed as a kwarg using
361         the Aggregate object's default alias.
362         """
363         if self.query.distinct_fields:
364             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
365         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
366         for arg in args:
367             # The default_alias property raises TypeError if default_alias
368             # can't be set automatically or AttributeError if it isn't an
369             # attribute.
370             try:
371                 arg.default_alias
372             except (AttributeError, TypeError):
373                 raise TypeError("Complex aggregates require an alias")
374             kwargs[arg.default_alias] = arg
375 
376         query = self.query.chain()
377         for (alias, aggregate_expr) in kwargs.items():
378             query.add_annotation(aggregate_expr, alias, is_summary=True)
379             if not query.annotations[alias].contains_aggregate:
380                 raise TypeError("%s is not an aggregate expression" % alias)
381         return query.get_aggregation(self.db, kwargs)
382 
383     def count(self):
384         """
385         Perform a SELECT COUNT() and return the number of records as an
386         integer.
387 
388         If the QuerySet is already fully cached, return the length of the
389         cached results set to avoid multiple SELECT COUNT(*) calls.
390         """
391         if self._result_cache is not None:
392             return len(self._result_cache)
393 
394         return self.query.get_count(using=self.db)
395 
396     def get(self, *args, **kwargs):
397         """
398         Perform the query and return a single object matching the given
399         keyword arguments.
400         """
401         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
402         if self.query.can_filter() and not self.query.distinct_fields:
403             clone = clone.order_by()
404         limit = None
405         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
406             limit = MAX_GET_RESULTS
407             clone.query.set_limits(high=limit)
408         num = len(clone)
409         if num == 1:
410             return clone._result_cache[0]
411         if not num:
412             raise self.model.DoesNotExist(
413                 "%s matching query does not exist." %
414                 self.model._meta.object_name
415             )
416         raise self.model.MultipleObjectsReturned(
417             'get() returned more than one %s -- it returned %s!' % (
418                 self.model._meta.object_name,
419                 num if not limit or num < limit else 'more than %s' % (limit - 1),
420             )
421         )
422 
423     def create(self, **kwargs):
424         """
425         Create a new object with the given kwargs, saving it to the database
426         and returning the created object.
427         """
428         obj = self.model(**kwargs)
429         self._for_write = True
430         obj.save(force_insert=True, using=self.db)
431         return obj
432 
433     def _populate_pk_values(self, objs):
434         for obj in objs:
435             if obj.pk is None:
436                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
437 
438     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
439         """
440         Insert each of the instances into the database. Do *not* call
441         save() on each of the instances, do not send any pre/post_save
442         signals, and do not set the primary key attribute if it is an
443         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
444         Multi-table models are not supported.
445         """
446         # When you bulk insert you don't get the primary keys back (if it's an
447         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
448         # you can't insert into the child tables which references this. There
449         # are two workarounds:
450         # 1) This could be implemented if you didn't have an autoincrement pk
451         # 2) You could do it by doing O(n) normal inserts into the parent
452         #    tables to get the primary keys back and then doing a single bulk
453         #    insert into the childmost table.
454         # We currently set the primary keys on the objects when using
455         # PostgreSQL via the RETURNING ID clause. It should be possible for
456         # Oracle as well, but the semantics for extracting the primary keys is
457         # trickier so it's not done yet.
458         assert batch_size is None or batch_size > 0
459         # Check that the parents share the same concrete model with the our
460         # model to detect the inheritance pattern ConcreteGrandParent ->
461         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
462         # would not identify that case as involving multiple tables.
463         for parent in self.model._meta.get_parent_list():
464             if parent._meta.concrete_model is not self.model._meta.concrete_model:
465                 raise ValueError("Can't bulk create a multi-table inherited model")
466         if not objs:
467             return objs
468         self._for_write = True
469         connection = connections[self.db]
470         opts = self.model._meta
471         fields = opts.concrete_fields
472         objs = list(objs)
473         self._populate_pk_values(objs)
474         with transaction.atomic(using=self.db, savepoint=False):
475             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
476             if objs_with_pk:
477                 returned_columns = self._batched_insert(
478                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
479                 )
480                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
481                     for result, field in zip(results, opts.db_returning_fields):
482                         if field != opts.pk:
483                             setattr(obj_with_pk, field.attname, result)
484                 for obj_with_pk in objs_with_pk:
485                     obj_with_pk._state.adding = False
486                     obj_with_pk._state.db = self.db
487             if objs_without_pk:
488                 fields = [f for f in fields if not isinstance(f, AutoField)]
489                 returned_columns = self._batched_insert(
490                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
491                 )
492                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
493                     assert len(returned_columns) == len(objs_without_pk)
494                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
495                     for result, field in zip(results, opts.db_returning_fields):
496                         setattr(obj_without_pk, field.attname, result)
497                     obj_without_pk._state.adding = False
498                     obj_without_pk._state.db = self.db
499 
500         return objs
501 
502     def bulk_update(self, objs, fields, batch_size=None):
503         """
504         Update the given fields in each of the given objects in the database.
505         """
506         if batch_size is not None and batch_size < 0:
507             raise ValueError('Batch size must be a positive integer.')
508         if not fields:
509             raise ValueError('Field names must be given to bulk_update().')
510         objs = tuple(objs)
511         if any(obj.pk is None for obj in objs):
512             raise ValueError('All bulk_update() objects must have a primary key set.')
513         fields = [self.model._meta.get_field(name) for name in fields]
514         if any(not f.concrete or f.many_to_many for f in fields):
515             raise ValueError('bulk_update() can only be used with concrete fields.')
516         if any(f.primary_key for f in fields):
517             raise ValueError('bulk_update() cannot be used with primary key fields.')
518         if not objs:
519             return
520         # PK is used twice in the resulting update query, once in the filter
521         # and once in the WHEN. Each field will also have one CAST.
522         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
523         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
524         requires_casting = connections[self.db].features.requires_casted_case_in_updates
525         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
526         updates = []
527         for batch_objs in batches:
528             update_kwargs = {}
529             for field in fields:
530                 when_statements = []
531                 for obj in batch_objs:
532                     attr = getattr(obj, field.attname)
533                     if not isinstance(attr, Expression):
534                         attr = Value(attr, output_field=field)
535                     when_statements.append(When(pk=obj.pk, then=attr))
536                 case_statement = Case(*when_statements, output_field=field)
537                 if requires_casting:
538                     case_statement = Cast(case_statement, output_field=field)
539                 update_kwargs[field.attname] = case_statement
540             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
541         with transaction.atomic(using=self.db, savepoint=False):
542             for pks, update_kwargs in updates:
543                 self.filter(pk__in=pks).update(**update_kwargs)
544     bulk_update.alters_data = True
545 
546     def get_or_create(self, defaults=None, **kwargs):
547         """
548         Look up an object with the given kwargs, creating one if necessary.
549         Return a tuple of (object, created), where created is a boolean
550         specifying whether an object was created.
551         """
552         # The get() needs to be targeted at the write database in order
553         # to avoid potential transaction consistency problems.
554         self._for_write = True
555         try:
556             return self.get(**kwargs), False
557         except self.model.DoesNotExist:
558             params = self._extract_model_params(defaults, **kwargs)
559             return self._create_object_from_params(kwargs, params)
560 
561     def update_or_create(self, defaults=None, **kwargs):
562         """
563         Look up an object with the given kwargs, updating one with defaults
564         if it exists, otherwise create a new one.
565         Return a tuple (object, created), where created is a boolean
566         specifying whether an object was created.
567         """
568         defaults = defaults or {}
569         self._for_write = True
570         with transaction.atomic(using=self.db):
571             try:
572                 obj = self.select_for_update().get(**kwargs)
573             except self.model.DoesNotExist:
574                 params = self._extract_model_params(defaults, **kwargs)
575                 # Lock the row so that a concurrent update is blocked until
576                 # after update_or_create() has performed its save.
577                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
578                 if created:
579                     return obj, created
580             for k, v in defaults.items():
581                 setattr(obj, k, v() if callable(v) else v)
582             obj.save(using=self.db)
583         return obj, False
584 
585     def _create_object_from_params(self, lookup, params, lock=False):
586         """
587         Try to create an object using passed params. Used by get_or_create()
588         and update_or_create().
589         """
590         try:
591             with transaction.atomic(using=self.db):
592                 params = {k: v() if callable(v) else v for k, v in params.items()}
593                 obj = self.create(**params)
594             return obj, True
595         except IntegrityError as e:
596             try:
597                 qs = self.select_for_update() if lock else self
598                 return qs.get(**lookup), False
599             except self.model.DoesNotExist:
600                 pass
601             raise e
602 
603     def _extract_model_params(self, defaults, **kwargs):
604         """
605         Prepare `params` for creating a model instance based on the given
606         kwargs; for use by get_or_create() and update_or_create().
607         """
608         defaults = defaults or {}
609         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
610         params.update(defaults)
611         property_names = self.model._meta._property_names
612         invalid_params = []
613         for param in params:
614             try:
615                 self.model._meta.get_field(param)
616             except exceptions.FieldDoesNotExist:
617                 # It's okay to use a model's property if it has a setter.
618                 if not (param in property_names and getattr(self.model, param).fset):
619                     invalid_params.append(param)
620         if invalid_params:
621             raise exceptions.FieldError(
622                 "Invalid field name(s) for model %s: '%s'." % (
623                     self.model._meta.object_name,
624                     "', '".join(sorted(invalid_params)),
625                 ))
626         return params
627 
628     def _earliest(self, *fields):
629         """
630         Return the earliest object according to fields (if given) or by the
631         model's Meta.get_latest_by.
632         """
633         if fields:
634             order_by = fields
635         else:
636             order_by = getattr(self.model._meta, 'get_latest_by')
637             if order_by and not isinstance(order_by, (tuple, list)):
638                 order_by = (order_by,)
639         if order_by is None:
640             raise ValueError(
641                 "earliest() and latest() require either fields as positional "
642                 "arguments or 'get_latest_by' in the model's Meta."
643             )
644 
645         assert not self.query.is_sliced, \
646             "Cannot change a query once a slice has been taken."
647         obj = self._chain()
648         obj.query.set_limits(high=1)
649         obj.query.clear_ordering(force_empty=True)
650         obj.query.add_ordering(*order_by)
651         return obj.get()
652 
653     def earliest(self, *fields):
654         return self._earliest(*fields)
655 
656     def latest(self, *fields):
657         return self.reverse()._earliest(*fields)
658 
659     def first(self):
660         """Return the first object of a query or None if no match is found."""
661         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
662             return obj
663 
664     def last(self):
665         """Return the last object of a query or None if no match is found."""
666         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
667             return obj
668 
669     def in_bulk(self, id_list=None, *, field_name='pk'):
670         """
671         Return a dictionary mapping each of the given IDs to the object with
672         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
673         """
674         assert not self.query.is_sliced, \
675             "Cannot use 'limit' or 'offset' with in_bulk"
676         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
677             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
678         if id_list is not None:
679             if not id_list:
680                 return {}
681             filter_key = '{}__in'.format(field_name)
682             batch_size = connections[self.db].features.max_query_params
683             id_list = tuple(id_list)
684             # If the database has a limit on the number of query parameters
685             # (e.g. SQLite), retrieve objects in batches if necessary.
686             if batch_size and batch_size < len(id_list):
687                 qs = ()
688                 for offset in range(0, len(id_list), batch_size):
689                     batch = id_list[offset:offset + batch_size]
690                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
691             else:
692                 qs = self.filter(**{filter_key: id_list}).order_by()
693         else:
694             qs = self._chain()
695         return {getattr(obj, field_name): obj for obj in qs}
696 
697     def delete(self):
698         """Delete the records in the current QuerySet."""
699         assert not self.query.is_sliced, \
700             "Cannot use 'limit' or 'offset' with delete."
701 
702         if self._fields is not None:
703             raise TypeError("Cannot call delete() after .values() or .values_list()")
704 
705         del_query = self._chain()
706 
707         # The delete is actually 2 queries - one to find related objects,
708         # and one to delete. Make sure that the discovery of related
709         # objects is performed on the same database as the deletion.
710         del_query._for_write = True
711 
712         # Disable non-supported fields.
713         del_query.query.select_for_update = False
714         del_query.query.select_related = False
715         del_query.query.clear_ordering(force_empty=True)
716 
717         collector = Collector(using=del_query.db)
718         collector.collect(del_query)
719         deleted, _rows_count = collector.delete()
720 
721         # Clear the result cache, in case this QuerySet gets reused.
722         self._result_cache = None
723         return deleted, _rows_count
724 
725     delete.alters_data = True
726     delete.queryset_only = True
727 
728     def _raw_delete(self, using):
729         """
730         Delete objects found from the given queryset in single direct SQL
731         query. No signals are sent and there is no protection for cascades.
732         """
733         return sql.DeleteQuery(self.model).delete_qs(self, using)
734     _raw_delete.alters_data = True
735 
736     def update(self, **kwargs):
737         """
738         Update all elements in the current QuerySet, setting all the given
739         fields to the appropriate values.
740         """
741         assert not self.query.is_sliced, \
742             "Cannot update a query once a slice has been taken."
743         self._for_write = True
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_values(kwargs)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         with transaction.mark_for_rollback_on_error(using=self.db):
749             rows = query.get_compiler(self.db).execute_sql(CURSOR)
750         self._result_cache = None
751         return rows
752     update.alters_data = True
753 
754     def _update(self, values):
755         """
756         A version of update() that accepts field objects instead of field names.
757         Used primarily for model saving and not intended for use by general
758         code (it requires too much poking around at model internals to be
759         useful at that level).
760         """
761         assert not self.query.is_sliced, \
762             "Cannot update a query once a slice has been taken."
763         query = self.query.chain(sql.UpdateQuery)
764         query.add_update_fields(values)
765         # Clear any annotations so that they won't be present in subqueries.
766         query.annotations = {}
767         self._result_cache = None
768         return query.get_compiler(self.db).execute_sql(CURSOR)
769     _update.alters_data = True
770     _update.queryset_only = False
771 
772     def exists(self):
773         if self._result_cache is None:
774             return self.query.has_results(using=self.db)
775         return bool(self._result_cache)
776 
777     def _prefetch_related_objects(self):
778         # This method can only be called once the result cache has been filled.
779         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
780         self._prefetch_done = True
781 
782     def explain(self, *, format=None, **options):
783         return self.query.explain(using=self.db, format=format, **options)
784 
785     ##################################################
786     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
787     ##################################################
788 
789     def raw(self, raw_query, params=None, translations=None, using=None):
790         if using is None:
791             using = self.db
792         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
793         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
794         return qs
795 
796     def _values(self, *fields, **expressions):
797         clone = self._chain()
798         if expressions:
799             clone = clone.annotate(**expressions)
800         clone._fields = fields
801         clone.query.set_values(fields)
802         return clone
803 
804     def values(self, *fields, **expressions):
805         fields += tuple(expressions)
806         clone = self._values(*fields, **expressions)
807         clone._iterable_class = ValuesIterable
808         return clone
809 
810     def values_list(self, *fields, flat=False, named=False):
811         if flat and named:
812             raise TypeError("'flat' and 'named' can't be used together.")
813         if flat and len(fields) > 1:
814             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
815 
816         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
817         _fields = []
818         expressions = {}
819         counter = 1
820         for field in fields:
821             if hasattr(field, 'resolve_expression'):
822                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
823                 while True:
824                     field_id = field_id_prefix + str(counter)
825                     counter += 1
826                     if field_id not in field_names:
827                         break
828                 expressions[field_id] = field
829                 _fields.append(field_id)
830             else:
831                 _fields.append(field)
832 
833         clone = self._values(*_fields, **expressions)
834         clone._iterable_class = (
835             NamedValuesListIterable if named
836             else FlatValuesListIterable if flat
837             else ValuesListIterable
838         )
839         return clone
840 
841     def dates(self, field_name, kind, order='ASC'):
842         """
843         Return a list of date objects representing all available dates for
844         the given field_name, scoped to 'kind'.
845         """
846         assert kind in ('year', 'month', 'week', 'day'), \
847             "'kind' must be one of 'year', 'month', 'week', or 'day'."
848         assert order in ('ASC', 'DESC'), \
849             "'order' must be either 'ASC' or 'DESC'."
850         return self.annotate(
851             datefield=Trunc(field_name, kind, output_field=DateField()),
852             plain_field=F(field_name)
853         ).values_list(
854             'datefield', flat=True
855         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
856 
857     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
858         """
859         Return a list of datetime objects representing all available
860         datetimes for the given field_name, scoped to 'kind'.
861         """
862         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
863             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
864         assert order in ('ASC', 'DESC'), \
865             "'order' must be either 'ASC' or 'DESC'."
866         if settings.USE_TZ:
867             if tzinfo is None:
868                 tzinfo = timezone.get_current_timezone()
869         else:
870             tzinfo = None
871         return self.annotate(
872             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
873             plain_field=F(field_name)
874         ).values_list(
875             'datetimefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
877 
878     def none(self):
879         """Return an empty QuerySet."""
880         clone = self._chain()
881         clone.query.set_empty()
882         return clone
883 
884     ##################################################################
885     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
886     ##################################################################
887 
888     def all(self):
889         """
890         Return a new QuerySet that is a copy of the current one. This allows a
891         QuerySet to proxy for a model manager in some cases.
892         """
893         return self._chain()
894 
895     def filter(self, *args, **kwargs):
896         """
897         Return a new QuerySet instance with the args ANDed to the existing
898         set.
899         """
900         self._not_support_combined_queries('filter')
901         return self._filter_or_exclude(False, *args, **kwargs)
902 
903     def exclude(self, *args, **kwargs):
904         """
905         Return a new QuerySet instance with NOT (args) ANDed to the existing
906         set.
907         """
908         self._not_support_combined_queries('exclude')
909         return self._filter_or_exclude(True, *args, **kwargs)
910 
911     def _filter_or_exclude(self, negate, *args, **kwargs):
912         if args or kwargs:
913             assert not self.query.is_sliced, \
914                 "Cannot filter a query once a slice has been taken."
915 
916         clone = self._chain()
917         if negate:
918             clone.query.add_q(~Q(*args, **kwargs))
919         else:
920             clone.query.add_q(Q(*args, **kwargs))
921         return clone
922 
923     def complex_filter(self, filter_obj):
924         """
925         Return a new QuerySet instance with filter_obj added to the filters.
926 
927         filter_obj can be a Q object or a dictionary of keyword lookup
928         arguments.
929 
930         This exists to support framework features such as 'limit_choices_to',
931         and usually it will be more natural to use other methods.
932         """
933         if isinstance(filter_obj, Q):
934             clone = self._chain()
935             clone.query.add_q(filter_obj)
936             return clone
937         else:
938             return self._filter_or_exclude(None, **filter_obj)
939 
940     def _combinator_query(self, combinator, *other_qs, all=False):
941         # Clone the query to inherit the select list and everything
942         clone = self._chain()
943         # Clear limits and ordering so they can be reapplied
944         clone.query.clear_ordering(True)
945         clone.query.clear_limits()
946         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
947         clone.query.combinator = combinator
948         clone.query.combinator_all = all
949         return clone
950 
951     def union(self, *other_qs, all=False):
952         # If the query is an EmptyQuerySet, combine all nonempty querysets.
953         if isinstance(self, EmptyQuerySet):
954             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
955             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
956         return self._combinator_query('union', *other_qs, all=all)
957 
958     def intersection(self, *other_qs):
959         # If any query is an EmptyQuerySet, return it.
960         if isinstance(self, EmptyQuerySet):
961             return self
962         for other in other_qs:
963             if isinstance(other, EmptyQuerySet):
964                 return other
965         return self._combinator_query('intersection', *other_qs)
966 
967     def difference(self, *other_qs):
968         # If the query is an EmptyQuerySet, return it.
969         if isinstance(self, EmptyQuerySet):
970             return self
971         return self._combinator_query('difference', *other_qs)
972 
973     def select_for_update(self, nowait=False, skip_locked=False, of=()):
974         """
975         Return a new QuerySet instance that will select objects with a
976         FOR UPDATE lock.
977         """
978         if nowait and skip_locked:
979             raise ValueError('The nowait option cannot be used with skip_locked.')
980         obj = self._chain()
981         obj._for_write = True
982         obj.query.select_for_update = True
983         obj.query.select_for_update_nowait = nowait
984         obj.query.select_for_update_skip_locked = skip_locked
985         obj.query.select_for_update_of = of
986         return obj
987 
988     def select_related(self, *fields):
989         """
990         Return a new QuerySet instance that will select related objects.
991 
992         If fields are specified, they must be ForeignKey fields and only those
993         related objects are included in the selection.
994 
995         If select_related(None) is called, clear the list.
996         """
997         self._not_support_combined_queries('select_related')
998         if self._fields is not None:
999             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1000 
1001         obj = self._chain()
1002         if fields == (None,):
1003             obj.query.select_related = False
1004         elif fields:
1005             obj.query.add_select_related(fields)
1006         else:
1007             obj.query.select_related = True
1008         return obj
1009 
1010     def prefetch_related(self, *lookups):
1011         """
1012         Return a new QuerySet instance that will prefetch the specified
1013         Many-To-One and Many-To-Many related objects when the QuerySet is
1014         evaluated.
1015 
1016         When prefetch_related() is called more than once, append to the list of
1017         prefetch lookups. If prefetch_related(None) is called, clear the list.
1018         """
1019         self._not_support_combined_queries('prefetch_related')
1020         clone = self._chain()
1021         if lookups == (None,):
1022             clone._prefetch_related_lookups = ()
1023         else:
1024             for lookup in lookups:
1025                 if isinstance(lookup, Prefetch):
1026                     lookup = lookup.prefetch_to
1027                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1028                 if lookup in self.query._filtered_relations:
1029                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1030             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1031         return clone
1032 
1033     def annotate(self, *args, **kwargs):
1034         """
1035         Return a query set in which the returned objects have been annotated
1036         with extra data or aggregations.
1037         """
1038         self._not_support_combined_queries('annotate')
1039         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1040         annotations = {}
1041         for arg in args:
1042             # The default_alias property may raise a TypeError.
1043             try:
1044                 if arg.default_alias in kwargs:
1045                     raise ValueError("The named annotation '%s' conflicts with the "
1046                                      "default name for another annotation."
1047                                      % arg.default_alias)
1048             except TypeError:
1049                 raise TypeError("Complex annotations require an alias")
1050             annotations[arg.default_alias] = arg
1051         annotations.update(kwargs)
1052 
1053         clone = self._chain()
1054         names = self._fields
1055         if names is None:
1056             names = set(chain.from_iterable(
1057                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1058                 for field in self.model._meta.get_fields()
1059             ))
1060 
1061         for alias, annotation in annotations.items():
1062             if alias in names:
1063                 raise ValueError("The annotation '%s' conflicts with a field on "
1064                                  "the model." % alias)
1065             if isinstance(annotation, FilteredRelation):
1066                 clone.query.add_filtered_relation(annotation, alias)
1067             else:
1068                 clone.query.add_annotation(annotation, alias, is_summary=False)
1069 
1070         for alias, annotation in clone.query.annotations.items():
1071             if alias in annotations and annotation.contains_aggregate:
1072                 if clone._fields is None:
1073                     clone.query.group_by = True
1074                 else:
1075                     clone.query.set_group_by()
1076                 break
1077 
1078         return clone
1079 
1080     def order_by(self, *field_names):
1081         """Return a new QuerySet instance with the ordering changed."""
1082         assert not self.query.is_sliced, \
1083             "Cannot reorder a query once a slice has been taken."
1084         obj = self._chain()
1085         obj.query.clear_ordering(force_empty=False)
1086         obj.query.add_ordering(*field_names)
1087         return obj
1088 
1089     def distinct(self, *field_names):
1090         """
1091         Return a new QuerySet instance that will select only distinct results.
1092         """
1093         assert not self.query.is_sliced, \
1094             "Cannot create distinct fields once a slice has been taken."
1095         obj = self._chain()
1096         obj.query.add_distinct_fields(*field_names)
1097         return obj
1098 
1099     def extra(self, select=None, where=None, params=None, tables=None,
1100               order_by=None, select_params=None):
1101         """Add extra SQL fragments to the query."""
1102         self._not_support_combined_queries('extra')
1103         assert not self.query.is_sliced, \
1104             "Cannot change a query once a slice has been taken"
1105         clone = self._chain()
1106         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1107         return clone
1108 
1109     def reverse(self):
1110         """Reverse the ordering of the QuerySet."""
1111         if self.query.is_sliced:
1112             raise TypeError('Cannot reverse a query once a slice has been taken.')
1113         clone = self._chain()
1114         clone.query.standard_ordering = not clone.query.standard_ordering
1115         return clone
1116 
1117     def defer(self, *fields):
1118         """
1119         Defer the loading of data for certain fields until they are accessed.
1120         Add the set of deferred fields to any existing set of deferred fields.
1121         The only exception to this is if None is passed in as the only
1122         parameter, in which case removal all deferrals.
1123         """
1124         self._not_support_combined_queries('defer')
1125         if self._fields is not None:
1126             raise TypeError("Cannot call defer() after .values() or .values_list()")
1127         clone = self._chain()
1128         if fields == (None,):
1129             clone.query.clear_deferred_loading()
1130         else:
1131             clone.query.add_deferred_loading(fields)
1132         return clone
1133 
1134     def only(self, *fields):
1135         """
1136         Essentially, the opposite of defer(). Only the fields passed into this
1137         method and that are not already specified as deferred are loaded
1138         immediately when the queryset is evaluated.
1139         """
1140         self._not_support_combined_queries('only')
1141         if self._fields is not None:
1142             raise TypeError("Cannot call only() after .values() or .values_list()")
1143         if fields == (None,):
1144             # Can only pass None to defer(), not only(), as the rest option.
1145             # That won't stop people trying to do this, so let's be explicit.
1146             raise TypeError("Cannot pass None as an argument to only().")
1147         for field in fields:
1148             field = field.split(LOOKUP_SEP, 1)[0]
1149             if field in self.query._filtered_relations:
1150                 raise ValueError('only() is not supported with FilteredRelation.')
1151         clone = self._chain()
1152         clone.query.add_immediate_loading(fields)
1153         return clone
1154 
1155     def using(self, alias):
1156         """Select which database this QuerySet should execute against."""
1157         clone = self._chain()
1158         clone._db = alias
1159         return clone
1160 
1161     ###################################
1162     # PUBLIC INTROSPECTION ATTRIBUTES #
1163     ###################################
1164 
1165     @property
1166     def ordered(self):
1167         """
1168         Return True if the QuerySet is ordered -- i.e. has an order_by()
1169         clause or a default ordering on the model (or is empty).
1170         """
1171         if isinstance(self, EmptyQuerySet):
1172             return True
1173         if self.query.extra_order_by or self.query.order_by:
1174             return True
1175         elif self.query.default_ordering and self.query.get_meta().ordering:
1176             return True
1177         else:
1178             return False
1179 
1180     @property
1181     def db(self):
1182         """Return the database used if this query is executed now."""
1183         if self._for_write:
1184             return self._db or router.db_for_write(self.model, **self._hints)
1185         return self._db or router.db_for_read(self.model, **self._hints)
1186 
1187     ###################
1188     # PRIVATE METHODS #
1189     ###################
1190 
1191     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1192         """
1193         Insert a new record for the given model. This provides an interface to
1194         the InsertQuery class and is how Model.save() is implemented.
1195         """
1196         self._for_write = True
1197         if using is None:
1198             using = self.db
1199         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1200         query.insert_values(fields, objs, raw=raw)
1201         return query.get_compiler(using=using).execute_sql(returning_fields)
1202     _insert.alters_data = True
1203     _insert.queryset_only = False
1204 
1205     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1206         """
1207         Helper method for bulk_create() to insert objs one batch at a time.
1208         """
1209         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1210             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1211         ops = connections[self.db].ops
1212         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1213         inserted_rows = []
1214         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1215         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1216             if bulk_return and not ignore_conflicts:
1217                 inserted_columns = self._insert(
1218                     item, fields=fields, using=self.db,
1219                     returning_fields=self.model._meta.db_returning_fields,
1220                     ignore_conflicts=ignore_conflicts,
1221                 )
1222                 if isinstance(inserted_columns, list):
1223                     inserted_rows.extend(inserted_columns)
1224                 else:
1225                     inserted_rows.append(inserted_columns)
1226             else:
1227                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1228         return inserted_rows
1229 
1230     def _chain(self, **kwargs):
1231         """
1232         Return a copy of the current QuerySet that's ready for another
1233         operation.
1234         """
1235         obj = self._clone()
1236         if obj._sticky_filter:
1237             obj.query.filter_is_sticky = True
1238             obj._sticky_filter = False
1239         obj.__dict__.update(kwargs)
1240         return obj
1241 
1242     def _clone(self):
1243         """
1244         Return a copy of the current QuerySet. A lightweight alternative
1245         to deepcopy().
1246         """
1247         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1248         c._sticky_filter = self._sticky_filter
1249         c._for_write = self._for_write
1250         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1251         c._known_related_objects = self._known_related_objects
1252         c._iterable_class = self._iterable_class
1253         c._fields = self._fields
1254         return c
1255 
1256     def _fetch_all(self):
1257         if self._result_cache is None:
1258             self._result_cache = list(self._iterable_class(self))
1259         if self._prefetch_related_lookups and not self._prefetch_done:
1260             self._prefetch_related_objects()
1261 
1262     def _next_is_sticky(self):
1263         """
1264         Indicate that the next filter call and the one following that should
1265         be treated as a single filter. This is only important when it comes to
1266         determining when to reuse tables for many-to-many filters. Required so
1267         that we can filter naturally on the results of related managers.
1268 
1269         This doesn't return a clone of the current QuerySet (it returns
1270         "self"). The method is only used internally and should be immediately
1271         followed by a filter() that does create a clone.
1272         """
1273         self._sticky_filter = True
1274         return self
1275 
1276     def _merge_sanity_check(self, other):
1277         """Check that two QuerySet classes may be merged."""
1278         if self._fields is not None and (
1279                 set(self.query.values_select) != set(other.query.values_select) or
1280                 set(self.query.extra_select) != set(other.query.extra_select) or
1281                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1282             raise TypeError(
1283                 "Merging '%s' classes must involve the same values in each case."
1284                 % self.__class__.__name__
1285             )
1286 
1287     def _merge_known_related_objects(self, other):
1288         """
1289         Keep track of all known related objects from either QuerySet instance.
1290         """
1291         for field, objects in other._known_related_objects.items():
1292             self._known_related_objects.setdefault(field, {}).update(objects)
1293 
1294     def resolve_expression(self, *args, **kwargs):
1295         if self._fields and len(self._fields) > 1:
1296             # values() queryset can only be used as nested queries
1297             # if they are set up to select only a single field.
1298             raise TypeError('Cannot use multi-field values as a filter value.')
1299         query = self.query.resolve_expression(*args, **kwargs)
1300         query._db = self._db
1301         return query
1302     resolve_expression.queryset_only = True
1303 
1304     def _add_hints(self, **hints):
1305         """
1306         Update hinting information for use by routers. Add new key/values or
1307         overwrite existing key/values.
1308         """
1309         self._hints.update(hints)
1310 
1311     def _has_filters(self):
1312         """
1313         Check if this QuerySet has any filtering going on. This isn't
1314         equivalent with checking if all objects are present in results, for
1315         example, qs[1:]._has_filters() -> False.
1316         """
1317         return self.query.has_filters()
1318 
1319     @staticmethod
1320     def _validate_values_are_expressions(values, method_name):
1321         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1322         if invalid_args:
1323             raise TypeError(
1324                 'QuerySet.%s() received non-expression(s): %s.' % (
1325                     method_name,
1326                     ', '.join(invalid_args),
1327                 )
1328             )
1329 
1330     def _not_support_combined_queries(self, operation_name):
1331         if self.query.combinator:
1332             raise NotSupportedError(
1333                 'Calling QuerySet.%s() after %s() is not supported.'
1334                 % (operation_name, self.query.combinator)
1335             )

</code><intended_behavior>Inside the `filter` and `_filter_or_exclude` methods, whenever a `__isnull` lookup is encountered, introduce a validation check to assert that the value associated with `__isnull` is a boolean (`True` or `False`). If a non-boolean value is detected, the method should raise a `ValueError` with a clear message indicating that `__isnull` only accepts boolean values. This preserves the existing functionality of allowing to filter querysets based on whether a field is `NULL` or not, but enforces correct usage and prevents the misuse of the lookup with unintended value types.</intended_behavior>, <file>django/db/backends/mysql/compiler.py</file>
<class>SQLCompiler</class>
<method>None</method>
<code>
4 class SQLCompiler(compiler.SQLCompiler):
5     def as_subquery_condition(self, alias, columns, compiler):
6         qn = compiler.quote_name_unless_alias
7         qn2 = self.connection.ops.quote_name
8         sql, params = self.as_sql()
9         return '(%s) IN (%s)' % (', '.join('%s.%s' % (qn(alias), qn2(column)) for column in columns), sql), params

</code><intended_behavior>While not directly modifying this class for the boolean validation, ensuring that the `SQLCompiler` has access to the validation logic or error raised in `QuerySet`'s methods might help with broader error handling and messaging. The actual change would be in the queryset filtering logic, but attention should be given here to ensure that errors are gracefully handled and correctly propagate up to developers when incorrect `__isnull` usage is attempted. The intended behavior is not so much an alteration of `SQLCompiler` but an acknowledgment that changes in `QuerySet` logic may affect how SQL compilation errors are caught and reported.</intended_behavior>, <file>django/db/models/sql/compiler.py</file>
<class>SQLCompiler</class>
<method>None</method>
<code>
20 class SQLCompiler:
21     def __init__(self, query, connection, using):
22         self.query = query
23         self.connection = connection
24         self.using = using
25         self.quote_cache = {'*': '*'}
26         # The select, klass_info, and annotations are needed by QuerySet.iterator()
27         # these are set as a side-effect of executing the query. Note that we calculate
28         # separately a list of extra select columns needed for grammatical correctness
29         # of the query, but these columns are not included in self.select.
30         self.select = None
31         self.annotation_col_map = None
32         self.klass_info = None
33         # Multiline ordering SQL clause may appear from RawSQL.
34         self.ordering_parts = re.compile(r'^(.*)\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)
35         self._meta_ordering = None
36 
37     def setup_query(self):
38         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):
39             self.query.get_initial_alias()
40         self.select, self.klass_info, self.annotation_col_map = self.get_select()
41         self.col_count = len(self.select)
42 
43     def pre_sql_setup(self):
44         """
45         Do any necessary class setup immediately prior to producing SQL. This
46         is for things that can't necessarily be done in __init__ because we
47         might not have all the pieces in place at that time.
48         """
49         self.setup_query()
50         order_by = self.get_order_by()
51         self.where, self.having = self.query.where.split_having()
52         extra_select = self.get_extra_select(order_by, self.select)
53         self.has_extra_select = bool(extra_select)
54         group_by = self.get_group_by(self.select + extra_select, order_by)
55         return extra_select, order_by, group_by
56 
57     def get_group_by(self, select, order_by):
58         """
59         Return a list of 2-tuples of form (sql, params).
60 
61         The logic of what exactly the GROUP BY clause contains is hard
62         to describe in other words than "if it passes the test suite,
63         then it is correct".
64         """
65         # Some examples:
66         #     SomeModel.objects.annotate(Count('somecol'))
67         #     GROUP BY: all fields of the model
68         #
69         #    SomeModel.objects.values('name').annotate(Count('somecol'))
70         #    GROUP BY: name
71         #
72         #    SomeModel.objects.annotate(Count('somecol')).values('name')
73         #    GROUP BY: all cols of the model
74         #
75         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')
76         #    GROUP BY: name, pk
77         #
78         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')
79         #    GROUP BY: name, pk
80         #
81         # In fact, the self.query.group_by is the minimal set to GROUP BY. It
82         # can't be ever restricted to a smaller set, but additional columns in
83         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately
84         # the end result is that it is impossible to force the query to have
85         # a chosen GROUP BY clause - you can almost do this by using the form:
86         #     .values(*wanted_cols).annotate(AnAggregate())
87         # but any later annotations, extra selects, values calls that
88         # refer some column outside of the wanted_cols, order_by, or even
89         # filter calls can alter the GROUP BY clause.
90 
91         # The query.group_by is either None (no GROUP BY at all), True
92         # (group by select fields), or a list of expressions to be added
93         # to the group by.
94         if self.query.group_by is None:
95             return []
96         expressions = []
97         if self.query.group_by is not True:
98             # If the group by is set to a list (by .values() call most likely),
99             # then we need to add everything in it to the GROUP BY clause.
100             # Backwards compatibility hack for setting query.group_by. Remove
101             # when  we have public API way of forcing the GROUP BY clause.
102             # Converts string references to expressions.
103             for expr in self.query.group_by:
104                 if not hasattr(expr, 'as_sql'):
105                     expressions.append(self.query.resolve_ref(expr))
106                 else:
107                     expressions.append(expr)
108         # Note that even if the group_by is set, it is only the minimal
109         # set to group by. So, we need to add cols in select, order_by, and
110         # having into the select in any case.
111         for expr, _, _ in select:
112             cols = expr.get_group_by_cols()
113             for col in cols:
114                 expressions.append(col)
115         for expr, (sql, params, is_ref) in order_by:
116             # Skip References to the select clause, as all expressions in the
117             # select clause are already part of the group by.
118             if not expr.contains_aggregate and not is_ref:
119                 expressions.extend(expr.get_source_expressions())
120         having_group_by = self.having.get_group_by_cols() if self.having else ()
121         for expr in having_group_by:
122             expressions.append(expr)
123         result = []
124         seen = set()
125         expressions = self.collapse_group_by(expressions, having_group_by)
126 
127         for expr in expressions:
128             sql, params = self.compile(expr)
129             params_hash = make_hashable(params)
130             if (sql, params_hash) not in seen:
131                 result.append((sql, params))
132                 seen.add((sql, params_hash))
133         return result
134 
135     def collapse_group_by(self, expressions, having):
136         # If the DB can group by primary key, then group by the primary key of
137         # query's main model. Note that for PostgreSQL the GROUP BY clause must
138         # include the primary key of every table, but for MySQL it is enough to
139         # have the main table's primary key.
140         if self.connection.features.allows_group_by_pk:
141             # Determine if the main model's primary key is in the query.
142             pk = None
143             for expr in expressions:
144                 # Is this a reference to query's base table primary key? If the
145                 # expression isn't a Col-like, then skip the expression.
146                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and
147                         getattr(expr, 'alias', None) == self.query.base_table):
148                     pk = expr
149                     break
150             # If the main model's primary key is in the query, group by that
151             # field, HAVING expressions, and expressions associated with tables
152             # that don't have a primary key included in the grouped columns.
153             if pk:
154                 pk_aliases = {
155                     expr.alias for expr in expressions
156                     if hasattr(expr, 'target') and expr.target.primary_key
157                 }
158                 expressions = [pk] + [
159                     expr for expr in expressions
160                     if expr in having or (
161                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases
162                     )
163                 ]
164         elif self.connection.features.allows_group_by_selected_pks:
165             # Filter out all expressions associated with a table's primary key
166             # present in the grouped columns. This is done by identifying all
167             # tables that have their primary key included in the grouped
168             # columns and removing non-primary key columns referring to them.
169             # Unmanaged models are excluded because they could be representing
170             # database views on which the optimization might not be allowed.
171             pks = {
172                 expr for expr in expressions
173                 if (
174                     hasattr(expr, 'target') and
175                     expr.target.primary_key and
176                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
177                 )
178             }
179             aliases = {expr.alias for expr in pks}
180             expressions = [
181                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases
182             ]
183         return expressions
184 
185     def get_select(self):
186         """
187         Return three values:
188         - a list of 3-tuples of (expression, (sql, params), alias)
189         - a klass_info structure,
190         - a dictionary of annotations
191 
192         The (sql, params) is what the expression will produce, and alias is the
193         "AS alias" for the column (possibly None).
194 
195         The klass_info structure contains the following information:
196         - The base model of the query.
197         - Which columns for that model are present in the query (by
198           position of the select clause).
199         - related_klass_infos: [f, klass_info] to descent into
200 
201         The annotations is a dictionary of {'attname': column position} values.
202         """
203         select = []
204         klass_info = None
205         annotations = {}
206         select_idx = 0
207         for alias, (sql, params) in self.query.extra_select.items():
208             annotations[alias] = select_idx
209             select.append((RawSQL(sql, params), alias))
210             select_idx += 1
211         assert not (self.query.select and self.query.default_cols)
212         if self.query.default_cols:
213             cols = self.get_default_columns()
214         else:
215             # self.query.select is a special case. These columns never go to
216             # any model.
217             cols = self.query.select
218         if cols:
219             select_list = []
220             for col in cols:
221                 select_list.append(select_idx)
222                 select.append((col, None))
223                 select_idx += 1
224             klass_info = {
225                 'model': self.query.model,
226                 'select_fields': select_list,
227             }
228         for alias, annotation in self.query.annotation_select.items():
229             annotations[alias] = select_idx
230             select.append((annotation, alias))
231             select_idx += 1
232 
233         if self.query.select_related:
234             related_klass_infos = self.get_related_selections(select)
235             klass_info['related_klass_infos'] = related_klass_infos
236 
237             def get_select_from_parent(klass_info):
238                 for ki in klass_info['related_klass_infos']:
239                     if ki['from_parent']:
240                         ki['select_fields'] = (klass_info['select_fields'] +
241                                                ki['select_fields'])
242                     get_select_from_parent(ki)
243             get_select_from_parent(klass_info)
244 
245         ret = []
246         for col, alias in select:
247             try:
248                 sql, params = self.compile(col)
249             except EmptyResultSet:
250                 # Select a predicate that's always False.
251                 sql, params = '0', ()
252             else:
253                 sql, params = col.select_format(self, sql, params)
254             ret.append((col, (sql, params), alias))
255         return ret, klass_info, annotations
256 
257     def get_order_by(self):
258         """
259         Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the
260         ORDER BY clause.
261 
262         The order_by clause can alter the select clause (for example it
263         can add aliases to clauses that do not yet have one, or it can
264         add totally new select clauses).
265         """
266         if self.query.extra_order_by:
267             ordering = self.query.extra_order_by
268         elif not self.query.default_ordering:
269             ordering = self.query.order_by
270         elif self.query.order_by:
271             ordering = self.query.order_by
272         elif self.query.get_meta().ordering:
273             ordering = self.query.get_meta().ordering
274             self._meta_ordering = ordering
275         else:
276             ordering = []
277         if self.query.standard_ordering:
278             asc, desc = ORDER_DIR['ASC']
279         else:
280             asc, desc = ORDER_DIR['DESC']
281 
282         order_by = []
283         for field in ordering:
284             if hasattr(field, 'resolve_expression'):
285                 if isinstance(field, Value):
286                     # output_field must be resolved for constants.
287                     field = Cast(field, field.output_field)
288                 if not isinstance(field, OrderBy):
289                     field = field.asc()
290                 if not self.query.standard_ordering:
291                     field = field.copy()
292                     field.reverse_ordering()
293                 order_by.append((field, False))
294                 continue
295             if field == '?':  # random
296                 order_by.append((OrderBy(Random()), False))
297                 continue
298 
299             col, order = get_order_dir(field, asc)
300             descending = order == 'DESC'
301 
302             if col in self.query.annotation_select:
303                 # Reference to expression in SELECT clause
304                 order_by.append((
305                     OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),
306                     True))
307                 continue
308             if col in self.query.annotations:
309                 # References to an expression which is masked out of the SELECT
310                 # clause.
311                 expr = self.query.annotations[col]
312                 if isinstance(expr, Value):
313                     # output_field must be resolved for constants.
314                     expr = Cast(expr, expr.output_field)
315                 order_by.append((OrderBy(expr, descending=descending), False))
316                 continue
317 
318             if '.' in field:
319                 # This came in through an extra(order_by=...) addition. Pass it
320                 # on verbatim.
321                 table, col = col.split('.', 1)
322                 order_by.append((
323                     OrderBy(
324                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),
325                         descending=descending
326                     ), False))
327                 continue
328 
329             if not self.query.extra or col not in self.query.extra:
330                 # 'col' is of the form 'field' or 'field1__field2' or
331                 # '-field1__field2__field', etc.
332                 order_by.extend(self.find_ordering_name(
333                     field, self.query.get_meta(), default_order=asc))
334             else:
335                 if col not in self.query.extra_select:
336                     order_by.append((
337                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),
338                         False))
339                 else:
340                     order_by.append((
341                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),
342                         True))
343         result = []
344         seen = set()
345 
346         for expr, is_ref in order_by:
347             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
348             if self.query.combinator:
349                 src = resolved.get_source_expressions()[0]
350                 # Relabel order by columns to raw numbers if this is a combined
351                 # query; necessary since the columns can't be referenced by the
352                 # fully qualified name and the simple column names may collide.
353                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
354                     if is_ref and col_alias == src.refs:
355                         src = src.source
356                     elif col_alias:
357                         continue
358                     if src == sel_expr:
359                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
360                         break
361                 else:
362                     if col_alias:
363                         raise DatabaseError('ORDER BY term does not match any column in the result set.')
364                     # Add column used in ORDER BY clause without an alias to
365                     # the selected columns.
366                     self.query.add_select_col(src)
367                     resolved.set_source_expressions([RawSQL('%d' % len(self.query.select), ())])
368             sql, params = self.compile(resolved)
369             # Don't add the same column twice, but the order direction is
370             # not taken into account so we strip it. When this entire method
371             # is refactored into expressions, then we can check each part as we
372             # generate it.
373             without_ordering = self.ordering_parts.search(sql).group(1)
374             params_hash = make_hashable(params)
375             if (without_ordering, params_hash) in seen:
376                 continue
377             seen.add((without_ordering, params_hash))
378             result.append((resolved, (sql, params, is_ref)))
379         return result
380 
381     def get_extra_select(self, order_by, select):
382         extra_select = []
383         if self.query.distinct and not self.query.distinct_fields:
384             select_sql = [t[1] for t in select]
385             for expr, (sql, params, is_ref) in order_by:
386                 without_ordering = self.ordering_parts.search(sql).group(1)
387                 if not is_ref and (without_ordering, params) not in select_sql:
388                     extra_select.append((expr, (without_ordering, params), None))
389         return extra_select
390 
391     def quote_name_unless_alias(self, name):
392         """
393         A wrapper around connection.ops.quote_name that doesn't quote aliases
394         for table names. This avoids problems with some SQL dialects that treat
395         quoted strings specially (e.g. PostgreSQL).
396         """
397         if name in self.quote_cache:
398             return self.quote_cache[name]
399         if ((name in self.query.alias_map and name not in self.query.table_map) or
400                 name in self.query.extra_select or (
401                     name in self.query.external_aliases and name not in self.query.table_map)):
402             self.quote_cache[name] = name
403             return name
404         r = self.connection.ops.quote_name(name)
405         self.quote_cache[name] = r
406         return r
407 
408     def compile(self, node):
409         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
410         if vendor_impl:
411             sql, params = vendor_impl(self, self.connection)
412         else:
413             sql, params = node.as_sql(self, self.connection)
414         return sql, params
415 
416     def get_combinator_sql(self, combinator, all):
417         features = self.connection.features
418         compilers = [
419             query.get_compiler(self.using, self.connection)
420             for query in self.query.combined_queries if not query.is_empty()
421         ]
422         if not features.supports_slicing_ordering_in_compound:
423             for query, compiler in zip(self.query.combined_queries, compilers):
424                 if query.low_mark or query.high_mark:
425                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')
426                 if compiler.get_order_by():
427                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')
428         parts = ()
429         for compiler in compilers:
430             try:
431                 # If the columns list is limited, then all combined queries
432                 # must have the same columns list. Set the selects defined on
433                 # the query on all combined queries, if not already set.
434                 if not compiler.query.values_select and self.query.values_select:
435                     compiler.query = compiler.query.clone()
436                     compiler.query.set_values((
437                         *self.query.extra_select,
438                         *self.query.values_select,
439                         *self.query.annotation_select,
440                     ))
441                 part_sql, part_args = compiler.as_sql()
442                 if compiler.query.combinator:
443                     # Wrap in a subquery if wrapping in parentheses isn't
444                     # supported.
445                     if not features.supports_parentheses_in_compound:
446                         part_sql = 'SELECT * FROM ({})'.format(part_sql)
447                     # Add parentheses when combining with compound query if not
448                     # already added for all compound queries.
449                     elif not features.supports_slicing_ordering_in_compound:
450                         part_sql = '({})'.format(part_sql)
451                 parts += ((part_sql, part_args),)
452             except EmptyResultSet:
453                 # Omit the empty queryset with UNION and with DIFFERENCE if the
454                 # first queryset is nonempty.
455                 if combinator == 'union' or (combinator == 'difference' and parts):
456                     continue
457                 raise
458         if not parts:
459             raise EmptyResultSet
460         combinator_sql = self.connection.ops.set_operators[combinator]
461         if all and combinator == 'union':
462             combinator_sql += ' ALL'
463         braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'
464         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))
465         result = [' {} '.format(combinator_sql).join(sql_parts)]
466         params = []
467         for part in args_parts:
468             params.extend(part)
469         return result, params
470 
471     def as_sql(self, with_limits=True, with_col_aliases=False):
472         """
473         Create the SQL for this query. Return the SQL string and list of
474         parameters.
475 
476         If 'with_limits' is False, any limit/offset information is not included
477         in the query.
478         """
479         refcounts_before = self.query.alias_refcount.copy()
480         try:
481             extra_select, order_by, group_by = self.pre_sql_setup()
482             for_update_part = None
483             # Is a LIMIT/OFFSET clause needed?
484             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)
485             combinator = self.query.combinator
486             features = self.connection.features
487             if combinator:
488                 if not getattr(features, 'supports_select_{}'.format(combinator)):
489                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))
490                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)
491             else:
492                 distinct_fields, distinct_params = self.get_distinct()
493                 # This must come after 'select', 'ordering', and 'distinct'
494                 # (see docstring of get_from_clause() for details).
495                 from_, f_params = self.get_from_clause()
496                 where, w_params = self.compile(self.where) if self.where is not None else ("", [])
497                 having, h_params = self.compile(self.having) if self.having is not None else ("", [])
498                 result = ['SELECT']
499                 params = []
500 
501                 if self.query.distinct:
502                     distinct_result, distinct_params = self.connection.ops.distinct_sql(
503                         distinct_fields,
504                         distinct_params,
505                     )
506                     result += distinct_result
507                     params += distinct_params
508 
509                 out_cols = []
510                 col_idx = 1
511                 for _, (s_sql, s_params), alias in self.select + extra_select:
512                     if alias:
513                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))
514                     elif with_col_aliases:
515                         s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)
516                         col_idx += 1
517                     params.extend(s_params)
518                     out_cols.append(s_sql)
519 
520                 result += [', '.join(out_cols), 'FROM', *from_]
521                 params.extend(f_params)
522 
523                 if self.query.select_for_update and self.connection.features.has_select_for_update:
524                     if self.connection.get_autocommit():
525                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')
526 
527                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:
528                         raise NotSupportedError(
529                             'LIMIT/OFFSET is not supported with '
530                             'select_for_update on this database backend.'
531                         )
532                     nowait = self.query.select_for_update_nowait
533                     skip_locked = self.query.select_for_update_skip_locked
534                     of = self.query.select_for_update_of
535                     # If it's a NOWAIT/SKIP LOCKED/OF query but the backend
536                     # doesn't support it, raise NotSupportedError to prevent a
537                     # possible deadlock.
538                     if nowait and not self.connection.features.has_select_for_update_nowait:
539                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
540                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
541                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
542                     elif of and not self.connection.features.has_select_for_update_of:
543                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
544                     for_update_part = self.connection.ops.for_update_sql(
545                         nowait=nowait,
546                         skip_locked=skip_locked,
547                         of=self.get_select_for_update_of_arguments(),
548                     )
549 
550                 if for_update_part and self.connection.features.for_update_after_from:
551                     result.append(for_update_part)
552 
553                 if where:
554                     result.append('WHERE %s' % where)
555                     params.extend(w_params)
556 
557                 grouping = []
558                 for g_sql, g_params in group_by:
559                     grouping.append(g_sql)
560                     params.extend(g_params)
561                 if grouping:
562                     if distinct_fields:
563                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')
564                     order_by = order_by or self.connection.ops.force_no_ordering()
565                     result.append('GROUP BY %s' % ', '.join(grouping))
566                     if self._meta_ordering:
567                         order_by = None
568                 if having:
569                     result.append('HAVING %s' % having)
570                     params.extend(h_params)
571 
572             if self.query.explain_query:
573                 result.insert(0, self.connection.ops.explain_query_prefix(
574                     self.query.explain_format,
575                     **self.query.explain_options
576                 ))
577 
578             if order_by:
579                 ordering = []
580                 for _, (o_sql, o_params, _) in order_by:
581                     ordering.append(o_sql)
582                     params.extend(o_params)
583                 result.append('ORDER BY %s' % ', '.join(ordering))
584 
585             if with_limit_offset:
586                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))
587 
588             if for_update_part and not self.connection.features.for_update_after_from:
589                 result.append(for_update_part)
590 
591             if self.query.subquery and extra_select:
592                 # If the query is used as a subquery, the extra selects would
593                 # result in more columns than the left-hand side expression is
594                 # expecting. This can happen when a subquery uses a combination
595                 # of order_by() and distinct(), forcing the ordering expressions
596                 # to be selected as well. Wrap the query in another subquery
597                 # to exclude extraneous selects.
598                 sub_selects = []
599                 sub_params = []
600                 for index, (select, _, alias) in enumerate(self.select, start=1):
601                     if not alias and with_col_aliases:
602                         alias = 'col%d' % index
603                     if alias:
604                         sub_selects.append("%s.%s" % (
605                             self.connection.ops.quote_name('subquery'),
606                             self.connection.ops.quote_name(alias),
607                         ))
608                     else:
609                         select_clone = select.relabeled_clone({select.alias: 'subquery'})
610                         subselect, subparams = select_clone.as_sql(self, self.connection)
611                         sub_selects.append(subselect)
612                         sub_params.extend(subparams)
613                 return 'SELECT %s FROM (%s) subquery' % (
614                     ', '.join(sub_selects),
615                     ' '.join(result),
616                 ), tuple(sub_params + params)
617 
618             return ' '.join(result), tuple(params)
619         finally:
620             # Finally do cleanup - get rid of the joins we created above.
621             self.query.reset_refcounts(refcounts_before)
622 
623     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):
624         """
625         Compute the default columns for selecting every field in the base
626         model. Will sometimes be called to pull in related models (e.g. via
627         select_related), in which case "opts" and "start_alias" will be given
628         to provide a starting point for the traversal.
629 
630         Return a list of strings, quoted appropriately for use in SQL
631         directly, as well as a set of aliases used in the select statement (if
632         'as_pairs' is True, return a list of (alias, col_name) pairs instead
633         of strings as the first component and None as the second component).
634         """
635         result = []
636         if opts is None:
637             opts = self.query.get_meta()
638         only_load = self.deferred_to_columns()
639         start_alias = start_alias or self.query.get_initial_alias()
640         # The 'seen_models' is used to optimize checking the needed parent
641         # alias for a given field. This also includes None -> start_alias to
642         # be used by local fields.
643         seen_models = {None: start_alias}
644 
645         for field in opts.concrete_fields:
646             model = field.model._meta.concrete_model
647             # A proxy model will have a different model and concrete_model. We
648             # will assign None if the field belongs to this model.
649             if model == opts.model:
650                 model = None
651             if from_parent and model is not None and issubclass(
652                     from_parent._meta.concrete_model, model._meta.concrete_model):
653                 # Avoid loading data for already loaded parents.
654                 # We end up here in the case select_related() resolution
655                 # proceeds from parent model to child model. In that case the
656                 # parent model data is already present in the SELECT clause,
657                 # and we want to avoid reloading the same data again.
658                 continue
659             if field.model in only_load and field.attname not in only_load[field.model]:
660                 continue
661             alias = self.query.join_parent_model(opts, model, start_alias,
662                                                  seen_models)
663             column = field.get_col(alias)
664             result.append(column)
665         return result
666 
667     def get_distinct(self):
668         """
669         Return a quoted list of fields to use in DISTINCT ON part of the query.
670 
671         This method can alter the tables in the query, and thus it must be
672         called before get_from_clause().
673         """
674         result = []
675         params = []
676         opts = self.query.get_meta()
677 
678         for name in self.query.distinct_fields:
679             parts = name.split(LOOKUP_SEP)
680             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)
681             targets, alias, _ = self.query.trim_joins(targets, joins, path)
682             for target in targets:
683                 if name in self.query.annotation_select:
684                     result.append(name)
685                 else:
686                     r, p = self.compile(transform_function(target, alias))
687                     result.append(r)
688                     params.append(p)
689         return result, params
690 
691     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',
692                            already_seen=None):
693         """
694         Return the table alias (the name might be ambiguous, the alias will
695         not be) and column name for ordering by the given 'name' parameter.
696         The 'name' is of the form 'field1__field2__...__fieldN'.
697         """
698         name, order = get_order_dir(name, default_order)
699         descending = order == 'DESC'
700         pieces = name.split(LOOKUP_SEP)
701         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)
702 
703         # If we get to this point and the field is a relation to another model,
704         # append the default ordering for that model unless the attribute name
705         # of the field is specified.
706         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:
707             # Firstly, avoid infinite loops.
708             already_seen = already_seen or set()
709             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
710             if join_tuple in already_seen:
711                 raise FieldError('Infinite loop caused by ordering.')
712             already_seen.add(join_tuple)
713 
714             results = []
715             for item in opts.ordering:
716                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
717                     item = item.desc() if descending else item.asc()
718                 if isinstance(item, OrderBy):
719                     results.append((item, False))
720                     continue
721                 results.extend(self.find_ordering_name(item, opts, alias,
722                                                        order, already_seen))
723             return results
724         targets, alias, _ = self.query.trim_joins(targets, joins, path)
725         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]
726 
727     def _setup_joins(self, pieces, opts, alias):
728         """
729         Helper method for get_order_by() and get_distinct().
730 
731         get_ordering() and get_distinct() must produce same target columns on
732         same input, as the prefixes of get_ordering() and get_distinct() must
733         match. Executing SQL where this is not true is an error.
734         """
735         alias = alias or self.query.get_initial_alias()
736         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)
737         alias = joins[-1]
738         return field, targets, alias, joins, path, opts, transform_function
739 
740     def get_from_clause(self):
741         """
742         Return a list of strings that are joined together to go after the
743         "FROM" part of the query, as well as a list any extra parameters that
744         need to be included. Subclasses, can override this to create a
745         from-clause via a "select".
746 
747         This should only be called after any SQL construction methods that
748         might change the tables that are needed. This means the select columns,
749         ordering, and distinct must be done first.
750         """
751         result = []
752         params = []
753         for alias in tuple(self.query.alias_map):
754             if not self.query.alias_refcount[alias]:
755                 continue
756             try:
757                 from_clause = self.query.alias_map[alias]
758             except KeyError:
759                 # Extra tables can end up in self.tables, but not in the
760                 # alias_map if they aren't in a join. That's OK. We skip them.
761                 continue
762             clause_sql, clause_params = self.compile(from_clause)
763             result.append(clause_sql)
764             params.extend(clause_params)
765         for t in self.query.extra_tables:
766             alias, _ = self.query.table_alias(t)
767             # Only add the alias if it's not already present (the table_alias()
768             # call increments the refcount, so an alias refcount of one means
769             # this is the only reference).
770             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:
771                 result.append(', %s' % self.quote_name_unless_alias(alias))
772         return result, params
773 
774     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,
775                                requested=None, restricted=None):
776         """
777         Fill in the information needed for a select_related query. The current
778         depth is measured as the number of connections away from the root model
779         (for example, cur_depth=1 means we are looking at models with direct
780         connections to the root model).
781         """
782         def _get_field_choices():
783             direct_choices = (f.name for f in opts.fields if f.is_relation)
784             reverse_choices = (
785                 f.field.related_query_name()
786                 for f in opts.related_objects if f.field.unique
787             )
788             return chain(direct_choices, reverse_choices, self.query._filtered_relations)
789 
790         related_klass_infos = []
791         if not restricted and cur_depth > self.query.max_depth:
792             # We've recursed far enough; bail out.
793             return related_klass_infos
794 
795         if not opts:
796             opts = self.query.get_meta()
797             root_alias = self.query.get_initial_alias()
798         only_load = self.query.get_loaded_field_names()
799 
800         # Setup for the case when only particular related fields should be
801         # included in the related selection.
802         fields_found = set()
803         if requested is None:
804             restricted = isinstance(self.query.select_related, dict)
805             if restricted:
806                 requested = self.query.select_related
807 
808         def get_related_klass_infos(klass_info, related_klass_infos):
809             klass_info['related_klass_infos'] = related_klass_infos
810 
811         for f in opts.fields:
812             field_model = f.model._meta.concrete_model
813             fields_found.add(f.name)
814 
815             if restricted:
816                 next = requested.get(f.name, {})
817                 if not f.is_relation:
818                     # If a non-related field is used like a relation,
819                     # or if a single non-relational field is given.
820                     if next or f.name in requested:
821                         raise FieldError(
822                             "Non-relational field given in select_related: '%s'. "
823                             "Choices are: %s" % (
824                                 f.name,
825                                 ", ".join(_get_field_choices()) or '(none)',
826                             )
827                         )
828             else:
829                 next = False
830 
831             if not select_related_descend(f, restricted, requested,
832                                           only_load.get(field_model)):
833                 continue
834             klass_info = {
835                 'model': f.remote_field.model,
836                 'field': f,
837                 'reverse': False,
838                 'local_setter': f.set_cached_value,
839                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,
840                 'from_parent': False,
841             }
842             related_klass_infos.append(klass_info)
843             select_fields = []
844             _, _, _, joins, _, _ = self.query.setup_joins(
845                 [f.name], opts, root_alias)
846             alias = joins[-1]
847             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)
848             for col in columns:
849                 select_fields.append(len(select))
850                 select.append((col, None))
851             klass_info['select_fields'] = select_fields
852             next_klass_infos = self.get_related_selections(
853                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)
854             get_related_klass_infos(klass_info, next_klass_infos)
855 
856         if restricted:
857             related_fields = [
858                 (o.field, o.related_model)
859                 for o in opts.related_objects
860                 if o.field.unique and not o.many_to_many
861             ]
862             for f, model in related_fields:
863                 if not select_related_descend(f, restricted, requested,
864                                               only_load.get(model), reverse=True):
865                     continue
866 
867                 related_field_name = f.related_query_name()
868                 fields_found.add(related_field_name)
869 
870                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)
871                 alias = join_info.joins[-1]
872                 from_parent = issubclass(model, opts.model) and model is not opts.model
873                 klass_info = {
874                     'model': model,
875                     'field': f,
876                     'reverse': True,
877                     'local_setter': f.remote_field.set_cached_value,
878                     'remote_setter': f.set_cached_value,
879                     'from_parent': from_parent,
880                 }
881                 related_klass_infos.append(klass_info)
882                 select_fields = []
883                 columns = self.get_default_columns(
884                     start_alias=alias, opts=model._meta, from_parent=opts.model)
885                 for col in columns:
886                     select_fields.append(len(select))
887                     select.append((col, None))
888                 klass_info['select_fields'] = select_fields
889                 next = requested.get(f.related_query_name(), {})
890                 next_klass_infos = self.get_related_selections(
891                     select, model._meta, alias, cur_depth + 1,
892                     next, restricted)
893                 get_related_klass_infos(klass_info, next_klass_infos)
894 
895             def local_setter(obj, from_obj):
896                 # Set a reverse fk object when relation is non-empty.
897                 if from_obj:
898                     f.remote_field.set_cached_value(from_obj, obj)
899 
900             def remote_setter(name, obj, from_obj):
901                 setattr(from_obj, name, obj)
902 
903             for name in list(requested):
904                 # Filtered relations work only on the topmost level.
905                 if cur_depth > 1:
906                     break
907                 if name in self.query._filtered_relations:
908                     fields_found.add(name)
909                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)
910                     model = join_opts.model
911                     alias = joins[-1]
912                     from_parent = issubclass(model, opts.model) and model is not opts.model
913                     klass_info = {
914                         'model': model,
915                         'field': f,
916                         'reverse': True,
917                         'local_setter': local_setter,
918                         'remote_setter': partial(remote_setter, name),
919                         'from_parent': from_parent,
920                     }
921                     related_klass_infos.append(klass_info)
922                     select_fields = []
923                     columns = self.get_default_columns(
924                         start_alias=alias, opts=model._meta,
925                         from_parent=opts.model,
926                     )
927                     for col in columns:
928                         select_fields.append(len(select))
929                         select.append((col, None))
930                     klass_info['select_fields'] = select_fields
931                     next_requested = requested.get(name, {})
932                     next_klass_infos = self.get_related_selections(
933                         select, opts=model._meta, root_alias=alias,
934                         cur_depth=cur_depth + 1, requested=next_requested,
935                         restricted=restricted,
936                     )
937                     get_related_klass_infos(klass_info, next_klass_infos)
938             fields_not_found = set(requested).difference(fields_found)
939             if fields_not_found:
940                 invalid_fields = ("'%s'" % s for s in fields_not_found)
941                 raise FieldError(
942                     'Invalid field name(s) given in select_related: %s. '
943                     'Choices are: %s' % (
944                         ', '.join(invalid_fields),
945                         ', '.join(_get_field_choices()) or '(none)',
946                     )
947                 )
948         return related_klass_infos
949 
950     def get_select_for_update_of_arguments(self):
951         """
952         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
953         the query.
954         """
955         def _get_field_choices():
956             """Yield all allowed field paths in breadth-first search order."""
957             queue = collections.deque([(None, self.klass_info)])
958             while queue:
959                 parent_path, klass_info = queue.popleft()
960                 if parent_path is None:
961                     path = []
962                     yield 'self'
963                 else:
964                     field = klass_info['field']
965                     if klass_info['reverse']:
966                         field = field.remote_field
967                     path = parent_path + [field.name]
968                     yield LOOKUP_SEP.join(path)
969                 queue.extend(
970                     (path, klass_info)
971                     for klass_info in klass_info.get('related_klass_infos', [])
972                 )
973         result = []
974         invalid_names = []
975         for name in self.query.select_for_update_of:
976             parts = [] if name == 'self' else name.split(LOOKUP_SEP)
977             klass_info = self.klass_info
978             for part in parts:
979                 for related_klass_info in klass_info.get('related_klass_infos', []):
980                     field = related_klass_info['field']
981                     if related_klass_info['reverse']:
982                         field = field.remote_field
983                     if field.name == part:
984                         klass_info = related_klass_info
985                         break
986                 else:
987                     klass_info = None
988                     break
989             if klass_info is None:
990                 invalid_names.append(name)
991                 continue
992             select_index = klass_info['select_fields'][0]
993             col = self.select[select_index][0]
994             if self.connection.features.select_for_update_of_column:
995                 result.append(self.compile(col)[0])
996             else:
997                 result.append(self.quote_name_unless_alias(col.alias))
998         if invalid_names:
999             raise FieldError(
1000                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
1001                 'Only relational fields followed in the query are allowed. '
1002                 'Choices are: %s.' % (
1003                     ', '.join(invalid_names),
1004                     ', '.join(_get_field_choices()),
1005                 )
1006             )
1007         return result
1008 
1009     def deferred_to_columns(self):
1010         """
1011         Convert the self.deferred_loading data structure to mapping of table
1012         names to sets of column names which are to be loaded. Return the
1013         dictionary.
1014         """
1015         columns = {}
1016         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)
1017         return columns
1018 
1019     def get_converters(self, expressions):
1020         converters = {}
1021         for i, expression in enumerate(expressions):
1022             if expression:
1023                 backend_converters = self.connection.ops.get_db_converters(expression)
1024                 field_converters = expression.get_db_converters(self.connection)
1025                 if backend_converters or field_converters:
1026                     converters[i] = (backend_converters + field_converters, expression)
1027         return converters
1028 
1029     def apply_converters(self, rows, converters):
1030         connection = self.connection
1031         converters = list(converters.items())
1032         for row in map(list, rows):
1033             for pos, (convs, expression) in converters:
1034                 value = row[pos]
1035                 for converter in convs:
1036                     value = converter(value, expression, connection)
1037                 row[pos] = value
1038             yield row
1039 
1040     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,
1041                      chunk_size=GET_ITERATOR_CHUNK_SIZE):
1042         """Return an iterator over the results from executing this query."""
1043         if results is None:
1044             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)
1045         fields = [s[0] for s in self.select[0:self.col_count]]
1046         converters = self.get_converters(fields)
1047         rows = chain.from_iterable(results)
1048         if converters:
1049             rows = self.apply_converters(rows, converters)
1050             if tuple_expected:
1051                 rows = map(tuple, rows)
1052         return rows
1053 
1054     def has_results(self):
1055         """
1056         Backends (e.g. NoSQL) can override this in order to use optimized
1057         versions of "query has any results."
1058         """
1059         # This is always executed on a query clone, so we can modify self.query
1060         self.query.add_extra({'a': 1}, None, None, None, None, None)
1061         self.query.set_extra_mask(['a'])
1062         return bool(self.execute_sql(SINGLE))
1063 
1064     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
1065         """
1066         Run the query against the database and return the result(s). The
1067         return value is a single data item if result_type is SINGLE, or an
1068         iterator over the results if the result_type is MULTI.
1069 
1070         result_type is either MULTI (use fetchmany() to retrieve all rows),
1071         SINGLE (only retrieve a single row), or None. In this last case, the
1072         cursor is returned if any query is executed, since it's used by
1073         subclasses such as InsertQuery). It's possible, however, that no query
1074         is needed, as the filters describe an empty set. In that case, None is
1075         returned, to avoid any unnecessary database interaction.
1076         """
1077         result_type = result_type or NO_RESULTS
1078         try:
1079             sql, params = self.as_sql()
1080             if not sql:
1081                 raise EmptyResultSet
1082         except EmptyResultSet:
1083             if result_type == MULTI:
1084                 return iter([])
1085             else:
1086                 return
1087         if chunked_fetch:
1088             cursor = self.connection.chunked_cursor()
1089         else:
1090             cursor = self.connection.cursor()
1091         try:
1092             cursor.execute(sql, params)
1093         except Exception:
1094             # Might fail for server-side cursors (e.g. connection closed)
1095             cursor.close()
1096             raise
1097 
1098         if result_type == CURSOR:
1099             # Give the caller the cursor to process and close.
1100             return cursor
1101         if result_type == SINGLE:
1102             try:
1103                 val = cursor.fetchone()
1104                 if val:
1105                     return val[0:self.col_count]
1106                 return val
1107             finally:
1108                 # done with the cursor
1109                 cursor.close()
1110         if result_type == NO_RESULTS:
1111             cursor.close()
1112             return
1113 
1114         result = cursor_iter(
1115             cursor, self.connection.features.empty_fetchmany_value,
1116             self.col_count if self.has_extra_select else None,
1117             chunk_size,
1118         )
1119         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:
1120             try:
1121                 # If we are using non-chunked reads, we return the same data
1122                 # structure as normally, but ensure it is all read into memory
1123                 # before going any further. Use chunked_fetch if requested,
1124                 # unless the database doesn't support it.
1125                 return list(result)
1126             finally:
1127                 # done with the cursor
1128                 cursor.close()
1129         return result
1130 
1131     def as_subquery_condition(self, alias, columns, compiler):
1132         qn = compiler.quote_name_unless_alias
1133         qn2 = self.connection.ops.quote_name
1134 
1135         for index, select_col in enumerate(self.query.select):
1136             lhs_sql, lhs_params = self.compile(select_col)
1137             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
1138             self.query.where.add(
1139                 QueryWrapper('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
1140 
1141         sql, params = self.as_sql()
1142         return 'EXISTS (%s)' % sql, params
1143 
1144     def explain_query(self):
1145         result = list(self.execute_sql())
1146         # Some backends return 1 item tuples with strings, and others return
1147         # tuples with integers and strings. Flatten them out into strings.
1148         for row in result[0]:
1149             if not isinstance(row, str):
1150                 yield ' '.join(str(c) for c in row)
1151             else:
1152                 yield row

</code><intended_behavior>While not directly modifying this class for the boolean validation, ensuring that the `SQLCompiler` has access to the validation logic or error raised in `QuerySet`'s methods might help with broader error handling and messaging. The actual change would be in the queryset filtering logic, but attention should be given here to ensure that errors are gracefully handled and correctly propagate up to developers when incorrect `__isnull` usage is attempted. The intended behavior is not so much an alteration of `SQLCompiler` but an acknowledgment that changes in `QuerySet` logic may affect how SQL compilation errors are caught and reported.</intended_behavior>]
2025-10-18 11:14:13.778 | INFO     | Search completed. Bug locations: [<file>django/db/models/query.py</file>
<class>QuerySet</class>
<method>None</method>
<code>
185 class QuerySet:
186     """Represent a lazy database lookup for a set of objects."""
187 
188     def __init__(self, model=None, query=None, using=None, hints=None):
189         self.model = model
190         self._db = using
191         self._hints = hints or {}
192         self.query = query or sql.Query(self.model)
193         self._result_cache = None
194         self._sticky_filter = False
195         self._for_write = False
196         self._prefetch_related_lookups = ()
197         self._prefetch_done = False
198         self._known_related_objects = {}  # {rel_field: {pk: rel_obj}}
199         self._iterable_class = ModelIterable
200         self._fields = None
201 
202     def as_manager(cls):
203         # Address the circular dependency between `Queryset` and `Manager`.
204         from django.db.models.manager import Manager
205         manager = Manager.from_queryset(cls)()
206         manager._built_with_as_manager = True
207         return manager
208     as_manager.queryset_only = True
209     as_manager = classmethod(as_manager)
210 
211     ########################
212     # PYTHON MAGIC METHODS #
213     ########################
214 
215     def __deepcopy__(self, memo):
216         """Don't populate the QuerySet's cache."""
217         obj = self.__class__()
218         for k, v in self.__dict__.items():
219             if k == '_result_cache':
220                 obj.__dict__[k] = None
221             else:
222                 obj.__dict__[k] = copy.deepcopy(v, memo)
223         return obj
224 
225     def __getstate__(self):
226         # Force the cache to be fully populated.
227         self._fetch_all()
228         return {**self.__dict__, DJANGO_VERSION_PICKLE_KEY: get_version()}
229 
230     def __setstate__(self, state):
231         msg = None
232         pickled_version = state.get(DJANGO_VERSION_PICKLE_KEY)
233         if pickled_version:
234             current_version = get_version()
235             if current_version != pickled_version:
236                 msg = (
237                     "Pickled queryset instance's Django version %s does not "
238                     "match the current version %s." % (pickled_version, current_version)
239                 )
240         else:
241             msg = "Pickled queryset instance's Django version is not specified."
242 
243         if msg:
244             warnings.warn(msg, RuntimeWarning, stacklevel=2)
245 
246         self.__dict__.update(state)
247 
248     def __repr__(self):
249         data = list(self[:REPR_OUTPUT_SIZE + 1])
250         if len(data) > REPR_OUTPUT_SIZE:
251             data[-1] = "...(remaining elements truncated)..."
252         return '<%s %r>' % (self.__class__.__name__, data)
253 
254     def __len__(self):
255         self._fetch_all()
256         return len(self._result_cache)
257 
258     def __iter__(self):
259         """
260         The queryset iterator protocol uses three nested iterators in the
261         default case:
262             1. sql.compiler.execute_sql()
263                - Returns 100 rows at time (constants.GET_ITERATOR_CHUNK_SIZE)
264                  using cursor.fetchmany(). This part is responsible for
265                  doing some column masking, and returning the rows in chunks.
266             2. sql.compiler.results_iter()
267                - Returns one row at time. At this point the rows are still just
268                  tuples. In some cases the return values are converted to
269                  Python values at this location.
270             3. self.iterator()
271                - Responsible for turning the rows into model objects.
272         """
273         self._fetch_all()
274         return iter(self._result_cache)
275 
276     def __bool__(self):
277         self._fetch_all()
278         return bool(self._result_cache)
279 
280     def __getitem__(self, k):
281         """Retrieve an item or slice from the set of results."""
282         if not isinstance(k, (int, slice)):
283             raise TypeError(
284                 'QuerySet indices must be integers or slices, not %s.'
285                 % type(k).__name__
286             )
287         assert ((not isinstance(k, slice) and (k >= 0)) or
288                 (isinstance(k, slice) and (k.start is None or k.start >= 0) and
289                  (k.stop is None or k.stop >= 0))), \
290             "Negative indexing is not supported."
291 
292         if self._result_cache is not None:
293             return self._result_cache[k]
294 
295         if isinstance(k, slice):
296             qs = self._chain()
297             if k.start is not None:
298                 start = int(k.start)
299             else:
300                 start = None
301             if k.stop is not None:
302                 stop = int(k.stop)
303             else:
304                 stop = None
305             qs.query.set_limits(start, stop)
306             return list(qs)[::k.step] if k.step else qs
307 
308         qs = self._chain()
309         qs.query.set_limits(k, k + 1)
310         qs._fetch_all()
311         return qs._result_cache[0]
312 
313     def __and__(self, other):
314         self._merge_sanity_check(other)
315         if isinstance(other, EmptyQuerySet):
316             return other
317         if isinstance(self, EmptyQuerySet):
318             return self
319         combined = self._chain()
320         combined._merge_known_related_objects(other)
321         combined.query.combine(other.query, sql.AND)
322         return combined
323 
324     def __or__(self, other):
325         self._merge_sanity_check(other)
326         if isinstance(self, EmptyQuerySet):
327             return other
328         if isinstance(other, EmptyQuerySet):
329             return self
330         query = self if self.query.can_filter() else self.model._base_manager.filter(pk__in=self.values('pk'))
331         combined = query._chain()
332         combined._merge_known_related_objects(other)
333         if not other.query.can_filter():
334             other = other.model._base_manager.filter(pk__in=other.values('pk'))
335         combined.query.combine(other.query, sql.OR)
336         return combined
337 
338     ####################################
339     # METHODS THAT DO DATABASE QUERIES #
340     ####################################
341 
342     def _iterator(self, use_chunked_fetch, chunk_size):
343         yield from self._iterable_class(self, chunked_fetch=use_chunked_fetch, chunk_size=chunk_size)
344 
345     def iterator(self, chunk_size=2000):
346         """
347         An iterator over the results from applying this QuerySet to the
348         database.
349         """
350         if chunk_size <= 0:
351             raise ValueError('Chunk size must be strictly positive.')
352         use_chunked_fetch = not connections[self.db].settings_dict.get('DISABLE_SERVER_SIDE_CURSORS')
353         return self._iterator(use_chunked_fetch, chunk_size)
354 
355     def aggregate(self, *args, **kwargs):
356         """
357         Return a dictionary containing the calculations (aggregation)
358         over the current queryset.
359 
360         If args is present the expression is passed as a kwarg using
361         the Aggregate object's default alias.
362         """
363         if self.query.distinct_fields:
364             raise NotImplementedError("aggregate() + distinct(fields) not implemented.")
365         self._validate_values_are_expressions((*args, *kwargs.values()), method_name='aggregate')
366         for arg in args:
367             # The default_alias property raises TypeError if default_alias
368             # can't be set automatically or AttributeError if it isn't an
369             # attribute.
370             try:
371                 arg.default_alias
372             except (AttributeError, TypeError):
373                 raise TypeError("Complex aggregates require an alias")
374             kwargs[arg.default_alias] = arg
375 
376         query = self.query.chain()
377         for (alias, aggregate_expr) in kwargs.items():
378             query.add_annotation(aggregate_expr, alias, is_summary=True)
379             if not query.annotations[alias].contains_aggregate:
380                 raise TypeError("%s is not an aggregate expression" % alias)
381         return query.get_aggregation(self.db, kwargs)
382 
383     def count(self):
384         """
385         Perform a SELECT COUNT() and return the number of records as an
386         integer.
387 
388         If the QuerySet is already fully cached, return the length of the
389         cached results set to avoid multiple SELECT COUNT(*) calls.
390         """
391         if self._result_cache is not None:
392             return len(self._result_cache)
393 
394         return self.query.get_count(using=self.db)
395 
396     def get(self, *args, **kwargs):
397         """
398         Perform the query and return a single object matching the given
399         keyword arguments.
400         """
401         clone = self._chain() if self.query.combinator else self.filter(*args, **kwargs)
402         if self.query.can_filter() and not self.query.distinct_fields:
403             clone = clone.order_by()
404         limit = None
405         if not clone.query.select_for_update or connections[clone.db].features.supports_select_for_update_with_limit:
406             limit = MAX_GET_RESULTS
407             clone.query.set_limits(high=limit)
408         num = len(clone)
409         if num == 1:
410             return clone._result_cache[0]
411         if not num:
412             raise self.model.DoesNotExist(
413                 "%s matching query does not exist." %
414                 self.model._meta.object_name
415             )
416         raise self.model.MultipleObjectsReturned(
417             'get() returned more than one %s -- it returned %s!' % (
418                 self.model._meta.object_name,
419                 num if not limit or num < limit else 'more than %s' % (limit - 1),
420             )
421         )
422 
423     def create(self, **kwargs):
424         """
425         Create a new object with the given kwargs, saving it to the database
426         and returning the created object.
427         """
428         obj = self.model(**kwargs)
429         self._for_write = True
430         obj.save(force_insert=True, using=self.db)
431         return obj
432 
433     def _populate_pk_values(self, objs):
434         for obj in objs:
435             if obj.pk is None:
436                 obj.pk = obj._meta.pk.get_pk_value_on_save(obj)
437 
438     def bulk_create(self, objs, batch_size=None, ignore_conflicts=False):
439         """
440         Insert each of the instances into the database. Do *not* call
441         save() on each of the instances, do not send any pre/post_save
442         signals, and do not set the primary key attribute if it is an
443         autoincrement field (except if features.can_return_rows_from_bulk_insert=True).
444         Multi-table models are not supported.
445         """
446         # When you bulk insert you don't get the primary keys back (if it's an
447         # autoincrement, except if can_return_rows_from_bulk_insert=True), so
448         # you can't insert into the child tables which references this. There
449         # are two workarounds:
450         # 1) This could be implemented if you didn't have an autoincrement pk
451         # 2) You could do it by doing O(n) normal inserts into the parent
452         #    tables to get the primary keys back and then doing a single bulk
453         #    insert into the childmost table.
454         # We currently set the primary keys on the objects when using
455         # PostgreSQL via the RETURNING ID clause. It should be possible for
456         # Oracle as well, but the semantics for extracting the primary keys is
457         # trickier so it's not done yet.
458         assert batch_size is None or batch_size > 0
459         # Check that the parents share the same concrete model with the our
460         # model to detect the inheritance pattern ConcreteGrandParent ->
461         # MultiTableParent -> ProxyChild. Simply checking self.model._meta.proxy
462         # would not identify that case as involving multiple tables.
463         for parent in self.model._meta.get_parent_list():
464             if parent._meta.concrete_model is not self.model._meta.concrete_model:
465                 raise ValueError("Can't bulk create a multi-table inherited model")
466         if not objs:
467             return objs
468         self._for_write = True
469         connection = connections[self.db]
470         opts = self.model._meta
471         fields = opts.concrete_fields
472         objs = list(objs)
473         self._populate_pk_values(objs)
474         with transaction.atomic(using=self.db, savepoint=False):
475             objs_with_pk, objs_without_pk = partition(lambda o: o.pk is None, objs)
476             if objs_with_pk:
477                 returned_columns = self._batched_insert(
478                     objs_with_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
479                 )
480                 for obj_with_pk, results in zip(objs_with_pk, returned_columns):
481                     for result, field in zip(results, opts.db_returning_fields):
482                         if field != opts.pk:
483                             setattr(obj_with_pk, field.attname, result)
484                 for obj_with_pk in objs_with_pk:
485                     obj_with_pk._state.adding = False
486                     obj_with_pk._state.db = self.db
487             if objs_without_pk:
488                 fields = [f for f in fields if not isinstance(f, AutoField)]
489                 returned_columns = self._batched_insert(
490                     objs_without_pk, fields, batch_size, ignore_conflicts=ignore_conflicts,
491                 )
492                 if connection.features.can_return_rows_from_bulk_insert and not ignore_conflicts:
493                     assert len(returned_columns) == len(objs_without_pk)
494                 for obj_without_pk, results in zip(objs_without_pk, returned_columns):
495                     for result, field in zip(results, opts.db_returning_fields):
496                         setattr(obj_without_pk, field.attname, result)
497                     obj_without_pk._state.adding = False
498                     obj_without_pk._state.db = self.db
499 
500         return objs
501 
502     def bulk_update(self, objs, fields, batch_size=None):
503         """
504         Update the given fields in each of the given objects in the database.
505         """
506         if batch_size is not None and batch_size < 0:
507             raise ValueError('Batch size must be a positive integer.')
508         if not fields:
509             raise ValueError('Field names must be given to bulk_update().')
510         objs = tuple(objs)
511         if any(obj.pk is None for obj in objs):
512             raise ValueError('All bulk_update() objects must have a primary key set.')
513         fields = [self.model._meta.get_field(name) for name in fields]
514         if any(not f.concrete or f.many_to_many for f in fields):
515             raise ValueError('bulk_update() can only be used with concrete fields.')
516         if any(f.primary_key for f in fields):
517             raise ValueError('bulk_update() cannot be used with primary key fields.')
518         if not objs:
519             return
520         # PK is used twice in the resulting update query, once in the filter
521         # and once in the WHEN. Each field will also have one CAST.
522         max_batch_size = connections[self.db].ops.bulk_batch_size(['pk', 'pk'] + fields, objs)
523         batch_size = min(batch_size, max_batch_size) if batch_size else max_batch_size
524         requires_casting = connections[self.db].features.requires_casted_case_in_updates
525         batches = (objs[i:i + batch_size] for i in range(0, len(objs), batch_size))
526         updates = []
527         for batch_objs in batches:
528             update_kwargs = {}
529             for field in fields:
530                 when_statements = []
531                 for obj in batch_objs:
532                     attr = getattr(obj, field.attname)
533                     if not isinstance(attr, Expression):
534                         attr = Value(attr, output_field=field)
535                     when_statements.append(When(pk=obj.pk, then=attr))
536                 case_statement = Case(*when_statements, output_field=field)
537                 if requires_casting:
538                     case_statement = Cast(case_statement, output_field=field)
539                 update_kwargs[field.attname] = case_statement
540             updates.append(([obj.pk for obj in batch_objs], update_kwargs))
541         with transaction.atomic(using=self.db, savepoint=False):
542             for pks, update_kwargs in updates:
543                 self.filter(pk__in=pks).update(**update_kwargs)
544     bulk_update.alters_data = True
545 
546     def get_or_create(self, defaults=None, **kwargs):
547         """
548         Look up an object with the given kwargs, creating one if necessary.
549         Return a tuple of (object, created), where created is a boolean
550         specifying whether an object was created.
551         """
552         # The get() needs to be targeted at the write database in order
553         # to avoid potential transaction consistency problems.
554         self._for_write = True
555         try:
556             return self.get(**kwargs), False
557         except self.model.DoesNotExist:
558             params = self._extract_model_params(defaults, **kwargs)
559             return self._create_object_from_params(kwargs, params)
560 
561     def update_or_create(self, defaults=None, **kwargs):
562         """
563         Look up an object with the given kwargs, updating one with defaults
564         if it exists, otherwise create a new one.
565         Return a tuple (object, created), where created is a boolean
566         specifying whether an object was created.
567         """
568         defaults = defaults or {}
569         self._for_write = True
570         with transaction.atomic(using=self.db):
571             try:
572                 obj = self.select_for_update().get(**kwargs)
573             except self.model.DoesNotExist:
574                 params = self._extract_model_params(defaults, **kwargs)
575                 # Lock the row so that a concurrent update is blocked until
576                 # after update_or_create() has performed its save.
577                 obj, created = self._create_object_from_params(kwargs, params, lock=True)
578                 if created:
579                     return obj, created
580             for k, v in defaults.items():
581                 setattr(obj, k, v() if callable(v) else v)
582             obj.save(using=self.db)
583         return obj, False
584 
585     def _create_object_from_params(self, lookup, params, lock=False):
586         """
587         Try to create an object using passed params. Used by get_or_create()
588         and update_or_create().
589         """
590         try:
591             with transaction.atomic(using=self.db):
592                 params = {k: v() if callable(v) else v for k, v in params.items()}
593                 obj = self.create(**params)
594             return obj, True
595         except IntegrityError as e:
596             try:
597                 qs = self.select_for_update() if lock else self
598                 return qs.get(**lookup), False
599             except self.model.DoesNotExist:
600                 pass
601             raise e
602 
603     def _extract_model_params(self, defaults, **kwargs):
604         """
605         Prepare `params` for creating a model instance based on the given
606         kwargs; for use by get_or_create() and update_or_create().
607         """
608         defaults = defaults or {}
609         params = {k: v for k, v in kwargs.items() if LOOKUP_SEP not in k}
610         params.update(defaults)
611         property_names = self.model._meta._property_names
612         invalid_params = []
613         for param in params:
614             try:
615                 self.model._meta.get_field(param)
616             except exceptions.FieldDoesNotExist:
617                 # It's okay to use a model's property if it has a setter.
618                 if not (param in property_names and getattr(self.model, param).fset):
619                     invalid_params.append(param)
620         if invalid_params:
621             raise exceptions.FieldError(
622                 "Invalid field name(s) for model %s: '%s'." % (
623                     self.model._meta.object_name,
624                     "', '".join(sorted(invalid_params)),
625                 ))
626         return params
627 
628     def _earliest(self, *fields):
629         """
630         Return the earliest object according to fields (if given) or by the
631         model's Meta.get_latest_by.
632         """
633         if fields:
634             order_by = fields
635         else:
636             order_by = getattr(self.model._meta, 'get_latest_by')
637             if order_by and not isinstance(order_by, (tuple, list)):
638                 order_by = (order_by,)
639         if order_by is None:
640             raise ValueError(
641                 "earliest() and latest() require either fields as positional "
642                 "arguments or 'get_latest_by' in the model's Meta."
643             )
644 
645         assert not self.query.is_sliced, \
646             "Cannot change a query once a slice has been taken."
647         obj = self._chain()
648         obj.query.set_limits(high=1)
649         obj.query.clear_ordering(force_empty=True)
650         obj.query.add_ordering(*order_by)
651         return obj.get()
652 
653     def earliest(self, *fields):
654         return self._earliest(*fields)
655 
656     def latest(self, *fields):
657         return self.reverse()._earliest(*fields)
658 
659     def first(self):
660         """Return the first object of a query or None if no match is found."""
661         for obj in (self if self.ordered else self.order_by('pk'))[:1]:
662             return obj
663 
664     def last(self):
665         """Return the last object of a query or None if no match is found."""
666         for obj in (self.reverse() if self.ordered else self.order_by('-pk'))[:1]:
667             return obj
668 
669     def in_bulk(self, id_list=None, *, field_name='pk'):
670         """
671         Return a dictionary mapping each of the given IDs to the object with
672         that ID. If `id_list` isn't provided, evaluate the entire QuerySet.
673         """
674         assert not self.query.is_sliced, \
675             "Cannot use 'limit' or 'offset' with in_bulk"
676         if field_name != 'pk' and not self.model._meta.get_field(field_name).unique:
677             raise ValueError("in_bulk()'s field_name must be a unique field but %r isn't." % field_name)
678         if id_list is not None:
679             if not id_list:
680                 return {}
681             filter_key = '{}__in'.format(field_name)
682             batch_size = connections[self.db].features.max_query_params
683             id_list = tuple(id_list)
684             # If the database has a limit on the number of query parameters
685             # (e.g. SQLite), retrieve objects in batches if necessary.
686             if batch_size and batch_size < len(id_list):
687                 qs = ()
688                 for offset in range(0, len(id_list), batch_size):
689                     batch = id_list[offset:offset + batch_size]
690                     qs += tuple(self.filter(**{filter_key: batch}).order_by())
691             else:
692                 qs = self.filter(**{filter_key: id_list}).order_by()
693         else:
694             qs = self._chain()
695         return {getattr(obj, field_name): obj for obj in qs}
696 
697     def delete(self):
698         """Delete the records in the current QuerySet."""
699         assert not self.query.is_sliced, \
700             "Cannot use 'limit' or 'offset' with delete."
701 
702         if self._fields is not None:
703             raise TypeError("Cannot call delete() after .values() or .values_list()")
704 
705         del_query = self._chain()
706 
707         # The delete is actually 2 queries - one to find related objects,
708         # and one to delete. Make sure that the discovery of related
709         # objects is performed on the same database as the deletion.
710         del_query._for_write = True
711 
712         # Disable non-supported fields.
713         del_query.query.select_for_update = False
714         del_query.query.select_related = False
715         del_query.query.clear_ordering(force_empty=True)
716 
717         collector = Collector(using=del_query.db)
718         collector.collect(del_query)
719         deleted, _rows_count = collector.delete()
720 
721         # Clear the result cache, in case this QuerySet gets reused.
722         self._result_cache = None
723         return deleted, _rows_count
724 
725     delete.alters_data = True
726     delete.queryset_only = True
727 
728     def _raw_delete(self, using):
729         """
730         Delete objects found from the given queryset in single direct SQL
731         query. No signals are sent and there is no protection for cascades.
732         """
733         return sql.DeleteQuery(self.model).delete_qs(self, using)
734     _raw_delete.alters_data = True
735 
736     def update(self, **kwargs):
737         """
738         Update all elements in the current QuerySet, setting all the given
739         fields to the appropriate values.
740         """
741         assert not self.query.is_sliced, \
742             "Cannot update a query once a slice has been taken."
743         self._for_write = True
744         query = self.query.chain(sql.UpdateQuery)
745         query.add_update_values(kwargs)
746         # Clear any annotations so that they won't be present in subqueries.
747         query.annotations = {}
748         with transaction.mark_for_rollback_on_error(using=self.db):
749             rows = query.get_compiler(self.db).execute_sql(CURSOR)
750         self._result_cache = None
751         return rows
752     update.alters_data = True
753 
754     def _update(self, values):
755         """
756         A version of update() that accepts field objects instead of field names.
757         Used primarily for model saving and not intended for use by general
758         code (it requires too much poking around at model internals to be
759         useful at that level).
760         """
761         assert not self.query.is_sliced, \
762             "Cannot update a query once a slice has been taken."
763         query = self.query.chain(sql.UpdateQuery)
764         query.add_update_fields(values)
765         # Clear any annotations so that they won't be present in subqueries.
766         query.annotations = {}
767         self._result_cache = None
768         return query.get_compiler(self.db).execute_sql(CURSOR)
769     _update.alters_data = True
770     _update.queryset_only = False
771 
772     def exists(self):
773         if self._result_cache is None:
774             return self.query.has_results(using=self.db)
775         return bool(self._result_cache)
776 
777     def _prefetch_related_objects(self):
778         # This method can only be called once the result cache has been filled.
779         prefetch_related_objects(self._result_cache, *self._prefetch_related_lookups)
780         self._prefetch_done = True
781 
782     def explain(self, *, format=None, **options):
783         return self.query.explain(using=self.db, format=format, **options)
784 
785     ##################################################
786     # PUBLIC METHODS THAT RETURN A QUERYSET SUBCLASS #
787     ##################################################
788 
789     def raw(self, raw_query, params=None, translations=None, using=None):
790         if using is None:
791             using = self.db
792         qs = RawQuerySet(raw_query, model=self.model, params=params, translations=translations, using=using)
793         qs._prefetch_related_lookups = self._prefetch_related_lookups[:]
794         return qs
795 
796     def _values(self, *fields, **expressions):
797         clone = self._chain()
798         if expressions:
799             clone = clone.annotate(**expressions)
800         clone._fields = fields
801         clone.query.set_values(fields)
802         return clone
803 
804     def values(self, *fields, **expressions):
805         fields += tuple(expressions)
806         clone = self._values(*fields, **expressions)
807         clone._iterable_class = ValuesIterable
808         return clone
809 
810     def values_list(self, *fields, flat=False, named=False):
811         if flat and named:
812             raise TypeError("'flat' and 'named' can't be used together.")
813         if flat and len(fields) > 1:
814             raise TypeError("'flat' is not valid when values_list is called with more than one field.")
815 
816         field_names = {f for f in fields if not hasattr(f, 'resolve_expression')}
817         _fields = []
818         expressions = {}
819         counter = 1
820         for field in fields:
821             if hasattr(field, 'resolve_expression'):
822                 field_id_prefix = getattr(field, 'default_alias', field.__class__.__name__.lower())
823                 while True:
824                     field_id = field_id_prefix + str(counter)
825                     counter += 1
826                     if field_id not in field_names:
827                         break
828                 expressions[field_id] = field
829                 _fields.append(field_id)
830             else:
831                 _fields.append(field)
832 
833         clone = self._values(*_fields, **expressions)
834         clone._iterable_class = (
835             NamedValuesListIterable if named
836             else FlatValuesListIterable if flat
837             else ValuesListIterable
838         )
839         return clone
840 
841     def dates(self, field_name, kind, order='ASC'):
842         """
843         Return a list of date objects representing all available dates for
844         the given field_name, scoped to 'kind'.
845         """
846         assert kind in ('year', 'month', 'week', 'day'), \
847             "'kind' must be one of 'year', 'month', 'week', or 'day'."
848         assert order in ('ASC', 'DESC'), \
849             "'order' must be either 'ASC' or 'DESC'."
850         return self.annotate(
851             datefield=Trunc(field_name, kind, output_field=DateField()),
852             plain_field=F(field_name)
853         ).values_list(
854             'datefield', flat=True
855         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datefield')
856 
857     def datetimes(self, field_name, kind, order='ASC', tzinfo=None):
858         """
859         Return a list of datetime objects representing all available
860         datetimes for the given field_name, scoped to 'kind'.
861         """
862         assert kind in ('year', 'month', 'week', 'day', 'hour', 'minute', 'second'), \
863             "'kind' must be one of 'year', 'month', 'week', 'day', 'hour', 'minute', or 'second'."
864         assert order in ('ASC', 'DESC'), \
865             "'order' must be either 'ASC' or 'DESC'."
866         if settings.USE_TZ:
867             if tzinfo is None:
868                 tzinfo = timezone.get_current_timezone()
869         else:
870             tzinfo = None
871         return self.annotate(
872             datetimefield=Trunc(field_name, kind, output_field=DateTimeField(), tzinfo=tzinfo),
873             plain_field=F(field_name)
874         ).values_list(
875             'datetimefield', flat=True
876         ).distinct().filter(plain_field__isnull=False).order_by(('-' if order == 'DESC' else '') + 'datetimefield')
877 
878     def none(self):
879         """Return an empty QuerySet."""
880         clone = self._chain()
881         clone.query.set_empty()
882         return clone
883 
884     ##################################################################
885     # PUBLIC METHODS THAT ALTER ATTRIBUTES AND RETURN A NEW QUERYSET #
886     ##################################################################
887 
888     def all(self):
889         """
890         Return a new QuerySet that is a copy of the current one. This allows a
891         QuerySet to proxy for a model manager in some cases.
892         """
893         return self._chain()
894 
895     def filter(self, *args, **kwargs):
896         """
897         Return a new QuerySet instance with the args ANDed to the existing
898         set.
899         """
900         self._not_support_combined_queries('filter')
901         return self._filter_or_exclude(False, *args, **kwargs)
902 
903     def exclude(self, *args, **kwargs):
904         """
905         Return a new QuerySet instance with NOT (args) ANDed to the existing
906         set.
907         """
908         self._not_support_combined_queries('exclude')
909         return self._filter_or_exclude(True, *args, **kwargs)
910 
911     def _filter_or_exclude(self, negate, *args, **kwargs):
912         if args or kwargs:
913             assert not self.query.is_sliced, \
914                 "Cannot filter a query once a slice has been taken."
915 
916         clone = self._chain()
917         if negate:
918             clone.query.add_q(~Q(*args, **kwargs))
919         else:
920             clone.query.add_q(Q(*args, **kwargs))
921         return clone
922 
923     def complex_filter(self, filter_obj):
924         """
925         Return a new QuerySet instance with filter_obj added to the filters.
926 
927         filter_obj can be a Q object or a dictionary of keyword lookup
928         arguments.
929 
930         This exists to support framework features such as 'limit_choices_to',
931         and usually it will be more natural to use other methods.
932         """
933         if isinstance(filter_obj, Q):
934             clone = self._chain()
935             clone.query.add_q(filter_obj)
936             return clone
937         else:
938             return self._filter_or_exclude(None, **filter_obj)
939 
940     def _combinator_query(self, combinator, *other_qs, all=False):
941         # Clone the query to inherit the select list and everything
942         clone = self._chain()
943         # Clear limits and ordering so they can be reapplied
944         clone.query.clear_ordering(True)
945         clone.query.clear_limits()
946         clone.query.combined_queries = (self.query,) + tuple(qs.query for qs in other_qs)
947         clone.query.combinator = combinator
948         clone.query.combinator_all = all
949         return clone
950 
951     def union(self, *other_qs, all=False):
952         # If the query is an EmptyQuerySet, combine all nonempty querysets.
953         if isinstance(self, EmptyQuerySet):
954             qs = [q for q in other_qs if not isinstance(q, EmptyQuerySet)]
955             return qs[0]._combinator_query('union', *qs[1:], all=all) if qs else self
956         return self._combinator_query('union', *other_qs, all=all)
957 
958     def intersection(self, *other_qs):
959         # If any query is an EmptyQuerySet, return it.
960         if isinstance(self, EmptyQuerySet):
961             return self
962         for other in other_qs:
963             if isinstance(other, EmptyQuerySet):
964                 return other
965         return self._combinator_query('intersection', *other_qs)
966 
967     def difference(self, *other_qs):
968         # If the query is an EmptyQuerySet, return it.
969         if isinstance(self, EmptyQuerySet):
970             return self
971         return self._combinator_query('difference', *other_qs)
972 
973     def select_for_update(self, nowait=False, skip_locked=False, of=()):
974         """
975         Return a new QuerySet instance that will select objects with a
976         FOR UPDATE lock.
977         """
978         if nowait and skip_locked:
979             raise ValueError('The nowait option cannot be used with skip_locked.')
980         obj = self._chain()
981         obj._for_write = True
982         obj.query.select_for_update = True
983         obj.query.select_for_update_nowait = nowait
984         obj.query.select_for_update_skip_locked = skip_locked
985         obj.query.select_for_update_of = of
986         return obj
987 
988     def select_related(self, *fields):
989         """
990         Return a new QuerySet instance that will select related objects.
991 
992         If fields are specified, they must be ForeignKey fields and only those
993         related objects are included in the selection.
994 
995         If select_related(None) is called, clear the list.
996         """
997         self._not_support_combined_queries('select_related')
998         if self._fields is not None:
999             raise TypeError("Cannot call select_related() after .values() or .values_list()")
1000 
1001         obj = self._chain()
1002         if fields == (None,):
1003             obj.query.select_related = False
1004         elif fields:
1005             obj.query.add_select_related(fields)
1006         else:
1007             obj.query.select_related = True
1008         return obj
1009 
1010     def prefetch_related(self, *lookups):
1011         """
1012         Return a new QuerySet instance that will prefetch the specified
1013         Many-To-One and Many-To-Many related objects when the QuerySet is
1014         evaluated.
1015 
1016         When prefetch_related() is called more than once, append to the list of
1017         prefetch lookups. If prefetch_related(None) is called, clear the list.
1018         """
1019         self._not_support_combined_queries('prefetch_related')
1020         clone = self._chain()
1021         if lookups == (None,):
1022             clone._prefetch_related_lookups = ()
1023         else:
1024             for lookup in lookups:
1025                 if isinstance(lookup, Prefetch):
1026                     lookup = lookup.prefetch_to
1027                 lookup = lookup.split(LOOKUP_SEP, 1)[0]
1028                 if lookup in self.query._filtered_relations:
1029                     raise ValueError('prefetch_related() is not supported with FilteredRelation.')
1030             clone._prefetch_related_lookups = clone._prefetch_related_lookups + lookups
1031         return clone
1032 
1033     def annotate(self, *args, **kwargs):
1034         """
1035         Return a query set in which the returned objects have been annotated
1036         with extra data or aggregations.
1037         """
1038         self._not_support_combined_queries('annotate')
1039         self._validate_values_are_expressions(args + tuple(kwargs.values()), method_name='annotate')
1040         annotations = {}
1041         for arg in args:
1042             # The default_alias property may raise a TypeError.
1043             try:
1044                 if arg.default_alias in kwargs:
1045                     raise ValueError("The named annotation '%s' conflicts with the "
1046                                      "default name for another annotation."
1047                                      % arg.default_alias)
1048             except TypeError:
1049                 raise TypeError("Complex annotations require an alias")
1050             annotations[arg.default_alias] = arg
1051         annotations.update(kwargs)
1052 
1053         clone = self._chain()
1054         names = self._fields
1055         if names is None:
1056             names = set(chain.from_iterable(
1057                 (field.name, field.attname) if hasattr(field, 'attname') else (field.name,)
1058                 for field in self.model._meta.get_fields()
1059             ))
1060 
1061         for alias, annotation in annotations.items():
1062             if alias in names:
1063                 raise ValueError("The annotation '%s' conflicts with a field on "
1064                                  "the model." % alias)
1065             if isinstance(annotation, FilteredRelation):
1066                 clone.query.add_filtered_relation(annotation, alias)
1067             else:
1068                 clone.query.add_annotation(annotation, alias, is_summary=False)
1069 
1070         for alias, annotation in clone.query.annotations.items():
1071             if alias in annotations and annotation.contains_aggregate:
1072                 if clone._fields is None:
1073                     clone.query.group_by = True
1074                 else:
1075                     clone.query.set_group_by()
1076                 break
1077 
1078         return clone
1079 
1080     def order_by(self, *field_names):
1081         """Return a new QuerySet instance with the ordering changed."""
1082         assert not self.query.is_sliced, \
1083             "Cannot reorder a query once a slice has been taken."
1084         obj = self._chain()
1085         obj.query.clear_ordering(force_empty=False)
1086         obj.query.add_ordering(*field_names)
1087         return obj
1088 
1089     def distinct(self, *field_names):
1090         """
1091         Return a new QuerySet instance that will select only distinct results.
1092         """
1093         assert not self.query.is_sliced, \
1094             "Cannot create distinct fields once a slice has been taken."
1095         obj = self._chain()
1096         obj.query.add_distinct_fields(*field_names)
1097         return obj
1098 
1099     def extra(self, select=None, where=None, params=None, tables=None,
1100               order_by=None, select_params=None):
1101         """Add extra SQL fragments to the query."""
1102         self._not_support_combined_queries('extra')
1103         assert not self.query.is_sliced, \
1104             "Cannot change a query once a slice has been taken"
1105         clone = self._chain()
1106         clone.query.add_extra(select, select_params, where, params, tables, order_by)
1107         return clone
1108 
1109     def reverse(self):
1110         """Reverse the ordering of the QuerySet."""
1111         if self.query.is_sliced:
1112             raise TypeError('Cannot reverse a query once a slice has been taken.')
1113         clone = self._chain()
1114         clone.query.standard_ordering = not clone.query.standard_ordering
1115         return clone
1116 
1117     def defer(self, *fields):
1118         """
1119         Defer the loading of data for certain fields until they are accessed.
1120         Add the set of deferred fields to any existing set of deferred fields.
1121         The only exception to this is if None is passed in as the only
1122         parameter, in which case removal all deferrals.
1123         """
1124         self._not_support_combined_queries('defer')
1125         if self._fields is not None:
1126             raise TypeError("Cannot call defer() after .values() or .values_list()")
1127         clone = self._chain()
1128         if fields == (None,):
1129             clone.query.clear_deferred_loading()
1130         else:
1131             clone.query.add_deferred_loading(fields)
1132         return clone
1133 
1134     def only(self, *fields):
1135         """
1136         Essentially, the opposite of defer(). Only the fields passed into this
1137         method and that are not already specified as deferred are loaded
1138         immediately when the queryset is evaluated.
1139         """
1140         self._not_support_combined_queries('only')
1141         if self._fields is not None:
1142             raise TypeError("Cannot call only() after .values() or .values_list()")
1143         if fields == (None,):
1144             # Can only pass None to defer(), not only(), as the rest option.
1145             # That won't stop people trying to do this, so let's be explicit.
1146             raise TypeError("Cannot pass None as an argument to only().")
1147         for field in fields:
1148             field = field.split(LOOKUP_SEP, 1)[0]
1149             if field in self.query._filtered_relations:
1150                 raise ValueError('only() is not supported with FilteredRelation.')
1151         clone = self._chain()
1152         clone.query.add_immediate_loading(fields)
1153         return clone
1154 
1155     def using(self, alias):
1156         """Select which database this QuerySet should execute against."""
1157         clone = self._chain()
1158         clone._db = alias
1159         return clone
1160 
1161     ###################################
1162     # PUBLIC INTROSPECTION ATTRIBUTES #
1163     ###################################
1164 
1165     @property
1166     def ordered(self):
1167         """
1168         Return True if the QuerySet is ordered -- i.e. has an order_by()
1169         clause or a default ordering on the model (or is empty).
1170         """
1171         if isinstance(self, EmptyQuerySet):
1172             return True
1173         if self.query.extra_order_by or self.query.order_by:
1174             return True
1175         elif self.query.default_ordering and self.query.get_meta().ordering:
1176             return True
1177         else:
1178             return False
1179 
1180     @property
1181     def db(self):
1182         """Return the database used if this query is executed now."""
1183         if self._for_write:
1184             return self._db or router.db_for_write(self.model, **self._hints)
1185         return self._db or router.db_for_read(self.model, **self._hints)
1186 
1187     ###################
1188     # PRIVATE METHODS #
1189     ###################
1190 
1191     def _insert(self, objs, fields, returning_fields=None, raw=False, using=None, ignore_conflicts=False):
1192         """
1193         Insert a new record for the given model. This provides an interface to
1194         the InsertQuery class and is how Model.save() is implemented.
1195         """
1196         self._for_write = True
1197         if using is None:
1198             using = self.db
1199         query = sql.InsertQuery(self.model, ignore_conflicts=ignore_conflicts)
1200         query.insert_values(fields, objs, raw=raw)
1201         return query.get_compiler(using=using).execute_sql(returning_fields)
1202     _insert.alters_data = True
1203     _insert.queryset_only = False
1204 
1205     def _batched_insert(self, objs, fields, batch_size, ignore_conflicts=False):
1206         """
1207         Helper method for bulk_create() to insert objs one batch at a time.
1208         """
1209         if ignore_conflicts and not connections[self.db].features.supports_ignore_conflicts:
1210             raise NotSupportedError('This database backend does not support ignoring conflicts.')
1211         ops = connections[self.db].ops
1212         batch_size = (batch_size or max(ops.bulk_batch_size(fields, objs), 1))
1213         inserted_rows = []
1214         bulk_return = connections[self.db].features.can_return_rows_from_bulk_insert
1215         for item in [objs[i:i + batch_size] for i in range(0, len(objs), batch_size)]:
1216             if bulk_return and not ignore_conflicts:
1217                 inserted_columns = self._insert(
1218                     item, fields=fields, using=self.db,
1219                     returning_fields=self.model._meta.db_returning_fields,
1220                     ignore_conflicts=ignore_conflicts,
1221                 )
1222                 if isinstance(inserted_columns, list):
1223                     inserted_rows.extend(inserted_columns)
1224                 else:
1225                     inserted_rows.append(inserted_columns)
1226             else:
1227                 self._insert(item, fields=fields, using=self.db, ignore_conflicts=ignore_conflicts)
1228         return inserted_rows
1229 
1230     def _chain(self, **kwargs):
1231         """
1232         Return a copy of the current QuerySet that's ready for another
1233         operation.
1234         """
1235         obj = self._clone()
1236         if obj._sticky_filter:
1237             obj.query.filter_is_sticky = True
1238             obj._sticky_filter = False
1239         obj.__dict__.update(kwargs)
1240         return obj
1241 
1242     def _clone(self):
1243         """
1244         Return a copy of the current QuerySet. A lightweight alternative
1245         to deepcopy().
1246         """
1247         c = self.__class__(model=self.model, query=self.query.chain(), using=self._db, hints=self._hints)
1248         c._sticky_filter = self._sticky_filter
1249         c._for_write = self._for_write
1250         c._prefetch_related_lookups = self._prefetch_related_lookups[:]
1251         c._known_related_objects = self._known_related_objects
1252         c._iterable_class = self._iterable_class
1253         c._fields = self._fields
1254         return c
1255 
1256     def _fetch_all(self):
1257         if self._result_cache is None:
1258             self._result_cache = list(self._iterable_class(self))
1259         if self._prefetch_related_lookups and not self._prefetch_done:
1260             self._prefetch_related_objects()
1261 
1262     def _next_is_sticky(self):
1263         """
1264         Indicate that the next filter call and the one following that should
1265         be treated as a single filter. This is only important when it comes to
1266         determining when to reuse tables for many-to-many filters. Required so
1267         that we can filter naturally on the results of related managers.
1268 
1269         This doesn't return a clone of the current QuerySet (it returns
1270         "self"). The method is only used internally and should be immediately
1271         followed by a filter() that does create a clone.
1272         """
1273         self._sticky_filter = True
1274         return self
1275 
1276     def _merge_sanity_check(self, other):
1277         """Check that two QuerySet classes may be merged."""
1278         if self._fields is not None and (
1279                 set(self.query.values_select) != set(other.query.values_select) or
1280                 set(self.query.extra_select) != set(other.query.extra_select) or
1281                 set(self.query.annotation_select) != set(other.query.annotation_select)):
1282             raise TypeError(
1283                 "Merging '%s' classes must involve the same values in each case."
1284                 % self.__class__.__name__
1285             )
1286 
1287     def _merge_known_related_objects(self, other):
1288         """
1289         Keep track of all known related objects from either QuerySet instance.
1290         """
1291         for field, objects in other._known_related_objects.items():
1292             self._known_related_objects.setdefault(field, {}).update(objects)
1293 
1294     def resolve_expression(self, *args, **kwargs):
1295         if self._fields and len(self._fields) > 1:
1296             # values() queryset can only be used as nested queries
1297             # if they are set up to select only a single field.
1298             raise TypeError('Cannot use multi-field values as a filter value.')
1299         query = self.query.resolve_expression(*args, **kwargs)
1300         query._db = self._db
1301         return query
1302     resolve_expression.queryset_only = True
1303 
1304     def _add_hints(self, **hints):
1305         """
1306         Update hinting information for use by routers. Add new key/values or
1307         overwrite existing key/values.
1308         """
1309         self._hints.update(hints)
1310 
1311     def _has_filters(self):
1312         """
1313         Check if this QuerySet has any filtering going on. This isn't
1314         equivalent with checking if all objects are present in results, for
1315         example, qs[1:]._has_filters() -> False.
1316         """
1317         return self.query.has_filters()
1318 
1319     @staticmethod
1320     def _validate_values_are_expressions(values, method_name):
1321         invalid_args = sorted(str(arg) for arg in values if not hasattr(arg, 'resolve_expression'))
1322         if invalid_args:
1323             raise TypeError(
1324                 'QuerySet.%s() received non-expression(s): %s.' % (
1325                     method_name,
1326                     ', '.join(invalid_args),
1327                 )
1328             )
1329 
1330     def _not_support_combined_queries(self, operation_name):
1331         if self.query.combinator:
1332             raise NotSupportedError(
1333                 'Calling QuerySet.%s() after %s() is not supported.'
1334                 % (operation_name, self.query.combinator)
1335             )

</code><intended_behavior>Inside the `filter` and `_filter_or_exclude` methods, whenever a `__isnull` lookup is encountered, introduce a validation check to assert that the value associated with `__isnull` is a boolean (`True` or `False`). If a non-boolean value is detected, the method should raise a `ValueError` with a clear message indicating that `__isnull` only accepts boolean values. This preserves the existing functionality of allowing to filter querysets based on whether a field is `NULL` or not, but enforces correct usage and prevents the misuse of the lookup with unintended value types.</intended_behavior>, <file>django/db/backends/mysql/compiler.py</file>
<class>SQLCompiler</class>
<method>None</method>
<code>
4 class SQLCompiler(compiler.SQLCompiler):
5     def as_subquery_condition(self, alias, columns, compiler):
6         qn = compiler.quote_name_unless_alias
7         qn2 = self.connection.ops.quote_name
8         sql, params = self.as_sql()
9         return '(%s) IN (%s)' % (', '.join('%s.%s' % (qn(alias), qn2(column)) for column in columns), sql), params

</code><intended_behavior>While not directly modifying this class for the boolean validation, ensuring that the `SQLCompiler` has access to the validation logic or error raised in `QuerySet`'s methods might help with broader error handling and messaging. The actual change would be in the queryset filtering logic, but attention should be given here to ensure that errors are gracefully handled and correctly propagate up to developers when incorrect `__isnull` usage is attempted. The intended behavior is not so much an alteration of `SQLCompiler` but an acknowledgment that changes in `QuerySet` logic may affect how SQL compilation errors are caught and reported.</intended_behavior>, <file>django/db/models/sql/compiler.py</file>
<class>SQLCompiler</class>
<method>None</method>
<code>
20 class SQLCompiler:
21     def __init__(self, query, connection, using):
22         self.query = query
23         self.connection = connection
24         self.using = using
25         self.quote_cache = {'*': '*'}
26         # The select, klass_info, and annotations are needed by QuerySet.iterator()
27         # these are set as a side-effect of executing the query. Note that we calculate
28         # separately a list of extra select columns needed for grammatical correctness
29         # of the query, but these columns are not included in self.select.
30         self.select = None
31         self.annotation_col_map = None
32         self.klass_info = None
33         # Multiline ordering SQL clause may appear from RawSQL.
34         self.ordering_parts = re.compile(r'^(.*)\s(ASC|DESC)(.*)', re.MULTILINE | re.DOTALL)
35         self._meta_ordering = None
36 
37     def setup_query(self):
38         if all(self.query.alias_refcount[a] == 0 for a in self.query.alias_map):
39             self.query.get_initial_alias()
40         self.select, self.klass_info, self.annotation_col_map = self.get_select()
41         self.col_count = len(self.select)
42 
43     def pre_sql_setup(self):
44         """
45         Do any necessary class setup immediately prior to producing SQL. This
46         is for things that can't necessarily be done in __init__ because we
47         might not have all the pieces in place at that time.
48         """
49         self.setup_query()
50         order_by = self.get_order_by()
51         self.where, self.having = self.query.where.split_having()
52         extra_select = self.get_extra_select(order_by, self.select)
53         self.has_extra_select = bool(extra_select)
54         group_by = self.get_group_by(self.select + extra_select, order_by)
55         return extra_select, order_by, group_by
56 
57     def get_group_by(self, select, order_by):
58         """
59         Return a list of 2-tuples of form (sql, params).
60 
61         The logic of what exactly the GROUP BY clause contains is hard
62         to describe in other words than "if it passes the test suite,
63         then it is correct".
64         """
65         # Some examples:
66         #     SomeModel.objects.annotate(Count('somecol'))
67         #     GROUP BY: all fields of the model
68         #
69         #    SomeModel.objects.values('name').annotate(Count('somecol'))
70         #    GROUP BY: name
71         #
72         #    SomeModel.objects.annotate(Count('somecol')).values('name')
73         #    GROUP BY: all cols of the model
74         #
75         #    SomeModel.objects.values('name', 'pk').annotate(Count('somecol')).values('pk')
76         #    GROUP BY: name, pk
77         #
78         #    SomeModel.objects.values('name').annotate(Count('somecol')).values('pk')
79         #    GROUP BY: name, pk
80         #
81         # In fact, the self.query.group_by is the minimal set to GROUP BY. It
82         # can't be ever restricted to a smaller set, but additional columns in
83         # HAVING, ORDER BY, and SELECT clauses are added to it. Unfortunately
84         # the end result is that it is impossible to force the query to have
85         # a chosen GROUP BY clause - you can almost do this by using the form:
86         #     .values(*wanted_cols).annotate(AnAggregate())
87         # but any later annotations, extra selects, values calls that
88         # refer some column outside of the wanted_cols, order_by, or even
89         # filter calls can alter the GROUP BY clause.
90 
91         # The query.group_by is either None (no GROUP BY at all), True
92         # (group by select fields), or a list of expressions to be added
93         # to the group by.
94         if self.query.group_by is None:
95             return []
96         expressions = []
97         if self.query.group_by is not True:
98             # If the group by is set to a list (by .values() call most likely),
99             # then we need to add everything in it to the GROUP BY clause.
100             # Backwards compatibility hack for setting query.group_by. Remove
101             # when  we have public API way of forcing the GROUP BY clause.
102             # Converts string references to expressions.
103             for expr in self.query.group_by:
104                 if not hasattr(expr, 'as_sql'):
105                     expressions.append(self.query.resolve_ref(expr))
106                 else:
107                     expressions.append(expr)
108         # Note that even if the group_by is set, it is only the minimal
109         # set to group by. So, we need to add cols in select, order_by, and
110         # having into the select in any case.
111         for expr, _, _ in select:
112             cols = expr.get_group_by_cols()
113             for col in cols:
114                 expressions.append(col)
115         for expr, (sql, params, is_ref) in order_by:
116             # Skip References to the select clause, as all expressions in the
117             # select clause are already part of the group by.
118             if not expr.contains_aggregate and not is_ref:
119                 expressions.extend(expr.get_source_expressions())
120         having_group_by = self.having.get_group_by_cols() if self.having else ()
121         for expr in having_group_by:
122             expressions.append(expr)
123         result = []
124         seen = set()
125         expressions = self.collapse_group_by(expressions, having_group_by)
126 
127         for expr in expressions:
128             sql, params = self.compile(expr)
129             params_hash = make_hashable(params)
130             if (sql, params_hash) not in seen:
131                 result.append((sql, params))
132                 seen.add((sql, params_hash))
133         return result
134 
135     def collapse_group_by(self, expressions, having):
136         # If the DB can group by primary key, then group by the primary key of
137         # query's main model. Note that for PostgreSQL the GROUP BY clause must
138         # include the primary key of every table, but for MySQL it is enough to
139         # have the main table's primary key.
140         if self.connection.features.allows_group_by_pk:
141             # Determine if the main model's primary key is in the query.
142             pk = None
143             for expr in expressions:
144                 # Is this a reference to query's base table primary key? If the
145                 # expression isn't a Col-like, then skip the expression.
146                 if (getattr(expr, 'target', None) == self.query.model._meta.pk and
147                         getattr(expr, 'alias', None) == self.query.base_table):
148                     pk = expr
149                     break
150             # If the main model's primary key is in the query, group by that
151             # field, HAVING expressions, and expressions associated with tables
152             # that don't have a primary key included in the grouped columns.
153             if pk:
154                 pk_aliases = {
155                     expr.alias for expr in expressions
156                     if hasattr(expr, 'target') and expr.target.primary_key
157                 }
158                 expressions = [pk] + [
159                     expr for expr in expressions
160                     if expr in having or (
161                         getattr(expr, 'alias', None) is not None and expr.alias not in pk_aliases
162                     )
163                 ]
164         elif self.connection.features.allows_group_by_selected_pks:
165             # Filter out all expressions associated with a table's primary key
166             # present in the grouped columns. This is done by identifying all
167             # tables that have their primary key included in the grouped
168             # columns and removing non-primary key columns referring to them.
169             # Unmanaged models are excluded because they could be representing
170             # database views on which the optimization might not be allowed.
171             pks = {
172                 expr for expr in expressions
173                 if (
174                     hasattr(expr, 'target') and
175                     expr.target.primary_key and
176                     self.connection.features.allows_group_by_selected_pks_on_model(expr.target.model)
177                 )
178             }
179             aliases = {expr.alias for expr in pks}
180             expressions = [
181                 expr for expr in expressions if expr in pks or getattr(expr, 'alias', None) not in aliases
182             ]
183         return expressions
184 
185     def get_select(self):
186         """
187         Return three values:
188         - a list of 3-tuples of (expression, (sql, params), alias)
189         - a klass_info structure,
190         - a dictionary of annotations
191 
192         The (sql, params) is what the expression will produce, and alias is the
193         "AS alias" for the column (possibly None).
194 
195         The klass_info structure contains the following information:
196         - The base model of the query.
197         - Which columns for that model are present in the query (by
198           position of the select clause).
199         - related_klass_infos: [f, klass_info] to descent into
200 
201         The annotations is a dictionary of {'attname': column position} values.
202         """
203         select = []
204         klass_info = None
205         annotations = {}
206         select_idx = 0
207         for alias, (sql, params) in self.query.extra_select.items():
208             annotations[alias] = select_idx
209             select.append((RawSQL(sql, params), alias))
210             select_idx += 1
211         assert not (self.query.select and self.query.default_cols)
212         if self.query.default_cols:
213             cols = self.get_default_columns()
214         else:
215             # self.query.select is a special case. These columns never go to
216             # any model.
217             cols = self.query.select
218         if cols:
219             select_list = []
220             for col in cols:
221                 select_list.append(select_idx)
222                 select.append((col, None))
223                 select_idx += 1
224             klass_info = {
225                 'model': self.query.model,
226                 'select_fields': select_list,
227             }
228         for alias, annotation in self.query.annotation_select.items():
229             annotations[alias] = select_idx
230             select.append((annotation, alias))
231             select_idx += 1
232 
233         if self.query.select_related:
234             related_klass_infos = self.get_related_selections(select)
235             klass_info['related_klass_infos'] = related_klass_infos
236 
237             def get_select_from_parent(klass_info):
238                 for ki in klass_info['related_klass_infos']:
239                     if ki['from_parent']:
240                         ki['select_fields'] = (klass_info['select_fields'] +
241                                                ki['select_fields'])
242                     get_select_from_parent(ki)
243             get_select_from_parent(klass_info)
244 
245         ret = []
246         for col, alias in select:
247             try:
248                 sql, params = self.compile(col)
249             except EmptyResultSet:
250                 # Select a predicate that's always False.
251                 sql, params = '0', ()
252             else:
253                 sql, params = col.select_format(self, sql, params)
254             ret.append((col, (sql, params), alias))
255         return ret, klass_info, annotations
256 
257     def get_order_by(self):
258         """
259         Return a list of 2-tuples of form (expr, (sql, params, is_ref)) for the
260         ORDER BY clause.
261 
262         The order_by clause can alter the select clause (for example it
263         can add aliases to clauses that do not yet have one, or it can
264         add totally new select clauses).
265         """
266         if self.query.extra_order_by:
267             ordering = self.query.extra_order_by
268         elif not self.query.default_ordering:
269             ordering = self.query.order_by
270         elif self.query.order_by:
271             ordering = self.query.order_by
272         elif self.query.get_meta().ordering:
273             ordering = self.query.get_meta().ordering
274             self._meta_ordering = ordering
275         else:
276             ordering = []
277         if self.query.standard_ordering:
278             asc, desc = ORDER_DIR['ASC']
279         else:
280             asc, desc = ORDER_DIR['DESC']
281 
282         order_by = []
283         for field in ordering:
284             if hasattr(field, 'resolve_expression'):
285                 if isinstance(field, Value):
286                     # output_field must be resolved for constants.
287                     field = Cast(field, field.output_field)
288                 if not isinstance(field, OrderBy):
289                     field = field.asc()
290                 if not self.query.standard_ordering:
291                     field = field.copy()
292                     field.reverse_ordering()
293                 order_by.append((field, False))
294                 continue
295             if field == '?':  # random
296                 order_by.append((OrderBy(Random()), False))
297                 continue
298 
299             col, order = get_order_dir(field, asc)
300             descending = order == 'DESC'
301 
302             if col in self.query.annotation_select:
303                 # Reference to expression in SELECT clause
304                 order_by.append((
305                     OrderBy(Ref(col, self.query.annotation_select[col]), descending=descending),
306                     True))
307                 continue
308             if col in self.query.annotations:
309                 # References to an expression which is masked out of the SELECT
310                 # clause.
311                 expr = self.query.annotations[col]
312                 if isinstance(expr, Value):
313                     # output_field must be resolved for constants.
314                     expr = Cast(expr, expr.output_field)
315                 order_by.append((OrderBy(expr, descending=descending), False))
316                 continue
317 
318             if '.' in field:
319                 # This came in through an extra(order_by=...) addition. Pass it
320                 # on verbatim.
321                 table, col = col.split('.', 1)
322                 order_by.append((
323                     OrderBy(
324                         RawSQL('%s.%s' % (self.quote_name_unless_alias(table), col), []),
325                         descending=descending
326                     ), False))
327                 continue
328 
329             if not self.query.extra or col not in self.query.extra:
330                 # 'col' is of the form 'field' or 'field1__field2' or
331                 # '-field1__field2__field', etc.
332                 order_by.extend(self.find_ordering_name(
333                     field, self.query.get_meta(), default_order=asc))
334             else:
335                 if col not in self.query.extra_select:
336                     order_by.append((
337                         OrderBy(RawSQL(*self.query.extra[col]), descending=descending),
338                         False))
339                 else:
340                     order_by.append((
341                         OrderBy(Ref(col, RawSQL(*self.query.extra[col])), descending=descending),
342                         True))
343         result = []
344         seen = set()
345 
346         for expr, is_ref in order_by:
347             resolved = expr.resolve_expression(self.query, allow_joins=True, reuse=None)
348             if self.query.combinator:
349                 src = resolved.get_source_expressions()[0]
350                 # Relabel order by columns to raw numbers if this is a combined
351                 # query; necessary since the columns can't be referenced by the
352                 # fully qualified name and the simple column names may collide.
353                 for idx, (sel_expr, _, col_alias) in enumerate(self.select):
354                     if is_ref and col_alias == src.refs:
355                         src = src.source
356                     elif col_alias:
357                         continue
358                     if src == sel_expr:
359                         resolved.set_source_expressions([RawSQL('%d' % (idx + 1), ())])
360                         break
361                 else:
362                     if col_alias:
363                         raise DatabaseError('ORDER BY term does not match any column in the result set.')
364                     # Add column used in ORDER BY clause without an alias to
365                     # the selected columns.
366                     self.query.add_select_col(src)
367                     resolved.set_source_expressions([RawSQL('%d' % len(self.query.select), ())])
368             sql, params = self.compile(resolved)
369             # Don't add the same column twice, but the order direction is
370             # not taken into account so we strip it. When this entire method
371             # is refactored into expressions, then we can check each part as we
372             # generate it.
373             without_ordering = self.ordering_parts.search(sql).group(1)
374             params_hash = make_hashable(params)
375             if (without_ordering, params_hash) in seen:
376                 continue
377             seen.add((without_ordering, params_hash))
378             result.append((resolved, (sql, params, is_ref)))
379         return result
380 
381     def get_extra_select(self, order_by, select):
382         extra_select = []
383         if self.query.distinct and not self.query.distinct_fields:
384             select_sql = [t[1] for t in select]
385             for expr, (sql, params, is_ref) in order_by:
386                 without_ordering = self.ordering_parts.search(sql).group(1)
387                 if not is_ref and (without_ordering, params) not in select_sql:
388                     extra_select.append((expr, (without_ordering, params), None))
389         return extra_select
390 
391     def quote_name_unless_alias(self, name):
392         """
393         A wrapper around connection.ops.quote_name that doesn't quote aliases
394         for table names. This avoids problems with some SQL dialects that treat
395         quoted strings specially (e.g. PostgreSQL).
396         """
397         if name in self.quote_cache:
398             return self.quote_cache[name]
399         if ((name in self.query.alias_map and name not in self.query.table_map) or
400                 name in self.query.extra_select or (
401                     name in self.query.external_aliases and name not in self.query.table_map)):
402             self.quote_cache[name] = name
403             return name
404         r = self.connection.ops.quote_name(name)
405         self.quote_cache[name] = r
406         return r
407 
408     def compile(self, node):
409         vendor_impl = getattr(node, 'as_' + self.connection.vendor, None)
410         if vendor_impl:
411             sql, params = vendor_impl(self, self.connection)
412         else:
413             sql, params = node.as_sql(self, self.connection)
414         return sql, params
415 
416     def get_combinator_sql(self, combinator, all):
417         features = self.connection.features
418         compilers = [
419             query.get_compiler(self.using, self.connection)
420             for query in self.query.combined_queries if not query.is_empty()
421         ]
422         if not features.supports_slicing_ordering_in_compound:
423             for query, compiler in zip(self.query.combined_queries, compilers):
424                 if query.low_mark or query.high_mark:
425                     raise DatabaseError('LIMIT/OFFSET not allowed in subqueries of compound statements.')
426                 if compiler.get_order_by():
427                     raise DatabaseError('ORDER BY not allowed in subqueries of compound statements.')
428         parts = ()
429         for compiler in compilers:
430             try:
431                 # If the columns list is limited, then all combined queries
432                 # must have the same columns list. Set the selects defined on
433                 # the query on all combined queries, if not already set.
434                 if not compiler.query.values_select and self.query.values_select:
435                     compiler.query = compiler.query.clone()
436                     compiler.query.set_values((
437                         *self.query.extra_select,
438                         *self.query.values_select,
439                         *self.query.annotation_select,
440                     ))
441                 part_sql, part_args = compiler.as_sql()
442                 if compiler.query.combinator:
443                     # Wrap in a subquery if wrapping in parentheses isn't
444                     # supported.
445                     if not features.supports_parentheses_in_compound:
446                         part_sql = 'SELECT * FROM ({})'.format(part_sql)
447                     # Add parentheses when combining with compound query if not
448                     # already added for all compound queries.
449                     elif not features.supports_slicing_ordering_in_compound:
450                         part_sql = '({})'.format(part_sql)
451                 parts += ((part_sql, part_args),)
452             except EmptyResultSet:
453                 # Omit the empty queryset with UNION and with DIFFERENCE if the
454                 # first queryset is nonempty.
455                 if combinator == 'union' or (combinator == 'difference' and parts):
456                     continue
457                 raise
458         if not parts:
459             raise EmptyResultSet
460         combinator_sql = self.connection.ops.set_operators[combinator]
461         if all and combinator == 'union':
462             combinator_sql += ' ALL'
463         braces = '({})' if features.supports_slicing_ordering_in_compound else '{}'
464         sql_parts, args_parts = zip(*((braces.format(sql), args) for sql, args in parts))
465         result = [' {} '.format(combinator_sql).join(sql_parts)]
466         params = []
467         for part in args_parts:
468             params.extend(part)
469         return result, params
470 
471     def as_sql(self, with_limits=True, with_col_aliases=False):
472         """
473         Create the SQL for this query. Return the SQL string and list of
474         parameters.
475 
476         If 'with_limits' is False, any limit/offset information is not included
477         in the query.
478         """
479         refcounts_before = self.query.alias_refcount.copy()
480         try:
481             extra_select, order_by, group_by = self.pre_sql_setup()
482             for_update_part = None
483             # Is a LIMIT/OFFSET clause needed?
484             with_limit_offset = with_limits and (self.query.high_mark is not None or self.query.low_mark)
485             combinator = self.query.combinator
486             features = self.connection.features
487             if combinator:
488                 if not getattr(features, 'supports_select_{}'.format(combinator)):
489                     raise NotSupportedError('{} is not supported on this database backend.'.format(combinator))
490                 result, params = self.get_combinator_sql(combinator, self.query.combinator_all)
491             else:
492                 distinct_fields, distinct_params = self.get_distinct()
493                 # This must come after 'select', 'ordering', and 'distinct'
494                 # (see docstring of get_from_clause() for details).
495                 from_, f_params = self.get_from_clause()
496                 where, w_params = self.compile(self.where) if self.where is not None else ("", [])
497                 having, h_params = self.compile(self.having) if self.having is not None else ("", [])
498                 result = ['SELECT']
499                 params = []
500 
501                 if self.query.distinct:
502                     distinct_result, distinct_params = self.connection.ops.distinct_sql(
503                         distinct_fields,
504                         distinct_params,
505                     )
506                     result += distinct_result
507                     params += distinct_params
508 
509                 out_cols = []
510                 col_idx = 1
511                 for _, (s_sql, s_params), alias in self.select + extra_select:
512                     if alias:
513                         s_sql = '%s AS %s' % (s_sql, self.connection.ops.quote_name(alias))
514                     elif with_col_aliases:
515                         s_sql = '%s AS %s' % (s_sql, 'Col%d' % col_idx)
516                         col_idx += 1
517                     params.extend(s_params)
518                     out_cols.append(s_sql)
519 
520                 result += [', '.join(out_cols), 'FROM', *from_]
521                 params.extend(f_params)
522 
523                 if self.query.select_for_update and self.connection.features.has_select_for_update:
524                     if self.connection.get_autocommit():
525                         raise TransactionManagementError('select_for_update cannot be used outside of a transaction.')
526 
527                     if with_limit_offset and not self.connection.features.supports_select_for_update_with_limit:
528                         raise NotSupportedError(
529                             'LIMIT/OFFSET is not supported with '
530                             'select_for_update on this database backend.'
531                         )
532                     nowait = self.query.select_for_update_nowait
533                     skip_locked = self.query.select_for_update_skip_locked
534                     of = self.query.select_for_update_of
535                     # If it's a NOWAIT/SKIP LOCKED/OF query but the backend
536                     # doesn't support it, raise NotSupportedError to prevent a
537                     # possible deadlock.
538                     if nowait and not self.connection.features.has_select_for_update_nowait:
539                         raise NotSupportedError('NOWAIT is not supported on this database backend.')
540                     elif skip_locked and not self.connection.features.has_select_for_update_skip_locked:
541                         raise NotSupportedError('SKIP LOCKED is not supported on this database backend.')
542                     elif of and not self.connection.features.has_select_for_update_of:
543                         raise NotSupportedError('FOR UPDATE OF is not supported on this database backend.')
544                     for_update_part = self.connection.ops.for_update_sql(
545                         nowait=nowait,
546                         skip_locked=skip_locked,
547                         of=self.get_select_for_update_of_arguments(),
548                     )
549 
550                 if for_update_part and self.connection.features.for_update_after_from:
551                     result.append(for_update_part)
552 
553                 if where:
554                     result.append('WHERE %s' % where)
555                     params.extend(w_params)
556 
557                 grouping = []
558                 for g_sql, g_params in group_by:
559                     grouping.append(g_sql)
560                     params.extend(g_params)
561                 if grouping:
562                     if distinct_fields:
563                         raise NotImplementedError('annotate() + distinct(fields) is not implemented.')
564                     order_by = order_by or self.connection.ops.force_no_ordering()
565                     result.append('GROUP BY %s' % ', '.join(grouping))
566                     if self._meta_ordering:
567                         order_by = None
568                 if having:
569                     result.append('HAVING %s' % having)
570                     params.extend(h_params)
571 
572             if self.query.explain_query:
573                 result.insert(0, self.connection.ops.explain_query_prefix(
574                     self.query.explain_format,
575                     **self.query.explain_options
576                 ))
577 
578             if order_by:
579                 ordering = []
580                 for _, (o_sql, o_params, _) in order_by:
581                     ordering.append(o_sql)
582                     params.extend(o_params)
583                 result.append('ORDER BY %s' % ', '.join(ordering))
584 
585             if with_limit_offset:
586                 result.append(self.connection.ops.limit_offset_sql(self.query.low_mark, self.query.high_mark))
587 
588             if for_update_part and not self.connection.features.for_update_after_from:
589                 result.append(for_update_part)
590 
591             if self.query.subquery and extra_select:
592                 # If the query is used as a subquery, the extra selects would
593                 # result in more columns than the left-hand side expression is
594                 # expecting. This can happen when a subquery uses a combination
595                 # of order_by() and distinct(), forcing the ordering expressions
596                 # to be selected as well. Wrap the query in another subquery
597                 # to exclude extraneous selects.
598                 sub_selects = []
599                 sub_params = []
600                 for index, (select, _, alias) in enumerate(self.select, start=1):
601                     if not alias and with_col_aliases:
602                         alias = 'col%d' % index
603                     if alias:
604                         sub_selects.append("%s.%s" % (
605                             self.connection.ops.quote_name('subquery'),
606                             self.connection.ops.quote_name(alias),
607                         ))
608                     else:
609                         select_clone = select.relabeled_clone({select.alias: 'subquery'})
610                         subselect, subparams = select_clone.as_sql(self, self.connection)
611                         sub_selects.append(subselect)
612                         sub_params.extend(subparams)
613                 return 'SELECT %s FROM (%s) subquery' % (
614                     ', '.join(sub_selects),
615                     ' '.join(result),
616                 ), tuple(sub_params + params)
617 
618             return ' '.join(result), tuple(params)
619         finally:
620             # Finally do cleanup - get rid of the joins we created above.
621             self.query.reset_refcounts(refcounts_before)
622 
623     def get_default_columns(self, start_alias=None, opts=None, from_parent=None):
624         """
625         Compute the default columns for selecting every field in the base
626         model. Will sometimes be called to pull in related models (e.g. via
627         select_related), in which case "opts" and "start_alias" will be given
628         to provide a starting point for the traversal.
629 
630         Return a list of strings, quoted appropriately for use in SQL
631         directly, as well as a set of aliases used in the select statement (if
632         'as_pairs' is True, return a list of (alias, col_name) pairs instead
633         of strings as the first component and None as the second component).
634         """
635         result = []
636         if opts is None:
637             opts = self.query.get_meta()
638         only_load = self.deferred_to_columns()
639         start_alias = start_alias or self.query.get_initial_alias()
640         # The 'seen_models' is used to optimize checking the needed parent
641         # alias for a given field. This also includes None -> start_alias to
642         # be used by local fields.
643         seen_models = {None: start_alias}
644 
645         for field in opts.concrete_fields:
646             model = field.model._meta.concrete_model
647             # A proxy model will have a different model and concrete_model. We
648             # will assign None if the field belongs to this model.
649             if model == opts.model:
650                 model = None
651             if from_parent and model is not None and issubclass(
652                     from_parent._meta.concrete_model, model._meta.concrete_model):
653                 # Avoid loading data for already loaded parents.
654                 # We end up here in the case select_related() resolution
655                 # proceeds from parent model to child model. In that case the
656                 # parent model data is already present in the SELECT clause,
657                 # and we want to avoid reloading the same data again.
658                 continue
659             if field.model in only_load and field.attname not in only_load[field.model]:
660                 continue
661             alias = self.query.join_parent_model(opts, model, start_alias,
662                                                  seen_models)
663             column = field.get_col(alias)
664             result.append(column)
665         return result
666 
667     def get_distinct(self):
668         """
669         Return a quoted list of fields to use in DISTINCT ON part of the query.
670 
671         This method can alter the tables in the query, and thus it must be
672         called before get_from_clause().
673         """
674         result = []
675         params = []
676         opts = self.query.get_meta()
677 
678         for name in self.query.distinct_fields:
679             parts = name.split(LOOKUP_SEP)
680             _, targets, alias, joins, path, _, transform_function = self._setup_joins(parts, opts, None)
681             targets, alias, _ = self.query.trim_joins(targets, joins, path)
682             for target in targets:
683                 if name in self.query.annotation_select:
684                     result.append(name)
685                 else:
686                     r, p = self.compile(transform_function(target, alias))
687                     result.append(r)
688                     params.append(p)
689         return result, params
690 
691     def find_ordering_name(self, name, opts, alias=None, default_order='ASC',
692                            already_seen=None):
693         """
694         Return the table alias (the name might be ambiguous, the alias will
695         not be) and column name for ordering by the given 'name' parameter.
696         The 'name' is of the form 'field1__field2__...__fieldN'.
697         """
698         name, order = get_order_dir(name, default_order)
699         descending = order == 'DESC'
700         pieces = name.split(LOOKUP_SEP)
701         field, targets, alias, joins, path, opts, transform_function = self._setup_joins(pieces, opts, alias)
702 
703         # If we get to this point and the field is a relation to another model,
704         # append the default ordering for that model unless the attribute name
705         # of the field is specified.
706         if field.is_relation and opts.ordering and getattr(field, 'attname', None) != name:
707             # Firstly, avoid infinite loops.
708             already_seen = already_seen or set()
709             join_tuple = tuple(getattr(self.query.alias_map[j], 'join_cols', None) for j in joins)
710             if join_tuple in already_seen:
711                 raise FieldError('Infinite loop caused by ordering.')
712             already_seen.add(join_tuple)
713 
714             results = []
715             for item in opts.ordering:
716                 if hasattr(item, 'resolve_expression') and not isinstance(item, OrderBy):
717                     item = item.desc() if descending else item.asc()
718                 if isinstance(item, OrderBy):
719                     results.append((item, False))
720                     continue
721                 results.extend(self.find_ordering_name(item, opts, alias,
722                                                        order, already_seen))
723             return results
724         targets, alias, _ = self.query.trim_joins(targets, joins, path)
725         return [(OrderBy(transform_function(t, alias), descending=descending), False) for t in targets]
726 
727     def _setup_joins(self, pieces, opts, alias):
728         """
729         Helper method for get_order_by() and get_distinct().
730 
731         get_ordering() and get_distinct() must produce same target columns on
732         same input, as the prefixes of get_ordering() and get_distinct() must
733         match. Executing SQL where this is not true is an error.
734         """
735         alias = alias or self.query.get_initial_alias()
736         field, targets, opts, joins, path, transform_function = self.query.setup_joins(pieces, opts, alias)
737         alias = joins[-1]
738         return field, targets, alias, joins, path, opts, transform_function
739 
740     def get_from_clause(self):
741         """
742         Return a list of strings that are joined together to go after the
743         "FROM" part of the query, as well as a list any extra parameters that
744         need to be included. Subclasses, can override this to create a
745         from-clause via a "select".
746 
747         This should only be called after any SQL construction methods that
748         might change the tables that are needed. This means the select columns,
749         ordering, and distinct must be done first.
750         """
751         result = []
752         params = []
753         for alias in tuple(self.query.alias_map):
754             if not self.query.alias_refcount[alias]:
755                 continue
756             try:
757                 from_clause = self.query.alias_map[alias]
758             except KeyError:
759                 # Extra tables can end up in self.tables, but not in the
760                 # alias_map if they aren't in a join. That's OK. We skip them.
761                 continue
762             clause_sql, clause_params = self.compile(from_clause)
763             result.append(clause_sql)
764             params.extend(clause_params)
765         for t in self.query.extra_tables:
766             alias, _ = self.query.table_alias(t)
767             # Only add the alias if it's not already present (the table_alias()
768             # call increments the refcount, so an alias refcount of one means
769             # this is the only reference).
770             if alias not in self.query.alias_map or self.query.alias_refcount[alias] == 1:
771                 result.append(', %s' % self.quote_name_unless_alias(alias))
772         return result, params
773 
774     def get_related_selections(self, select, opts=None, root_alias=None, cur_depth=1,
775                                requested=None, restricted=None):
776         """
777         Fill in the information needed for a select_related query. The current
778         depth is measured as the number of connections away from the root model
779         (for example, cur_depth=1 means we are looking at models with direct
780         connections to the root model).
781         """
782         def _get_field_choices():
783             direct_choices = (f.name for f in opts.fields if f.is_relation)
784             reverse_choices = (
785                 f.field.related_query_name()
786                 for f in opts.related_objects if f.field.unique
787             )
788             return chain(direct_choices, reverse_choices, self.query._filtered_relations)
789 
790         related_klass_infos = []
791         if not restricted and cur_depth > self.query.max_depth:
792             # We've recursed far enough; bail out.
793             return related_klass_infos
794 
795         if not opts:
796             opts = self.query.get_meta()
797             root_alias = self.query.get_initial_alias()
798         only_load = self.query.get_loaded_field_names()
799 
800         # Setup for the case when only particular related fields should be
801         # included in the related selection.
802         fields_found = set()
803         if requested is None:
804             restricted = isinstance(self.query.select_related, dict)
805             if restricted:
806                 requested = self.query.select_related
807 
808         def get_related_klass_infos(klass_info, related_klass_infos):
809             klass_info['related_klass_infos'] = related_klass_infos
810 
811         for f in opts.fields:
812             field_model = f.model._meta.concrete_model
813             fields_found.add(f.name)
814 
815             if restricted:
816                 next = requested.get(f.name, {})
817                 if not f.is_relation:
818                     # If a non-related field is used like a relation,
819                     # or if a single non-relational field is given.
820                     if next or f.name in requested:
821                         raise FieldError(
822                             "Non-relational field given in select_related: '%s'. "
823                             "Choices are: %s" % (
824                                 f.name,
825                                 ", ".join(_get_field_choices()) or '(none)',
826                             )
827                         )
828             else:
829                 next = False
830 
831             if not select_related_descend(f, restricted, requested,
832                                           only_load.get(field_model)):
833                 continue
834             klass_info = {
835                 'model': f.remote_field.model,
836                 'field': f,
837                 'reverse': False,
838                 'local_setter': f.set_cached_value,
839                 'remote_setter': f.remote_field.set_cached_value if f.unique else lambda x, y: None,
840                 'from_parent': False,
841             }
842             related_klass_infos.append(klass_info)
843             select_fields = []
844             _, _, _, joins, _, _ = self.query.setup_joins(
845                 [f.name], opts, root_alias)
846             alias = joins[-1]
847             columns = self.get_default_columns(start_alias=alias, opts=f.remote_field.model._meta)
848             for col in columns:
849                 select_fields.append(len(select))
850                 select.append((col, None))
851             klass_info['select_fields'] = select_fields
852             next_klass_infos = self.get_related_selections(
853                 select, f.remote_field.model._meta, alias, cur_depth + 1, next, restricted)
854             get_related_klass_infos(klass_info, next_klass_infos)
855 
856         if restricted:
857             related_fields = [
858                 (o.field, o.related_model)
859                 for o in opts.related_objects
860                 if o.field.unique and not o.many_to_many
861             ]
862             for f, model in related_fields:
863                 if not select_related_descend(f, restricted, requested,
864                                               only_load.get(model), reverse=True):
865                     continue
866 
867                 related_field_name = f.related_query_name()
868                 fields_found.add(related_field_name)
869 
870                 join_info = self.query.setup_joins([related_field_name], opts, root_alias)
871                 alias = join_info.joins[-1]
872                 from_parent = issubclass(model, opts.model) and model is not opts.model
873                 klass_info = {
874                     'model': model,
875                     'field': f,
876                     'reverse': True,
877                     'local_setter': f.remote_field.set_cached_value,
878                     'remote_setter': f.set_cached_value,
879                     'from_parent': from_parent,
880                 }
881                 related_klass_infos.append(klass_info)
882                 select_fields = []
883                 columns = self.get_default_columns(
884                     start_alias=alias, opts=model._meta, from_parent=opts.model)
885                 for col in columns:
886                     select_fields.append(len(select))
887                     select.append((col, None))
888                 klass_info['select_fields'] = select_fields
889                 next = requested.get(f.related_query_name(), {})
890                 next_klass_infos = self.get_related_selections(
891                     select, model._meta, alias, cur_depth + 1,
892                     next, restricted)
893                 get_related_klass_infos(klass_info, next_klass_infos)
894 
895             def local_setter(obj, from_obj):
896                 # Set a reverse fk object when relation is non-empty.
897                 if from_obj:
898                     f.remote_field.set_cached_value(from_obj, obj)
899 
900             def remote_setter(name, obj, from_obj):
901                 setattr(from_obj, name, obj)
902 
903             for name in list(requested):
904                 # Filtered relations work only on the topmost level.
905                 if cur_depth > 1:
906                     break
907                 if name in self.query._filtered_relations:
908                     fields_found.add(name)
909                     f, _, join_opts, joins, _, _ = self.query.setup_joins([name], opts, root_alias)
910                     model = join_opts.model
911                     alias = joins[-1]
912                     from_parent = issubclass(model, opts.model) and model is not opts.model
913                     klass_info = {
914                         'model': model,
915                         'field': f,
916                         'reverse': True,
917                         'local_setter': local_setter,
918                         'remote_setter': partial(remote_setter, name),
919                         'from_parent': from_parent,
920                     }
921                     related_klass_infos.append(klass_info)
922                     select_fields = []
923                     columns = self.get_default_columns(
924                         start_alias=alias, opts=model._meta,
925                         from_parent=opts.model,
926                     )
927                     for col in columns:
928                         select_fields.append(len(select))
929                         select.append((col, None))
930                     klass_info['select_fields'] = select_fields
931                     next_requested = requested.get(name, {})
932                     next_klass_infos = self.get_related_selections(
933                         select, opts=model._meta, root_alias=alias,
934                         cur_depth=cur_depth + 1, requested=next_requested,
935                         restricted=restricted,
936                     )
937                     get_related_klass_infos(klass_info, next_klass_infos)
938             fields_not_found = set(requested).difference(fields_found)
939             if fields_not_found:
940                 invalid_fields = ("'%s'" % s for s in fields_not_found)
941                 raise FieldError(
942                     'Invalid field name(s) given in select_related: %s. '
943                     'Choices are: %s' % (
944                         ', '.join(invalid_fields),
945                         ', '.join(_get_field_choices()) or '(none)',
946                     )
947                 )
948         return related_klass_infos
949 
950     def get_select_for_update_of_arguments(self):
951         """
952         Return a quoted list of arguments for the SELECT FOR UPDATE OF part of
953         the query.
954         """
955         def _get_field_choices():
956             """Yield all allowed field paths in breadth-first search order."""
957             queue = collections.deque([(None, self.klass_info)])
958             while queue:
959                 parent_path, klass_info = queue.popleft()
960                 if parent_path is None:
961                     path = []
962                     yield 'self'
963                 else:
964                     field = klass_info['field']
965                     if klass_info['reverse']:
966                         field = field.remote_field
967                     path = parent_path + [field.name]
968                     yield LOOKUP_SEP.join(path)
969                 queue.extend(
970                     (path, klass_info)
971                     for klass_info in klass_info.get('related_klass_infos', [])
972                 )
973         result = []
974         invalid_names = []
975         for name in self.query.select_for_update_of:
976             parts = [] if name == 'self' else name.split(LOOKUP_SEP)
977             klass_info = self.klass_info
978             for part in parts:
979                 for related_klass_info in klass_info.get('related_klass_infos', []):
980                     field = related_klass_info['field']
981                     if related_klass_info['reverse']:
982                         field = field.remote_field
983                     if field.name == part:
984                         klass_info = related_klass_info
985                         break
986                 else:
987                     klass_info = None
988                     break
989             if klass_info is None:
990                 invalid_names.append(name)
991                 continue
992             select_index = klass_info['select_fields'][0]
993             col = self.select[select_index][0]
994             if self.connection.features.select_for_update_of_column:
995                 result.append(self.compile(col)[0])
996             else:
997                 result.append(self.quote_name_unless_alias(col.alias))
998         if invalid_names:
999             raise FieldError(
1000                 'Invalid field name(s) given in select_for_update(of=(...)): %s. '
1001                 'Only relational fields followed in the query are allowed. '
1002                 'Choices are: %s.' % (
1003                     ', '.join(invalid_names),
1004                     ', '.join(_get_field_choices()),
1005                 )
1006             )
1007         return result
1008 
1009     def deferred_to_columns(self):
1010         """
1011         Convert the self.deferred_loading data structure to mapping of table
1012         names to sets of column names which are to be loaded. Return the
1013         dictionary.
1014         """
1015         columns = {}
1016         self.query.deferred_to_data(columns, self.query.get_loaded_field_names_cb)
1017         return columns
1018 
1019     def get_converters(self, expressions):
1020         converters = {}
1021         for i, expression in enumerate(expressions):
1022             if expression:
1023                 backend_converters = self.connection.ops.get_db_converters(expression)
1024                 field_converters = expression.get_db_converters(self.connection)
1025                 if backend_converters or field_converters:
1026                     converters[i] = (backend_converters + field_converters, expression)
1027         return converters
1028 
1029     def apply_converters(self, rows, converters):
1030         connection = self.connection
1031         converters = list(converters.items())
1032         for row in map(list, rows):
1033             for pos, (convs, expression) in converters:
1034                 value = row[pos]
1035                 for converter in convs:
1036                     value = converter(value, expression, connection)
1037                 row[pos] = value
1038             yield row
1039 
1040     def results_iter(self, results=None, tuple_expected=False, chunked_fetch=False,
1041                      chunk_size=GET_ITERATOR_CHUNK_SIZE):
1042         """Return an iterator over the results from executing this query."""
1043         if results is None:
1044             results = self.execute_sql(MULTI, chunked_fetch=chunked_fetch, chunk_size=chunk_size)
1045         fields = [s[0] for s in self.select[0:self.col_count]]
1046         converters = self.get_converters(fields)
1047         rows = chain.from_iterable(results)
1048         if converters:
1049             rows = self.apply_converters(rows, converters)
1050             if tuple_expected:
1051                 rows = map(tuple, rows)
1052         return rows
1053 
1054     def has_results(self):
1055         """
1056         Backends (e.g. NoSQL) can override this in order to use optimized
1057         versions of "query has any results."
1058         """
1059         # This is always executed on a query clone, so we can modify self.query
1060         self.query.add_extra({'a': 1}, None, None, None, None, None)
1061         self.query.set_extra_mask(['a'])
1062         return bool(self.execute_sql(SINGLE))
1063 
1064     def execute_sql(self, result_type=MULTI, chunked_fetch=False, chunk_size=GET_ITERATOR_CHUNK_SIZE):
1065         """
1066         Run the query against the database and return the result(s). The
1067         return value is a single data item if result_type is SINGLE, or an
1068         iterator over the results if the result_type is MULTI.
1069 
1070         result_type is either MULTI (use fetchmany() to retrieve all rows),
1071         SINGLE (only retrieve a single row), or None. In this last case, the
1072         cursor is returned if any query is executed, since it's used by
1073         subclasses such as InsertQuery). It's possible, however, that no query
1074         is needed, as the filters describe an empty set. In that case, None is
1075         returned, to avoid any unnecessary database interaction.
1076         """
1077         result_type = result_type or NO_RESULTS
1078         try:
1079             sql, params = self.as_sql()
1080             if not sql:
1081                 raise EmptyResultSet
1082         except EmptyResultSet:
1083             if result_type == MULTI:
1084                 return iter([])
1085             else:
1086                 return
1087         if chunked_fetch:
1088             cursor = self.connection.chunked_cursor()
1089         else:
1090             cursor = self.connection.cursor()
1091         try:
1092             cursor.execute(sql, params)
1093         except Exception:
1094             # Might fail for server-side cursors (e.g. connection closed)
1095             cursor.close()
1096             raise
1097 
1098         if result_type == CURSOR:
1099             # Give the caller the cursor to process and close.
1100             return cursor
1101         if result_type == SINGLE:
1102             try:
1103                 val = cursor.fetchone()
1104                 if val:
1105                     return val[0:self.col_count]
1106                 return val
1107             finally:
1108                 # done with the cursor
1109                 cursor.close()
1110         if result_type == NO_RESULTS:
1111             cursor.close()
1112             return
1113 
1114         result = cursor_iter(
1115             cursor, self.connection.features.empty_fetchmany_value,
1116             self.col_count if self.has_extra_select else None,
1117             chunk_size,
1118         )
1119         if not chunked_fetch or not self.connection.features.can_use_chunked_reads:
1120             try:
1121                 # If we are using non-chunked reads, we return the same data
1122                 # structure as normally, but ensure it is all read into memory
1123                 # before going any further. Use chunked_fetch if requested,
1124                 # unless the database doesn't support it.
1125                 return list(result)
1126             finally:
1127                 # done with the cursor
1128                 cursor.close()
1129         return result
1130 
1131     def as_subquery_condition(self, alias, columns, compiler):
1132         qn = compiler.quote_name_unless_alias
1133         qn2 = self.connection.ops.quote_name
1134 
1135         for index, select_col in enumerate(self.query.select):
1136             lhs_sql, lhs_params = self.compile(select_col)
1137             rhs = '%s.%s' % (qn(alias), qn2(columns[index]))
1138             self.query.where.add(
1139                 QueryWrapper('%s = %s' % (lhs_sql, rhs), lhs_params), 'AND')
1140 
1141         sql, params = self.as_sql()
1142         return 'EXISTS (%s)' % sql, params
1143 
1144     def explain_query(self):
1145         result = list(self.execute_sql())
1146         # Some backends return 1 item tuples with strings, and others return
1147         # tuples with integers and strings. Flatten them out into strings.
1148         for row in result[0]:
1149             if not isinstance(row, str):
1150                 yield ' '.join(str(c) for c in row)
1151             else:
1152                 yield row

</code><intended_behavior>While not directly modifying this class for the boolean validation, ensuring that the `SQLCompiler` has access to the validation logic or error raised in `QuerySet`'s methods might help with broader error handling and messaging. The actual change would be in the queryset filtering logic, but attention should be given here to ensure that errors are gracefully handled and correctly propagate up to developers when incorrect `__isnull` usage is attempted. The intended behavior is not so much an alteration of `SQLCompiler` but an acknowledgment that changes in `QuerySet` logic may affect how SQL compilation errors are caught and reported.</intended_behavior>]
2025-10-18 11:14:13.778 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 11:14:13.778 | INFO     | Search-only: skipping patch selection
2025-10-18 11:14:13.928 | INFO     | Task django__django-11905 completed successfully.
2025-10-18 11:14:13.929 | INFO     | No patch generated. You can try running ACR again.
