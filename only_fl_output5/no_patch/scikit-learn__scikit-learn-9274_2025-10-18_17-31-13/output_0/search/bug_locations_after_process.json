[
    {
        "rel_file_path": "sklearn/neural_network/multilayer_perceptron.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/neural_network/multilayer_perceptron.py",
        "start": 310,
        "end": 376,
        "class_name": null,
        "method_name": "_fit",
        "code": "310     def _fit(self, X, y, incremental=False):\n311         # Make sure self.hidden_layer_sizes is a list\n312         hidden_layer_sizes = self.hidden_layer_sizes\n313         if not hasattr(hidden_layer_sizes, \"__iter__\"):\n314             hidden_layer_sizes = [hidden_layer_sizes]\n315         hidden_layer_sizes = list(hidden_layer_sizes)\n316 \n317         # Validate input parameters.\n318         self._validate_hyperparameters()\n319         if np.any(np.array(hidden_layer_sizes) <= 0):\n320             raise ValueError(\"hidden_layer_sizes must be > 0, got %s.\" %\n321                              hidden_layer_sizes)\n322 \n323         X, y = self._validate_input(X, y, incremental)\n324         n_samples, n_features = X.shape\n325 \n326         # Ensure y is 2D\n327         if y.ndim == 1:\n328             y = y.reshape((-1, 1))\n329 \n330         self.n_outputs_ = y.shape[1]\n331 \n332         layer_units = ([n_features] + hidden_layer_sizes +\n333                        [self.n_outputs_])\n334 \n335         # check random state\n336         self._random_state = check_random_state(self.random_state)\n337 \n338         if not hasattr(self, 'coefs_') or (not self.warm_start and not\n339                                            incremental):\n340             # First time training the model\n341             self._initialize(y, layer_units)\n342 \n343         # lbfgs does not support mini-batches\n344         if self.solver == 'lbfgs':\n345             batch_size = n_samples\n346         elif self.batch_size == 'auto':\n347             batch_size = min(200, n_samples)\n348         else:\n349             if self.batch_size < 1 or self.batch_size > n_samples:\n350                 warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n351                               \"sample size. It is going to be clipped\")\n352             batch_size = np.clip(self.batch_size, 1, n_samples)\n353 \n354         # Initialize lists\n355         activations = [X]\n356         activations.extend(np.empty((batch_size, n_fan_out))\n357                            for n_fan_out in layer_units[1:])\n358         deltas = [np.empty_like(a_layer) for a_layer in activations]\n359 \n360         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n361                       n_fan_out_ in zip(layer_units[:-1],\n362                                         layer_units[1:])]\n363 \n364         intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in\n365                            layer_units[1:]]\n366 \n367         # Run the Stochastic optimization solver\n368         if self.solver in _STOCHASTIC_SOLVERS:\n369             self._fit_stochastic(X, y, activations, deltas, coef_grads,\n370                                  intercept_grads, layer_units, incremental)\n371 \n372         # Run the LBFGS solver\n373         elif self.solver == 'lbfgs':\n374             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n375                             intercept_grads, layer_units)\n376         return self\n",
        "intended_behavior": "The `_fit` method should ensure that the `max_iter` parameter is correctly passed and used to set the maximum number of iterations for the `l-bfgs` solver. This requires adjusting parameters processing when the solver is `l-bfgs`, making `max_iter` directly influence the optimizer's iteration process."
    },
    {
        "rel_file_path": "sklearn/neural_network/multilayer_perceptron.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/scikit-learn__scikit-learn/setup_scikit-learn__scikit-learn__0.22/sklearn/neural_network/multilayer_perceptron.py",
        "start": 310,
        "end": 376,
        "class_name": "BaseMultilayerPerceptron",
        "method_name": "_fit",
        "code": "310     def _fit(self, X, y, incremental=False):\n311         # Make sure self.hidden_layer_sizes is a list\n312         hidden_layer_sizes = self.hidden_layer_sizes\n313         if not hasattr(hidden_layer_sizes, \"__iter__\"):\n314             hidden_layer_sizes = [hidden_layer_sizes]\n315         hidden_layer_sizes = list(hidden_layer_sizes)\n316 \n317         # Validate input parameters.\n318         self._validate_hyperparameters()\n319         if np.any(np.array(hidden_layer_sizes) <= 0):\n320             raise ValueError(\"hidden_layer_sizes must be > 0, got %s.\" %\n321                              hidden_layer_sizes)\n322 \n323         X, y = self._validate_input(X, y, incremental)\n324         n_samples, n_features = X.shape\n325 \n326         # Ensure y is 2D\n327         if y.ndim == 1:\n328             y = y.reshape((-1, 1))\n329 \n330         self.n_outputs_ = y.shape[1]\n331 \n332         layer_units = ([n_features] + hidden_layer_sizes +\n333                        [self.n_outputs_])\n334 \n335         # check random state\n336         self._random_state = check_random_state(self.random_state)\n337 \n338         if not hasattr(self, 'coefs_') or (not self.warm_start and not\n339                                            incremental):\n340             # First time training the model\n341             self._initialize(y, layer_units)\n342 \n343         # lbfgs does not support mini-batches\n344         if self.solver == 'lbfgs':\n345             batch_size = n_samples\n346         elif self.batch_size == 'auto':\n347             batch_size = min(200, n_samples)\n348         else:\n349             if self.batch_size < 1 or self.batch_size > n_samples:\n350                 warnings.warn(\"Got `batch_size` less than 1 or larger than \"\n351                               \"sample size. It is going to be clipped\")\n352             batch_size = np.clip(self.batch_size, 1, n_samples)\n353 \n354         # Initialize lists\n355         activations = [X]\n356         activations.extend(np.empty((batch_size, n_fan_out))\n357                            for n_fan_out in layer_units[1:])\n358         deltas = [np.empty_like(a_layer) for a_layer in activations]\n359 \n360         coef_grads = [np.empty((n_fan_in_, n_fan_out_)) for n_fan_in_,\n361                       n_fan_out_ in zip(layer_units[:-1],\n362                                         layer_units[1:])]\n363 \n364         intercept_grads = [np.empty(n_fan_out_) for n_fan_out_ in\n365                            layer_units[1:]]\n366 \n367         # Run the Stochastic optimization solver\n368         if self.solver in _STOCHASTIC_SOLVERS:\n369             self._fit_stochastic(X, y, activations, deltas, coef_grads,\n370                                  intercept_grads, layer_units, incremental)\n371 \n372         # Run the LBFGS solver\n373         elif self.solver == 'lbfgs':\n374             self._fit_lbfgs(X, y, activations, deltas, coef_grads,\n375                             intercept_grads, layer_units)\n376         return self\n",
        "intended_behavior": "The `_fit` method should ensure that the `max_iter` parameter is correctly passed and used to set the maximum number of iterations for the `l-bfgs` solver. This requires adjusting parameters processing when the solver is `l-bfgs`, making `max_iter` directly influence the optimizer's iteration process."
    }
]