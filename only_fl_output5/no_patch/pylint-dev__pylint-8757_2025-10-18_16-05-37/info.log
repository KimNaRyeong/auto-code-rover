2025-10-18 16:05:37.232 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 16:05:38.817 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=718, output_tokens=12, cost=0.007540
2025-10-18 16:06:13.998 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=898, output_tokens=818, cost=0.033520
2025-10-18 16:06:15.044 | INFO     | registered non reproducing test 0
2025-10-18 16:06:52.940 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2397, output_tokens=839, cost=0.049140
2025-10-18 16:06:53.930 | INFO     | registered non reproducing test 1
2025-10-18 16:07:15.453 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2193, output_tokens=736, cost=0.044010
2025-10-18 16:07:16.426 | INFO     | registered non reproducing test 2
2025-10-18 16:07:16.426 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 16:07:16.432 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:07:32.543 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1027, output_tokens=345, cost=0.020620
2025-10-18 16:07:32.553 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:07:38.146 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=699, output_tokens=78, cost=0.009330
2025-10-18 16:07:38.148 | DEBUG    | Extracted a valid json.
2025-10-18 16:07:38.148 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"--jobs\")",
        "search_class(\"Parallel\")",
        "search_method(\"execute_parallel\")",
        "search_method(\"collect_messages\")",
        "search_method(\"aggregate_results\")",
        "search_code(\"print_messages\")",
        "search_code(\"sort_messages\")"
    ],
    "bug_locations": []
}
2025-10-18 16:07:38.219 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 16:07:38.220 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 16:07:57.077 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1620, output_tokens=374, cost=0.027420
2025-10-18 16:07:57.088 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:08:00.841 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2210, output_tokens=54, cost=0.023720
2025-10-18 16:08:00.846 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:08:06.207 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=408, output_tokens=80, cost=0.006480
2025-10-18 16:08:06.209 | DEBUG    | Extracted a valid json.
2025-10-18 16:08:06.209 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"parallel\")",
        "search_code(\"process_file\")",
        "search_code(\"run_analysis_for_file\")",
        "search_code(\"collect_results\")",
        "search_code(\"print_results\")",
        "search_code(\"multiprocessing\")",
        "search_code(\"concurrent\")"
    ],
    "bug_locations": []
}
2025-10-18 16:08:06.353 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 16:08:06.354 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 16:08:32.116 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2831, output_tokens=626, cost=0.047090
2025-10-18 16:08:32.123 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:08:41.585 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=3673, output_tokens=383, cost=0.048220
2025-10-18 16:08:41.593 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:08:55.723 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=737, output_tokens=287, cost=0.015980
2025-10-18 16:08:55.724 | DEBUG    | Extracted a valid json.
2025-10-18 16:08:55.724 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations": [
        {
            "file": "pylint/lint/parallel.py",
            "class": "",
            "method": "",
            "intended_behavior": "This file should contain the logic that handles the distribution of lint tasks across multiple processes. The intended behavior here would be to ensure that tasks are distributed and executed in a way that results collected from each process are complete and consistent, regardless of the number of jobs specified. An improvement or bug fix in this area might involve ensuring that each process receives a full and correct context for its linting tasks, and that there's a robust collection and aggregation process for the results from each job."
        },
        {
            "file": "pylint/lint/run.py",
            "class": "",
            "method": "",
            "intended_behavior": "In this file, particularly in methods that manage the execution of parallel jobs, there should be clear handling of how results from different jobs are aggregated. The intended behavior would involve ensuring that results from all jobs are accounted for without loss or duplication and are aggregated in a manner that does not depend on the order of completion of the jobs. This may require reviewing and potentially adjusting the logic that initializes, monitors, and concludes the parallel processes, specifically ensuring that thereâ€™s an accurate and consistent method of collating results from multiple processes into the final output."
        }
    ]
}
2025-10-18 16:08:55.729 | DEBUG    | Bug location extracted successfully: [<file>pylint/lint/parallel.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import functools
8 from collections import defaultdict
9 from collections.abc import Iterable, Sequence
10 from typing import TYPE_CHECKING, Any
11 
12 import dill
13 
14 from pylint import reporters
15 from pylint.lint.utils import _augment_sys_path
16 from pylint.message import Message
17 from pylint.typing import FileItem
18 from pylint.utils import LinterStats, merge_stats
19 
20 try:
21     import multiprocessing
22 except ImportError:
23     multiprocessing = None  # type: ignore[assignment]
24 
25 try:
26     from concurrent.futures import ProcessPoolExecutor
27 except ImportError:
28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
29 
30 if TYPE_CHECKING:
31     from pylint.lint import PyLinter
32 
33 # PyLinter object used by worker processes when checking files using parallel mode
34 # should only be used by the worker processes
35 _worker_linter: PyLinter | None = None
36 
37 
38 def _worker_initialize(
39     linter: bytes, extra_packages_paths: Sequence[str] | None = None
40 ) -> None:
41     """Function called to initialize a worker for a Process within a concurrent Pool.
42 
43     :param linter: A linter-class (PyLinter) instance pickled with dill
44     :param extra_packages_paths: Extra entries to be added to `sys.path`
45     """
46     global _worker_linter  # pylint: disable=global-statement
47     _worker_linter = dill.loads(linter)
48     assert _worker_linter
49 
50     # On the worker process side the messages are just collected and passed back to
51     # parent process as _worker_check_file function's return value
52     _worker_linter.set_reporter(reporters.CollectingReporter())
53     _worker_linter.open()
54 
55     # Re-register dynamic plugins, since the pool does not have access to the
56     # astroid module that existed when the linter was pickled.
57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)
58     _worker_linter.load_plugin_configuration()
59 
60     if extra_packages_paths:
61         _augment_sys_path(extra_packages_paths)
62 
63 
64 def _worker_check_single_file(
65     file_item: FileItem,
66 ) -> tuple[
67     int,
68     str,
69     str,
70     str,
71     list[Message],
72     LinterStats,
73     int,
74     defaultdict[str, list[Any]],
75 ]:
76     if not _worker_linter:
77         raise RuntimeError("Worker linter not yet initialised")
78     _worker_linter.open()
79     _worker_linter.check_single_file_item(file_item)
80     mapreduce_data = defaultdict(list)
81     for checker in _worker_linter.get_checkers():
82         data = checker.get_map_data()
83         if data is not None:
84             mapreduce_data[checker.name].append(data)
85     msgs = _worker_linter.reporter.messages
86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)
87     _worker_linter.reporter.reset()
88     return (
89         id(multiprocessing.current_process()),
90         _worker_linter.current_name,
91         file_item.filepath,
92         _worker_linter.file_state.base_name,
93         msgs,
94         _worker_linter.stats,
95         _worker_linter.msg_status,
96         mapreduce_data,
97     )
98 
99 
100 def _merge_mapreduce_data(
101     linter: PyLinter,
102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],
103 ) -> None:
104     """Merges map/reduce data across workers, invoking relevant APIs on checkers."""
105     # First collate the data and prepare it, so we can send it to the checkers for
106     # validation. The intent here is to collect all the mapreduce data for all checker-
107     # runs across processes - that will then be passed to a static method on the
108     # checkers to be reduced and further processed.
109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)
110     for linter_data in all_mapreduce_data.values():
111         for run_data in linter_data:
112             for checker_name, data in run_data.items():
113                 collated_map_reduce_data[checker_name].extend(data)
114 
115     # Send the data to checkers that support/require consolidated data
116     original_checkers = linter.get_checkers()
117     for checker in original_checkers:
118         if checker.name in collated_map_reduce_data:
119             # Assume that if the check has returned map/reduce data that it has the
120             # reducer function
121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])
122 
123 
124 def check_parallel(
125     linter: PyLinter,
126     jobs: int,
127     files: Iterable[FileItem],
128     extra_packages_paths: Sequence[str] | None = None,
129 ) -> None:
130     """Use the given linter to lint the files with given amount of workers (jobs).
131 
132     This splits the work filestream-by-filestream. If you need to do work across
133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.
134     """
135     # The linter is inherited by all the pool's workers, i.e. the linter
136     # is identical to the linter object here. This is required so that
137     # a custom PyLinter object can be used.
138     initializer = functools.partial(
139         _worker_initialize, extra_packages_paths=extra_packages_paths
140     )
141     with ProcessPoolExecutor(
142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)
143     ) as executor:
144         linter.open()
145         all_stats = []
146         all_mapreduce_data: defaultdict[
147             int, list[defaultdict[str, list[Any]]]
148         ] = defaultdict(list)
149 
150         # Maps each file to be worked on by a single _worker_check_single_file() call,
151         # collecting any map/reduce data by checker module so that we can 'reduce' it
152         # later.
153         for (
154             worker_idx,  # used to merge map/reduce data across workers
155             module,
156             file_path,
157             base_name,
158             messages,
159             stats,
160             msg_status,
161             mapreduce_data,
162         ) in executor.map(_worker_check_single_file, files):
163             linter.file_state.base_name = base_name
164             linter.file_state._is_base_filestate = False
165             linter.set_current_module(module, file_path)
166             for msg in messages:
167                 linter.reporter.handle_message(msg)
168             all_stats.append(stats)
169             all_mapreduce_data[worker_idx].append(mapreduce_data)
170             linter.msg_status |= msg_status
171 
172     _merge_mapreduce_data(linter, all_mapreduce_data)
173     linter.stats = merge_stats([linter.stats, *all_stats])

</code><intended_behavior>This file should contain the logic that handles the distribution of lint tasks across multiple processes. The intended behavior here would be to ensure that tasks are distributed and executed in a way that results collected from each process are complete and consistent, regardless of the number of jobs specified. An improvement or bug fix in this area might involve ensuring that each process receives a full and correct context for its linting tasks, and that there's a robust collection and aggregation process for the results from each job.</intended_behavior>, <file>pylint/lint/run.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import os
8 import sys
9 import warnings
10 from collections.abc import Sequence
11 from pathlib import Path
12 from typing import ClassVar
13 
14 from pylint import config
15 from pylint.checkers.utils import clear_lru_caches
16 from pylint.config._pylint_config import (
17     _handle_pylint_config_commands,
18     _register_generate_config_options,
19 )
20 from pylint.config.config_initialization import _config_initialization
21 from pylint.config.exceptions import ArgumentPreprocessingError
22 from pylint.config.utils import _preprocess_options
23 from pylint.constants import full_version
24 from pylint.lint.base_options import _make_run_options
25 from pylint.lint.pylinter import MANAGER, PyLinter
26 from pylint.reporters.base_reporter import BaseReporter
27 
28 try:
29     import multiprocessing
30     from multiprocessing import synchronize  # noqa pylint: disable=unused-import
31 except ImportError:
32     multiprocessing = None  # type: ignore[assignment]
33 
34 try:
35     from concurrent.futures import ProcessPoolExecutor
36 except ImportError:
37     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
38 
39 
40 def _query_cpu() -> int | None:
41     """Try to determine number of CPUs allotted in a docker container.
42 
43     This is based on discussion and copied from suggestions in
44     https://bugs.python.org/issue36054.
45     """
46     cpu_quota, avail_cpu = None, None
47 
48     if Path("/sys/fs/cgroup/cpu/cpu.cfs_quota_us").is_file():
49         with open("/sys/fs/cgroup/cpu/cpu.cfs_quota_us", encoding="utf-8") as file:
50             # Not useful for AWS Batch based jobs as result is -1, but works on local linux systems
51             cpu_quota = int(file.read().rstrip())
52 
53     if (
54         cpu_quota
55         and cpu_quota != -1
56         and Path("/sys/fs/cgroup/cpu/cpu.cfs_period_us").is_file()
57     ):
58         with open("/sys/fs/cgroup/cpu/cpu.cfs_period_us", encoding="utf-8") as file:
59             cpu_period = int(file.read().rstrip())
60         # Divide quota by period and you should get num of allotted CPU to the container,
61         # rounded down if fractional.
62         avail_cpu = int(cpu_quota / cpu_period)
63     elif Path("/sys/fs/cgroup/cpu/cpu.shares").is_file():
64         with open("/sys/fs/cgroup/cpu/cpu.shares", encoding="utf-8") as file:
65             cpu_shares = int(file.read().rstrip())
66         # For AWS, gives correct value * 1024.
67         avail_cpu = int(cpu_shares / 1024)
68 
69     # In K8s Pods also a fraction of a single core could be available
70     # As multiprocessing is not able to run only a "fraction" of process
71     # assume we have 1 CPU available
72     if avail_cpu == 0:
73         avail_cpu = 1
74 
75     return avail_cpu
76 
77 
78 def _cpu_count() -> int:
79     """Use sched_affinity if available for virtualized or containerized
80     environments.
81     """
82     cpu_share = _query_cpu()
83     cpu_count = None
84     sched_getaffinity = getattr(os, "sched_getaffinity", None)
85     # pylint: disable=not-callable,using-constant-test,useless-suppression
86     if sched_getaffinity:
87         cpu_count = len(sched_getaffinity(0))
88     elif multiprocessing:
89         cpu_count = multiprocessing.cpu_count()
90     else:
91         cpu_count = 1
92     if sys.platform == "win32":
93         # See also https://github.com/python/cpython/issues/94242
94         cpu_count = min(cpu_count, 56)  # pragma: no cover
95     if cpu_share is not None:
96         return min(cpu_share, cpu_count)
97     return cpu_count
98 
99 
100 class Run:
101     """Helper class to use as main for pylint with 'run(*sys.argv[1:])'."""
102 
103     LinterClass = PyLinter
104     option_groups = (
105         (
106             "Commands",
107             "Options which are actually commands. Options in this \
108 group are mutually exclusive.",
109         ),
110     )
111     _is_pylint_config: ClassVar[bool] = False
112     """Boolean whether or not this is a 'pylint-config' run.
113 
114     Used by _PylintConfigRun to make the 'pylint-config' command work.
115     """
116 
117     # pylint: disable = too-many-statements, too-many-branches
118     def __init__(
119         self,
120         args: Sequence[str],
121         reporter: BaseReporter | None = None,
122         exit: bool = True,  # pylint: disable=redefined-builtin
123     ) -> None:
124         # Immediately exit if user asks for version
125         if "--version" in args:
126             print(full_version)
127             sys.exit(0)
128 
129         self._rcfile: str | None = None
130         self._output: str | None = None
131         self._plugins: list[str] = []
132         self.verbose: bool = False
133 
134         # Pre-process certain options and remove them from args list
135         try:
136             args = _preprocess_options(self, args)
137         except ArgumentPreprocessingError as ex:
138             print(ex, file=sys.stderr)
139             sys.exit(32)
140 
141         # Determine configuration file
142         if self._rcfile is None:
143             default_file = next(config.find_default_config_files(), None)
144             if default_file:
145                 self._rcfile = str(default_file)
146 
147         self.linter = linter = self.LinterClass(
148             _make_run_options(self),
149             option_groups=self.option_groups,
150             pylintrc=self._rcfile,
151         )
152         # register standard checkers
153         linter.load_default_plugins()
154         # load command line plugins
155         linter.load_plugin_modules(self._plugins)
156 
157         # Register the options needed for 'pylint-config'
158         # By not registering them by default they don't show up in the normal usage message
159         if self._is_pylint_config:
160             _register_generate_config_options(linter._arg_parser)
161 
162         args = _config_initialization(
163             linter, args, reporter, config_file=self._rcfile, verbose_mode=self.verbose
164         )
165 
166         # Handle the 'pylint-config' command
167         if self._is_pylint_config:
168             warnings.warn(
169                 "NOTE: The 'pylint-config' command is experimental and usage can change",
170                 UserWarning,
171                 stacklevel=2,
172             )
173             code = _handle_pylint_config_commands(linter)
174             if exit:
175                 sys.exit(code)
176             return
177 
178         # Display help messages if there are no files to lint
179         if not args:
180             print(linter.help())
181             sys.exit(32)
182 
183         if linter.config.jobs < 0:
184             print(
185                 f"Jobs number ({linter.config.jobs}) should be greater than or equal to 0",
186                 file=sys.stderr,
187             )
188             sys.exit(32)
189         if linter.config.jobs > 1 or linter.config.jobs == 0:
190             if ProcessPoolExecutor is None:
191                 print(
192                     "concurrent.futures module is missing, fallback to single process",
193                     file=sys.stderr,
194                 )
195                 linter.set_option("jobs", 1)
196             elif linter.config.jobs == 0:
197                 linter.config.jobs = _cpu_count()
198 
199         if self._output:
200             try:
201                 with open(self._output, "w", encoding="utf-8") as output:
202                     linter.reporter.out = output
203                     linter.check(args)
204                     score_value = linter.generate_reports()
205             except OSError as ex:
206                 print(ex, file=sys.stderr)
207                 sys.exit(32)
208         else:
209             linter.check(args)
210             score_value = linter.generate_reports()
211         if linter.config.clear_cache_post_run:
212             clear_lru_caches()
213             MANAGER.clear_cache()
214 
215         if exit:
216             if linter.config.exit_zero:
217                 sys.exit(0)
218             elif linter.any_fail_on_issues():
219                 # We need to make sure we return a failing exit code in this case.
220                 # So we use self.linter.msg_status if that is non-zero, otherwise we just return 1.
221                 sys.exit(self.linter.msg_status or 1)
222             elif score_value is not None:
223                 if score_value >= linter.config.fail_under:
224                     sys.exit(0)
225                 else:
226                     # We need to make sure we return a failing exit code in this case.
227                     # So we use self.linter.msg_status if that is non-zero, otherwise we just return 1.
228                     sys.exit(self.linter.msg_status or 1)
229             else:
230                 sys.exit(self.linter.msg_status)
231 
232 
233 class _PylintConfigRun(Run):
234     """A private wrapper for the 'pylint-config' command."""
235 
236     _is_pylint_config: ClassVar[bool] = True
237     """Boolean whether or not this is a 'pylint-config' run.
238 
239     Used by _PylintConfigRun to make the 'pylint-config' command work.
240     """

</code><intended_behavior>In this file, particularly in methods that manage the execution of parallel jobs, there should be clear handling of how results from different jobs are aggregated. The intended behavior would involve ensuring that results from all jobs are accounted for without loss or duplication and are aggregated in a manner that does not depend on the order of completion of the jobs. This may require reviewing and potentially adjusting the logic that initializes, monitors, and concludes the parallel processes, specifically ensuring that thereâ€™s an accurate and consistent method of collating results from multiple processes into the final output.</intended_behavior>]
2025-10-18 16:08:55.730 | INFO     | Search completed. Bug locations: [<file>pylint/lint/parallel.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import functools
8 from collections import defaultdict
9 from collections.abc import Iterable, Sequence
10 from typing import TYPE_CHECKING, Any
11 
12 import dill
13 
14 from pylint import reporters
15 from pylint.lint.utils import _augment_sys_path
16 from pylint.message import Message
17 from pylint.typing import FileItem
18 from pylint.utils import LinterStats, merge_stats
19 
20 try:
21     import multiprocessing
22 except ImportError:
23     multiprocessing = None  # type: ignore[assignment]
24 
25 try:
26     from concurrent.futures import ProcessPoolExecutor
27 except ImportError:
28     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
29 
30 if TYPE_CHECKING:
31     from pylint.lint import PyLinter
32 
33 # PyLinter object used by worker processes when checking files using parallel mode
34 # should only be used by the worker processes
35 _worker_linter: PyLinter | None = None
36 
37 
38 def _worker_initialize(
39     linter: bytes, extra_packages_paths: Sequence[str] | None = None
40 ) -> None:
41     """Function called to initialize a worker for a Process within a concurrent Pool.
42 
43     :param linter: A linter-class (PyLinter) instance pickled with dill
44     :param extra_packages_paths: Extra entries to be added to `sys.path`
45     """
46     global _worker_linter  # pylint: disable=global-statement
47     _worker_linter = dill.loads(linter)
48     assert _worker_linter
49 
50     # On the worker process side the messages are just collected and passed back to
51     # parent process as _worker_check_file function's return value
52     _worker_linter.set_reporter(reporters.CollectingReporter())
53     _worker_linter.open()
54 
55     # Re-register dynamic plugins, since the pool does not have access to the
56     # astroid module that existed when the linter was pickled.
57     _worker_linter.load_plugin_modules(_worker_linter._dynamic_plugins, force=True)
58     _worker_linter.load_plugin_configuration()
59 
60     if extra_packages_paths:
61         _augment_sys_path(extra_packages_paths)
62 
63 
64 def _worker_check_single_file(
65     file_item: FileItem,
66 ) -> tuple[
67     int,
68     str,
69     str,
70     str,
71     list[Message],
72     LinterStats,
73     int,
74     defaultdict[str, list[Any]],
75 ]:
76     if not _worker_linter:
77         raise RuntimeError("Worker linter not yet initialised")
78     _worker_linter.open()
79     _worker_linter.check_single_file_item(file_item)
80     mapreduce_data = defaultdict(list)
81     for checker in _worker_linter.get_checkers():
82         data = checker.get_map_data()
83         if data is not None:
84             mapreduce_data[checker.name].append(data)
85     msgs = _worker_linter.reporter.messages
86     assert isinstance(_worker_linter.reporter, reporters.CollectingReporter)
87     _worker_linter.reporter.reset()
88     return (
89         id(multiprocessing.current_process()),
90         _worker_linter.current_name,
91         file_item.filepath,
92         _worker_linter.file_state.base_name,
93         msgs,
94         _worker_linter.stats,
95         _worker_linter.msg_status,
96         mapreduce_data,
97     )
98 
99 
100 def _merge_mapreduce_data(
101     linter: PyLinter,
102     all_mapreduce_data: defaultdict[int, list[defaultdict[str, list[Any]]]],
103 ) -> None:
104     """Merges map/reduce data across workers, invoking relevant APIs on checkers."""
105     # First collate the data and prepare it, so we can send it to the checkers for
106     # validation. The intent here is to collect all the mapreduce data for all checker-
107     # runs across processes - that will then be passed to a static method on the
108     # checkers to be reduced and further processed.
109     collated_map_reduce_data: defaultdict[str, list[Any]] = defaultdict(list)
110     for linter_data in all_mapreduce_data.values():
111         for run_data in linter_data:
112             for checker_name, data in run_data.items():
113                 collated_map_reduce_data[checker_name].extend(data)
114 
115     # Send the data to checkers that support/require consolidated data
116     original_checkers = linter.get_checkers()
117     for checker in original_checkers:
118         if checker.name in collated_map_reduce_data:
119             # Assume that if the check has returned map/reduce data that it has the
120             # reducer function
121             checker.reduce_map_data(linter, collated_map_reduce_data[checker.name])
122 
123 
124 def check_parallel(
125     linter: PyLinter,
126     jobs: int,
127     files: Iterable[FileItem],
128     extra_packages_paths: Sequence[str] | None = None,
129 ) -> None:
130     """Use the given linter to lint the files with given amount of workers (jobs).
131 
132     This splits the work filestream-by-filestream. If you need to do work across
133     multiple files, as in the similarity-checker, then implement the map/reduce mixin functionality.
134     """
135     # The linter is inherited by all the pool's workers, i.e. the linter
136     # is identical to the linter object here. This is required so that
137     # a custom PyLinter object can be used.
138     initializer = functools.partial(
139         _worker_initialize, extra_packages_paths=extra_packages_paths
140     )
141     with ProcessPoolExecutor(
142         max_workers=jobs, initializer=initializer, initargs=(dill.dumps(linter),)
143     ) as executor:
144         linter.open()
145         all_stats = []
146         all_mapreduce_data: defaultdict[
147             int, list[defaultdict[str, list[Any]]]
148         ] = defaultdict(list)
149 
150         # Maps each file to be worked on by a single _worker_check_single_file() call,
151         # collecting any map/reduce data by checker module so that we can 'reduce' it
152         # later.
153         for (
154             worker_idx,  # used to merge map/reduce data across workers
155             module,
156             file_path,
157             base_name,
158             messages,
159             stats,
160             msg_status,
161             mapreduce_data,
162         ) in executor.map(_worker_check_single_file, files):
163             linter.file_state.base_name = base_name
164             linter.file_state._is_base_filestate = False
165             linter.set_current_module(module, file_path)
166             for msg in messages:
167                 linter.reporter.handle_message(msg)
168             all_stats.append(stats)
169             all_mapreduce_data[worker_idx].append(mapreduce_data)
170             linter.msg_status |= msg_status
171 
172     _merge_mapreduce_data(linter, all_mapreduce_data)
173     linter.stats = merge_stats([linter.stats, *all_stats])

</code><intended_behavior>This file should contain the logic that handles the distribution of lint tasks across multiple processes. The intended behavior here would be to ensure that tasks are distributed and executed in a way that results collected from each process are complete and consistent, regardless of the number of jobs specified. An improvement or bug fix in this area might involve ensuring that each process receives a full and correct context for its linting tasks, and that there's a robust collection and aggregation process for the results from each job.</intended_behavior>, <file>pylint/lint/run.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/pylint-dev/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/pylint-dev/pylint/blob/main/CONTRIBUTORS.txt
4 
5 from __future__ import annotations
6 
7 import os
8 import sys
9 import warnings
10 from collections.abc import Sequence
11 from pathlib import Path
12 from typing import ClassVar
13 
14 from pylint import config
15 from pylint.checkers.utils import clear_lru_caches
16 from pylint.config._pylint_config import (
17     _handle_pylint_config_commands,
18     _register_generate_config_options,
19 )
20 from pylint.config.config_initialization import _config_initialization
21 from pylint.config.exceptions import ArgumentPreprocessingError
22 from pylint.config.utils import _preprocess_options
23 from pylint.constants import full_version
24 from pylint.lint.base_options import _make_run_options
25 from pylint.lint.pylinter import MANAGER, PyLinter
26 from pylint.reporters.base_reporter import BaseReporter
27 
28 try:
29     import multiprocessing
30     from multiprocessing import synchronize  # noqa pylint: disable=unused-import
31 except ImportError:
32     multiprocessing = None  # type: ignore[assignment]
33 
34 try:
35     from concurrent.futures import ProcessPoolExecutor
36 except ImportError:
37     ProcessPoolExecutor = None  # type: ignore[assignment,misc]
38 
39 
40 def _query_cpu() -> int | None:
41     """Try to determine number of CPUs allotted in a docker container.
42 
43     This is based on discussion and copied from suggestions in
44     https://bugs.python.org/issue36054.
45     """
46     cpu_quota, avail_cpu = None, None
47 
48     if Path("/sys/fs/cgroup/cpu/cpu.cfs_quota_us").is_file():
49         with open("/sys/fs/cgroup/cpu/cpu.cfs_quota_us", encoding="utf-8") as file:
50             # Not useful for AWS Batch based jobs as result is -1, but works on local linux systems
51             cpu_quota = int(file.read().rstrip())
52 
53     if (
54         cpu_quota
55         and cpu_quota != -1
56         and Path("/sys/fs/cgroup/cpu/cpu.cfs_period_us").is_file()
57     ):
58         with open("/sys/fs/cgroup/cpu/cpu.cfs_period_us", encoding="utf-8") as file:
59             cpu_period = int(file.read().rstrip())
60         # Divide quota by period and you should get num of allotted CPU to the container,
61         # rounded down if fractional.
62         avail_cpu = int(cpu_quota / cpu_period)
63     elif Path("/sys/fs/cgroup/cpu/cpu.shares").is_file():
64         with open("/sys/fs/cgroup/cpu/cpu.shares", encoding="utf-8") as file:
65             cpu_shares = int(file.read().rstrip())
66         # For AWS, gives correct value * 1024.
67         avail_cpu = int(cpu_shares / 1024)
68 
69     # In K8s Pods also a fraction of a single core could be available
70     # As multiprocessing is not able to run only a "fraction" of process
71     # assume we have 1 CPU available
72     if avail_cpu == 0:
73         avail_cpu = 1
74 
75     return avail_cpu
76 
77 
78 def _cpu_count() -> int:
79     """Use sched_affinity if available for virtualized or containerized
80     environments.
81     """
82     cpu_share = _query_cpu()
83     cpu_count = None
84     sched_getaffinity = getattr(os, "sched_getaffinity", None)
85     # pylint: disable=not-callable,using-constant-test,useless-suppression
86     if sched_getaffinity:
87         cpu_count = len(sched_getaffinity(0))
88     elif multiprocessing:
89         cpu_count = multiprocessing.cpu_count()
90     else:
91         cpu_count = 1
92     if sys.platform == "win32":
93         # See also https://github.com/python/cpython/issues/94242
94         cpu_count = min(cpu_count, 56)  # pragma: no cover
95     if cpu_share is not None:
96         return min(cpu_share, cpu_count)
97     return cpu_count
98 
99 
100 class Run:
101     """Helper class to use as main for pylint with 'run(*sys.argv[1:])'."""
102 
103     LinterClass = PyLinter
104     option_groups = (
105         (
106             "Commands",
107             "Options which are actually commands. Options in this \
108 group are mutually exclusive.",
109         ),
110     )
111     _is_pylint_config: ClassVar[bool] = False
112     """Boolean whether or not this is a 'pylint-config' run.
113 
114     Used by _PylintConfigRun to make the 'pylint-config' command work.
115     """
116 
117     # pylint: disable = too-many-statements, too-many-branches
118     def __init__(
119         self,
120         args: Sequence[str],
121         reporter: BaseReporter | None = None,
122         exit: bool = True,  # pylint: disable=redefined-builtin
123     ) -> None:
124         # Immediately exit if user asks for version
125         if "--version" in args:
126             print(full_version)
127             sys.exit(0)
128 
129         self._rcfile: str | None = None
130         self._output: str | None = None
131         self._plugins: list[str] = []
132         self.verbose: bool = False
133 
134         # Pre-process certain options and remove them from args list
135         try:
136             args = _preprocess_options(self, args)
137         except ArgumentPreprocessingError as ex:
138             print(ex, file=sys.stderr)
139             sys.exit(32)
140 
141         # Determine configuration file
142         if self._rcfile is None:
143             default_file = next(config.find_default_config_files(), None)
144             if default_file:
145                 self._rcfile = str(default_file)
146 
147         self.linter = linter = self.LinterClass(
148             _make_run_options(self),
149             option_groups=self.option_groups,
150             pylintrc=self._rcfile,
151         )
152         # register standard checkers
153         linter.load_default_plugins()
154         # load command line plugins
155         linter.load_plugin_modules(self._plugins)
156 
157         # Register the options needed for 'pylint-config'
158         # By not registering them by default they don't show up in the normal usage message
159         if self._is_pylint_config:
160             _register_generate_config_options(linter._arg_parser)
161 
162         args = _config_initialization(
163             linter, args, reporter, config_file=self._rcfile, verbose_mode=self.verbose
164         )
165 
166         # Handle the 'pylint-config' command
167         if self._is_pylint_config:
168             warnings.warn(
169                 "NOTE: The 'pylint-config' command is experimental and usage can change",
170                 UserWarning,
171                 stacklevel=2,
172             )
173             code = _handle_pylint_config_commands(linter)
174             if exit:
175                 sys.exit(code)
176             return
177 
178         # Display help messages if there are no files to lint
179         if not args:
180             print(linter.help())
181             sys.exit(32)
182 
183         if linter.config.jobs < 0:
184             print(
185                 f"Jobs number ({linter.config.jobs}) should be greater than or equal to 0",
186                 file=sys.stderr,
187             )
188             sys.exit(32)
189         if linter.config.jobs > 1 or linter.config.jobs == 0:
190             if ProcessPoolExecutor is None:
191                 print(
192                     "concurrent.futures module is missing, fallback to single process",
193                     file=sys.stderr,
194                 )
195                 linter.set_option("jobs", 1)
196             elif linter.config.jobs == 0:
197                 linter.config.jobs = _cpu_count()
198 
199         if self._output:
200             try:
201                 with open(self._output, "w", encoding="utf-8") as output:
202                     linter.reporter.out = output
203                     linter.check(args)
204                     score_value = linter.generate_reports()
205             except OSError as ex:
206                 print(ex, file=sys.stderr)
207                 sys.exit(32)
208         else:
209             linter.check(args)
210             score_value = linter.generate_reports()
211         if linter.config.clear_cache_post_run:
212             clear_lru_caches()
213             MANAGER.clear_cache()
214 
215         if exit:
216             if linter.config.exit_zero:
217                 sys.exit(0)
218             elif linter.any_fail_on_issues():
219                 # We need to make sure we return a failing exit code in this case.
220                 # So we use self.linter.msg_status if that is non-zero, otherwise we just return 1.
221                 sys.exit(self.linter.msg_status or 1)
222             elif score_value is not None:
223                 if score_value >= linter.config.fail_under:
224                     sys.exit(0)
225                 else:
226                     # We need to make sure we return a failing exit code in this case.
227                     # So we use self.linter.msg_status if that is non-zero, otherwise we just return 1.
228                     sys.exit(self.linter.msg_status or 1)
229             else:
230                 sys.exit(self.linter.msg_status)
231 
232 
233 class _PylintConfigRun(Run):
234     """A private wrapper for the 'pylint-config' command."""
235 
236     _is_pylint_config: ClassVar[bool] = True
237     """Boolean whether or not this is a 'pylint-config' run.
238 
239     Used by _PylintConfigRun to make the 'pylint-config' command work.
240     """

</code><intended_behavior>In this file, particularly in methods that manage the execution of parallel jobs, there should be clear handling of how results from different jobs are aggregated. The intended behavior would involve ensuring that results from all jobs are accounted for without loss or duplication and are aggregated in a manner that does not depend on the order of completion of the jobs. This may require reviewing and potentially adjusting the logic that initializes, monitors, and concludes the parallel processes, specifically ensuring that thereâ€™s an accurate and consistent method of collating results from multiple processes into the final output.</intended_behavior>]
2025-10-18 16:08:55.730 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 16:08:55.730 | INFO     | Search-only: skipping patch selection
2025-10-18 16:08:55.815 | INFO     | Task pylint-dev__pylint-8757 completed successfully.
2025-10-18 16:08:55.816 | INFO     | No patch generated. You can try running ACR again.
