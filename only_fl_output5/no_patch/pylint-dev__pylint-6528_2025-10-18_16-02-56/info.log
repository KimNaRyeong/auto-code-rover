2025-10-18 16:02:56.497 | INFO     | Starting overall retry 0 with model gpt-4-0125-preview
2025-10-18 16:03:01.483 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=639, output_tokens=12, cost=0.006750
2025-10-18 16:03:33.623 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=819, output_tokens=648, cost=0.027630
2025-10-18 16:03:34.646 | INFO     | registered non reproducing test 0
2025-10-18 16:04:04.206 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2103, output_tokens=826, cost=0.045810
2025-10-18 16:04:33.437 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2103, output_tokens=778, cost=0.044370
2025-10-18 16:04:34.427 | INFO     | registered non reproducing test 2
2025-10-18 16:04:34.427 | WARNING  | Failed to write a reproducer test; skipping reproducer tracing
2025-10-18 16:04:34.433 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:04:51.566 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=934, output_tokens=374, cost=0.020560
2025-10-18 16:04:51.574 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:04:58.448 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=728, output_tokens=117, cost=0.010790
2025-10-18 16:04:58.449 | DEBUG    | Extracted a valid json.
2025-10-18 16:04:58.449 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [
        "search_code(\"parse command-line arguments\")",
        "search_code(\"--recursive\")",
        "search_code(\"--ignore\")",
        "search_code(\"--ignore-paths\")",
        "search_code(\"--ignore-patterns\")",
        "search_method(\"apply ignore rules\")",
        "search_method(\"filter files\")",
        "search_method(\"filter paths\")",
        "search_code(\"regex match file path\")",
        "search_code(\"regex match base names\")"
    ],
    "bug_locations": []
}
2025-10-18 16:04:58.501 | DEBUG    | Obtained search results from API invocation. Going into next retrieval round.
2025-10-18 16:04:58.501 | DEBUG    | <Agent search> Analyzing search results.
2025-10-18 16:05:25.030 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=1665, output_tokens=651, cost=0.036180
2025-10-18 16:05:25.037 | DEBUG    | <Agent search> Selecting APIs to call.
2025-10-18 16:05:30.789 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=2532, output_tokens=241, cost=0.032550
2025-10-18 16:05:30.796 | DEBUG    | Trying to convert API calls/bug locations into json. Try 1 of 5.
2025-10-18 16:05:37.003 | INFO     | Model (gpt-4-0125-preview) API request cost info: input_tokens=595, output_tokens=152, cost=0.010510
2025-10-18 16:05:37.004 | DEBUG    | Extracted a valid json.
2025-10-18 16:05:37.004 | DEBUG    | Agent proxy return the following json: {
    "API_calls": [],
    "bug_locations":[{
        "file": "pylint/checkers/similar.py",
        "class": "",
        "method": "",
        "intended_behavior": "The code within this file should properly heed the `--ignore`, `--ignore-paths`, and `--ignore-patterns` flags when performing similarity checks, particularly in recursive mode. This would involve filtering out files and directories that match the patterns specified by these flags before any analysis or similarity checks are conducted. The intended behavior ensures that any file or directory meant to be ignored as per the user's configuration is indeed skipped during linting, preserving the existing functionality of Pylint while fixing the issue of ignored entities still being analyzed."
    }]
}
2025-10-18 16:05:37.010 | DEBUG    | Bug location extracted successfully: [<file>pylint/checkers/similar.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/PyCQA/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/PyCQA/pylint/blob/main/CONTRIBUTORS.txt
4 
5 """A similarities / code duplication command line tool and pylint checker.
6 
7 The algorithm is based on comparing the hash value of n successive lines of a file.
8 First the files are read and any line that doesn't fulfill requirement are removed (comments, docstrings...)
9 Those stripped lines are stored in the LineSet class which gives access to them.
10 Then each index of the stripped lines collection is associated with the hash of n successive entries of the stripped lines starting at the current index
11 (n is the minimum common lines option).
12 The common hashes between both linesets are then looked for. If there are matches, then the match indices in both linesets are stored and associated
13 with the corresponding couples (start line number/end line number) in both files.
14 This association is then post-processed to handle the case of successive matches. For example if the minimum common lines setting is set to four, then
15 the hashes are computed with four lines. If one of match indices couple (12, 34) is the successor of another one (11, 33) then it means that there are
16 in fact five lines which are common.
17 Once post-processed the values of association table are the result looked for, i.e start and end lines numbers of common lines in both files.
18 """
19 
20 from __future__ import annotations
21 
22 import argparse
23 import copy
24 import functools
25 import itertools
26 import operator
27 import re
28 import sys
29 import warnings
30 from collections import defaultdict
31 from collections.abc import Callable, Generator, Iterable
32 from getopt import getopt
33 from io import BufferedIOBase, BufferedReader, BytesIO
34 from itertools import chain, groupby
35 from typing import (
36     TYPE_CHECKING,
37     Any,
38     Dict,
39     List,
40     NamedTuple,
41     NewType,
42     NoReturn,
43     TextIO,
44     Tuple,
45     Union,
46 )
47 
48 import astroid
49 from astroid import nodes
50 
51 from pylint.checkers import BaseChecker, BaseRawFileChecker, table_lines_from_stats
52 from pylint.reporters.ureports.nodes import Table
53 from pylint.typing import MessageDefinitionTuple, Options
54 from pylint.utils import LinterStats, decoding_stream
55 
56 if TYPE_CHECKING:
57     from pylint.lint import PyLinter
58 
59 DEFAULT_MIN_SIMILARITY_LINE = 4
60 
61 REGEX_FOR_LINES_WITH_CONTENT = re.compile(r".*\w+")
62 
63 # Index defines a location in a LineSet stripped lines collection
64 Index = NewType("Index", int)
65 
66 # LineNumber defines a location in a LinesSet real lines collection (the whole file lines)
67 LineNumber = NewType("LineNumber", int)
68 
69 
70 # LineSpecifs holds characteristics of a line in a file
71 class LineSpecifs(NamedTuple):
72     line_number: LineNumber
73     text: str
74 
75 
76 # Links LinesChunk object to the starting indices (in lineset's stripped lines)
77 # of the different chunk of lines that are used to compute the hash
78 HashToIndex_T = Dict["LinesChunk", List[Index]]
79 
80 # Links index in the lineset's stripped lines to the real lines in the file
81 IndexToLines_T = Dict[Index, "SuccessiveLinesLimits"]
82 
83 # The types the streams read by pylint can take. Originating from astroid.nodes.Module.stream() and open()
84 STREAM_TYPES = Union[TextIO, BufferedReader, BytesIO]
85 
86 
87 class CplSuccessiveLinesLimits:
88     """Holds a SuccessiveLinesLimits object for each file compared and a
89     counter on the number of common lines between both stripped lines collections extracted from both files.
90     """
91 
92     __slots__ = ("first_file", "second_file", "effective_cmn_lines_nb")
93 
94     def __init__(
95         self,
96         first_file: SuccessiveLinesLimits,
97         second_file: SuccessiveLinesLimits,
98         effective_cmn_lines_nb: int,
99     ) -> None:
100         self.first_file = first_file
101         self.second_file = second_file
102         self.effective_cmn_lines_nb = effective_cmn_lines_nb
103 
104 
105 # Links the indices to the starting line in both lineset's stripped lines to
106 # the start and end lines in both files
107 CplIndexToCplLines_T = Dict["LineSetStartCouple", CplSuccessiveLinesLimits]
108 
109 
110 class LinesChunk:
111     """The LinesChunk object computes and stores the hash of some consecutive stripped lines of a lineset."""
112 
113     __slots__ = ("_fileid", "_index", "_hash")
114 
115     def __init__(self, fileid: str, num_line: int, *lines: Iterable[str]) -> None:
116         self._fileid: str = fileid
117         """The name of the file from which the LinesChunk object is generated."""
118 
119         self._index: Index = Index(num_line)
120         """The index in the stripped lines that is the starting of consecutive lines."""
121 
122         self._hash: int = sum(hash(lin) for lin in lines)
123         """The hash of some consecutive lines."""
124 
125     def __eq__(self, o: Any) -> bool:
126         if not isinstance(o, LinesChunk):
127             return NotImplemented
128         return self._hash == o._hash
129 
130     def __hash__(self) -> int:
131         return self._hash
132 
133     def __repr__(self) -> str:
134         return (
135             f"<LinesChunk object for file {self._fileid} ({self._index}, {self._hash})>"
136         )
137 
138     def __str__(self) -> str:
139         return (
140             f"LinesChunk object for file {self._fileid}, starting at line {self._index} \n"
141             f"Hash is {self._hash}"
142         )
143 
144 
145 class SuccessiveLinesLimits:
146     """A class to handle the numbering of begin and end of successive lines.
147 
148     :note: Only the end line number can be updated.
149     """
150 
151     __slots__ = ("_start", "_end")
152 
153     def __init__(self, start: LineNumber, end: LineNumber) -> None:
154         self._start: LineNumber = start
155         self._end: LineNumber = end
156 
157     @property
158     def start(self) -> LineNumber:
159         return self._start
160 
161     @property
162     def end(self) -> LineNumber:
163         return self._end
164 
165     @end.setter
166     def end(self, value: LineNumber) -> None:
167         self._end = value
168 
169     def __repr__(self) -> str:
170         return f"<SuccessiveLinesLimits <{self._start};{self._end}>>"
171 
172 
173 class LineSetStartCouple(NamedTuple):
174     """Indices in both linesets that mark the beginning of successive lines."""
175 
176     fst_lineset_index: Index
177     snd_lineset_index: Index
178 
179     def __repr__(self) -> str:
180         return (
181             f"<LineSetStartCouple <{self.fst_lineset_index};{self.snd_lineset_index}>>"
182         )
183 
184     def __eq__(self, other) -> bool:
185         if not isinstance(other, LineSetStartCouple):
186             return NotImplemented
187         return (
188             self.fst_lineset_index == other.fst_lineset_index
189             and self.snd_lineset_index == other.snd_lineset_index
190         )
191 
192     def __hash__(self) -> int:
193         return hash(self.fst_lineset_index) + hash(self.snd_lineset_index)
194 
195     def increment(self, value: Index) -> LineSetStartCouple:
196         return LineSetStartCouple(
197             Index(self.fst_lineset_index + value),
198             Index(self.snd_lineset_index + value),
199         )
200 
201 
202 LinesChunkLimits_T = Tuple["LineSet", LineNumber, LineNumber]
203 
204 
205 def hash_lineset(
206     lineset: LineSet, min_common_lines: int = DEFAULT_MIN_SIMILARITY_LINE
207 ) -> tuple[HashToIndex_T, IndexToLines_T]:
208     """Return two dicts.
209 
210     The first associates the hash of successive stripped lines of a lineset
211     to the indices of the starting lines.
212     The second dict, associates the index of the starting line in the lineset's stripped lines to the
213     couple [start, end] lines number in the corresponding file.
214 
215     :param lineset: lineset object (i.e the lines in a file)
216     :param min_common_lines: number of successive lines that are used to compute the hash
217     :return: a dict linking hashes to corresponding start index and a dict that links this
218              index to the start and end lines in the file
219     """
220     hash2index = defaultdict(list)
221     index2lines = {}
222     # Comments, docstring and other specific patterns maybe excluded -> call to stripped_lines
223     # to get only what is desired
224     lines = tuple(x.text for x in lineset.stripped_lines)
225     # Need different iterators on same lines but each one is shifted 1 from the precedent
226     shifted_lines = [iter(lines[i:]) for i in range(min_common_lines)]
227 
228     for index_i, *succ_lines in enumerate(zip(*shifted_lines)):
229         start_linenumber = lineset.stripped_lines[index_i].line_number
230         try:
231             end_linenumber = lineset.stripped_lines[
232                 index_i + min_common_lines
233             ].line_number
234         except IndexError:
235             end_linenumber = lineset.stripped_lines[-1].line_number + 1
236 
237         index = Index(index_i)
238         index2lines[index] = SuccessiveLinesLimits(
239             start=LineNumber(start_linenumber), end=LineNumber(end_linenumber)
240         )
241 
242         l_c = LinesChunk(lineset.name, index, *succ_lines)
243         hash2index[l_c].append(index)
244 
245     return hash2index, index2lines
246 
247 
248 def remove_successive(all_couples: CplIndexToCplLines_T) -> None:
249     """Removes all successive entries in the dictionary in argument.
250 
251     :param all_couples: collection that has to be cleaned up from successive entries.
252                         The keys are couples of indices that mark the beginning of common entries
253                         in both linesets. The values have two parts. The first one is the couple
254                         of starting and ending line numbers of common successive lines in the first file.
255                         The second part is the same for the second file.
256 
257     For example consider the following dict:
258 
259     >>> all_couples
260     {(11, 34): ([5, 9], [27, 31]),
261      (23, 79): ([15, 19], [45, 49]),
262      (12, 35): ([6, 10], [28, 32])}
263 
264     There are two successive keys (11, 34) and (12, 35).
265     It means there are two consecutive similar chunks of lines in both files.
266     Thus remove last entry and update the last line numbers in the first entry
267 
268     >>> remove_successive(all_couples)
269     >>> all_couples
270     {(11, 34): ([5, 10], [27, 32]),
271      (23, 79): ([15, 19], [45, 49])}
272     """
273     couple: LineSetStartCouple
274     for couple in tuple(all_couples.keys()):
275         to_remove = []
276         test = couple.increment(Index(1))
277         while test in all_couples:
278             all_couples[couple].first_file.end = all_couples[test].first_file.end
279             all_couples[couple].second_file.end = all_couples[test].second_file.end
280             all_couples[couple].effective_cmn_lines_nb += 1
281             to_remove.append(test)
282             test = test.increment(Index(1))
283 
284         for target in to_remove:
285             try:
286                 all_couples.pop(target)
287             except KeyError:
288                 pass
289 
290 
291 def filter_noncode_lines(
292     ls_1: LineSet,
293     stindex_1: Index,
294     ls_2: LineSet,
295     stindex_2: Index,
296     common_lines_nb: int,
297 ) -> int:
298     """Return the effective number of common lines between lineset1
299     and lineset2 filtered from non code lines.
300 
301     That is to say the number of common successive stripped
302     lines except those that do not contain code (for example
303     a line with only an ending parenthesis)
304 
305     :param ls_1: first lineset
306     :param stindex_1: first lineset starting index
307     :param ls_2: second lineset
308     :param stindex_2: second lineset starting index
309     :param common_lines_nb: number of common successive stripped lines before being filtered from non code lines
310     :return: the number of common successive stripped lines that contain code
311     """
312     stripped_l1 = [
313         lspecif.text
314         for lspecif in ls_1.stripped_lines[stindex_1 : stindex_1 + common_lines_nb]
315         if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)
316     ]
317     stripped_l2 = [
318         lspecif.text
319         for lspecif in ls_2.stripped_lines[stindex_2 : stindex_2 + common_lines_nb]
320         if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)
321     ]
322     return sum(sline_1 == sline_2 for sline_1, sline_2 in zip(stripped_l1, stripped_l2))
323 
324 
325 class Commonality(NamedTuple):
326     cmn_lines_nb: int
327     fst_lset: LineSet
328     fst_file_start: LineNumber
329     fst_file_end: LineNumber
330     snd_lset: LineSet
331     snd_file_start: LineNumber
332     snd_file_end: LineNumber
333 
334 
335 class Similar:
336     """Finds copy-pasted lines of code in a project."""
337 
338     def __init__(
339         self,
340         min_lines: int = DEFAULT_MIN_SIMILARITY_LINE,
341         ignore_comments: bool = False,
342         ignore_docstrings: bool = False,
343         ignore_imports: bool = False,
344         ignore_signatures: bool = False,
345     ) -> None:
346         # If we run in pylint mode we link the namespace objects
347         if isinstance(self, BaseChecker):
348             self.namespace = self.linter.config
349         else:
350             self.namespace = argparse.Namespace()
351 
352         self.namespace.min_similarity_lines = min_lines
353         self.namespace.ignore_comments = ignore_comments
354         self.namespace.ignore_docstrings = ignore_docstrings
355         self.namespace.ignore_imports = ignore_imports
356         self.namespace.ignore_signatures = ignore_signatures
357         self.linesets: list[LineSet] = []
358 
359     def append_stream(
360         self, streamid: str, stream: STREAM_TYPES, encoding: str | None = None
361     ) -> None:
362         """Append a file to search for similarities."""
363         if isinstance(stream, BufferedIOBase):
364             if encoding is None:
365                 raise ValueError
366             readlines = decoding_stream(stream, encoding).readlines
367         else:
368             readlines = stream.readlines  # type: ignore[assignment] # hint parameter is incorrectly typed as non-optional
369 
370         try:
371             lines = readlines()
372         except UnicodeDecodeError:
373             lines = []
374 
375         self.linesets.append(
376             LineSet(
377                 streamid,
378                 lines,
379                 self.namespace.ignore_comments,
380                 self.namespace.ignore_docstrings,
381                 self.namespace.ignore_imports,
382                 self.namespace.ignore_signatures,
383                 line_enabled_callback=self.linter._is_one_message_enabled  # type: ignore[attr-defined]
384                 if hasattr(self, "linter")
385                 else None,
386             )
387         )
388 
389     def run(self) -> None:
390         """Start looking for similarities and display results on stdout."""
391         if self.namespace.min_similarity_lines == 0:
392             return
393         self._display_sims(self._compute_sims())
394 
395     def _compute_sims(self) -> list[tuple[int, set[LinesChunkLimits_T]]]:
396         """Compute similarities in appended files."""
397         no_duplicates: dict[int, list[set[LinesChunkLimits_T]]] = defaultdict(list)
398 
399         for commonality in self._iter_sims():
400             num = commonality.cmn_lines_nb
401             lineset1 = commonality.fst_lset
402             start_line_1 = commonality.fst_file_start
403             end_line_1 = commonality.fst_file_end
404             lineset2 = commonality.snd_lset
405             start_line_2 = commonality.snd_file_start
406             end_line_2 = commonality.snd_file_end
407 
408             duplicate = no_duplicates[num]
409             couples: set[LinesChunkLimits_T]
410             for couples in duplicate:
411                 if (lineset1, start_line_1, end_line_1) in couples or (
412                     lineset2,
413                     start_line_2,
414                     end_line_2,
415                 ) in couples:
416                     break
417             else:
418                 duplicate.append(
419                     {
420                         (lineset1, start_line_1, end_line_1),
421                         (lineset2, start_line_2, end_line_2),
422                     }
423                 )
424         sims: list[tuple[int, set[LinesChunkLimits_T]]] = []
425         ensembles: list[set[LinesChunkLimits_T]]
426         for num, ensembles in no_duplicates.items():
427             cpls: set[LinesChunkLimits_T]
428             for cpls in ensembles:
429                 sims.append((num, cpls))
430         sims.sort()
431         sims.reverse()
432         return sims
433 
434     def _display_sims(
435         self, similarities: list[tuple[int, set[LinesChunkLimits_T]]]
436     ) -> None:
437         """Display computed similarities on stdout."""
438         report = self._get_similarity_report(similarities)
439         print(report)
440 
441     def _get_similarity_report(
442         self, similarities: list[tuple[int, set[LinesChunkLimits_T]]]
443     ) -> str:
444         """Create a report from similarities."""
445         report: str = ""
446         duplicated_line_number: int = 0
447         for number, couples in similarities:
448             report += f"\n{number} similar lines in {len(couples)} files\n"
449             couples_l = sorted(couples)
450             line_set = start_line = end_line = None
451             for line_set, start_line, end_line in couples_l:
452                 report += f"=={line_set.name}:[{start_line}:{end_line}]\n"
453             if line_set:
454                 for line in line_set._real_lines[start_line:end_line]:
455                     report += f"   {line.rstrip()}\n" if line.rstrip() else "\n"
456             duplicated_line_number += number * (len(couples_l) - 1)
457         total_line_number: int = sum(len(lineset) for lineset in self.linesets)
458         report += f"TOTAL lines={total_line_number} duplicates={duplicated_line_number} percent={duplicated_line_number * 100.0 / total_line_number:.2f}\n"
459         return report
460 
461     def _find_common(
462         self, lineset1: LineSet, lineset2: LineSet
463     ) -> Generator[Commonality, None, None]:
464         """Find similarities in the two given linesets.
465 
466         This the core of the algorithm.
467         The idea is to compute the hashes of a minimal number of successive lines of each lineset and then compare the hashes.
468         Every match of such comparison is stored in a dict that links the couple of starting indices in both linesets to
469         the couple of corresponding starting and ending lines in both files.
470         Last regroups all successive couples in a bigger one. It allows to take into account common chunk of lines that have more
471         than the minimal number of successive lines required.
472         """
473         hash_to_index_1: HashToIndex_T
474         hash_to_index_2: HashToIndex_T
475         index_to_lines_1: IndexToLines_T
476         index_to_lines_2: IndexToLines_T
477         hash_to_index_1, index_to_lines_1 = hash_lineset(
478             lineset1, self.namespace.min_similarity_lines
479         )
480         hash_to_index_2, index_to_lines_2 = hash_lineset(
481             lineset2, self.namespace.min_similarity_lines
482         )
483 
484         hash_1: frozenset[LinesChunk] = frozenset(hash_to_index_1.keys())
485         hash_2: frozenset[LinesChunk] = frozenset(hash_to_index_2.keys())
486 
487         common_hashes: Iterable[LinesChunk] = sorted(
488             hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0]
489         )
490 
491         # all_couples is a dict that links the couple of indices in both linesets that mark the beginning of
492         # successive common lines, to the corresponding starting and ending number lines in both files
493         all_couples: CplIndexToCplLines_T = {}
494 
495         for c_hash in sorted(common_hashes, key=operator.attrgetter("_index")):
496             for indices_in_linesets in itertools.product(
497                 hash_to_index_1[c_hash], hash_to_index_2[c_hash]
498             ):
499                 index_1 = indices_in_linesets[0]
500                 index_2 = indices_in_linesets[1]
501                 all_couples[
502                     LineSetStartCouple(index_1, index_2)
503                 ] = CplSuccessiveLinesLimits(
504                     copy.copy(index_to_lines_1[index_1]),
505                     copy.copy(index_to_lines_2[index_2]),
506                     effective_cmn_lines_nb=self.namespace.min_similarity_lines,
507                 )
508 
509         remove_successive(all_couples)
510 
511         for cml_stripped_l, cmn_l in all_couples.items():
512             start_index_1 = cml_stripped_l.fst_lineset_index
513             start_index_2 = cml_stripped_l.snd_lineset_index
514             nb_common_lines = cmn_l.effective_cmn_lines_nb
515 
516             com = Commonality(
517                 cmn_lines_nb=nb_common_lines,
518                 fst_lset=lineset1,
519                 fst_file_start=cmn_l.first_file.start,
520                 fst_file_end=cmn_l.first_file.end,
521                 snd_lset=lineset2,
522                 snd_file_start=cmn_l.second_file.start,
523                 snd_file_end=cmn_l.second_file.end,
524             )
525 
526             eff_cmn_nb = filter_noncode_lines(
527                 lineset1, start_index_1, lineset2, start_index_2, nb_common_lines
528             )
529 
530             if eff_cmn_nb > self.namespace.min_similarity_lines:
531                 yield com
532 
533     def _iter_sims(self) -> Generator[Commonality, None, None]:
534         """Iterate on similarities among all files, by making a Cartesian
535         product.
536         """
537         for idx, lineset in enumerate(self.linesets[:-1]):
538             for lineset2 in self.linesets[idx + 1 :]:
539                 yield from self._find_common(lineset, lineset2)
540 
541     def get_map_data(self):
542         """Returns the data we can use for a map/reduce process.
543 
544         In this case we are returning this instance's Linesets, that is all file
545         information that will later be used for vectorisation.
546         """
547         return self.linesets
548 
549     def combine_mapreduce_data(self, linesets_collection):
550         """Reduces and recombines data into a format that we can report on.
551 
552         The partner function of get_map_data()
553         """
554         self.linesets = [line for lineset in linesets_collection for line in lineset]
555 
556 
557 def stripped_lines(
558     lines: Iterable[str],
559     ignore_comments: bool,
560     ignore_docstrings: bool,
561     ignore_imports: bool,
562     ignore_signatures: bool,
563     line_enabled_callback: Callable[[str, int], bool] | None = None,
564 ) -> list[LineSpecifs]:
565     """Return tuples of line/line number/line type with leading/trailing white-space and any ignored code features removed.
566 
567     :param lines: a collection of lines
568     :param ignore_comments: if true, any comment in the lines collection is removed from the result
569     :param ignore_docstrings: if true, any line that is a docstring is removed from the result
570     :param ignore_imports: if true, any line that is an import is removed from the result
571     :param ignore_signatures: if true, any line that is part of a function signature is removed from the result
572     :param line_enabled_callback: If called with "R0801" and a line number, a return value of False will disregard the line
573     :return: the collection of line/line number/line type tuples
574     """
575     if ignore_imports or ignore_signatures:
576         tree = astroid.parse("".join(lines))
577     if ignore_imports:
578         node_is_import_by_lineno = (
579             (node.lineno, isinstance(node, (nodes.Import, nodes.ImportFrom)))
580             for node in tree.body
581         )
582         line_begins_import = {
583             lineno: all(is_import for _, is_import in node_is_import_group)
584             for lineno, node_is_import_group in groupby(
585                 node_is_import_by_lineno, key=lambda x: x[0]
586             )
587         }
588         current_line_is_import = False
589     if ignore_signatures:
590 
591         def _get_functions(
592             functions: list[nodes.NodeNG], tree: nodes.NodeNG
593         ) -> list[nodes.NodeNG]:
594             """Recursively get all functions including nested in the classes from the tree."""
595 
596             for node in tree.body:
597                 if isinstance(node, (nodes.FunctionDef, nodes.AsyncFunctionDef)):
598                     functions.append(node)
599 
600                 if isinstance(
601                     node,
602                     (nodes.ClassDef, nodes.FunctionDef, nodes.AsyncFunctionDef),
603                 ):
604                     _get_functions(functions, node)
605 
606             return functions
607 
608         functions = _get_functions([], tree)
609         signature_lines = set(
610             chain(
611                 *(
612                     range(
613                         func.lineno,
614                         func.body[0].lineno if func.body else func.tolineno + 1,
615                     )
616                     for func in functions
617                 )
618             )
619         )
620 
621     strippedlines = []
622     docstring = None
623     for lineno, line in enumerate(lines, start=1):
624         if line_enabled_callback is not None and not line_enabled_callback(
625             "R0801", lineno
626         ):
627             continue
628         line = line.strip()
629         if ignore_docstrings:
630             if not docstring:
631                 if line.startswith('"""') or line.startswith("'''"):
632                     docstring = line[:3]
633                     line = line[3:]
634                 elif line.startswith('r"""') or line.startswith("r'''"):
635                     docstring = line[1:4]
636                     line = line[4:]
637             if docstring:
638                 if line.endswith(docstring):
639                     docstring = None
640                 line = ""
641         if ignore_imports:
642             current_line_is_import = line_begins_import.get(
643                 lineno, current_line_is_import
644             )
645             if current_line_is_import:
646                 line = ""
647         if ignore_comments:
648             line = line.split("#", 1)[0].strip()
649         if ignore_signatures and lineno in signature_lines:
650             line = ""
651         if line:
652             strippedlines.append(
653                 LineSpecifs(text=line, line_number=LineNumber(lineno - 1))
654             )
655     return strippedlines
656 
657 
658 @functools.total_ordering
659 class LineSet:
660     """Holds and indexes all the lines of a single source file.
661 
662     Allows for correspondence between real lines of the source file and stripped ones, which
663     are the real ones from which undesired patterns have been removed.
664     """
665 
666     def __init__(
667         self,
668         name: str,
669         lines: list[str],
670         ignore_comments: bool = False,
671         ignore_docstrings: bool = False,
672         ignore_imports: bool = False,
673         ignore_signatures: bool = False,
674         line_enabled_callback: Callable[[str, int], bool] | None = None,
675     ) -> None:
676         self.name = name
677         self._real_lines = lines
678         self._stripped_lines = stripped_lines(
679             lines,
680             ignore_comments,
681             ignore_docstrings,
682             ignore_imports,
683             ignore_signatures,
684             line_enabled_callback=line_enabled_callback,
685         )
686 
687     def __str__(self):
688         return f"<Lineset for {self.name}>"
689 
690     def __len__(self):
691         return len(self._real_lines)
692 
693     def __getitem__(self, index):
694         return self._stripped_lines[index]
695 
696     def __lt__(self, other):
697         return self.name < other.name
698 
699     def __hash__(self):
700         return id(self)
701 
702     def __eq__(self, other):
703         if not isinstance(other, LineSet):
704             return False
705         return self.__dict__ == other.__dict__
706 
707     @property
708     def stripped_lines(self):
709         return self._stripped_lines
710 
711     @property
712     def real_lines(self):
713         return self._real_lines
714 
715 
716 MSGS: dict[str, MessageDefinitionTuple] = {
717     "R0801": (
718         "Similar lines in %s files\n%s",
719         "duplicate-code",
720         "Indicates that a set of similar lines has been detected "
721         "among multiple file. This usually means that the code should "
722         "be refactored to avoid this duplication.",
723     )
724 }
725 
726 
727 def report_similarities(
728     sect,
729     stats: LinterStats,
730     old_stats: LinterStats | None,
731 ) -> None:
732     """Make a layout with some stats about duplication."""
733     lines = ["", "now", "previous", "difference"]
734     lines += table_lines_from_stats(stats, old_stats, "duplicated_lines")
735     sect.append(Table(children=lines, cols=4, rheaders=1, cheaders=1))
736 
737 
738 # wrapper to get a pylint checker from the similar class
739 class SimilarChecker(BaseRawFileChecker, Similar):
740     """Checks for similarities and duplicated code.
741 
742     This computation may be memory / CPU intensive, so you
743     should disable it if you experience some problems.
744     """
745 
746     # configuration section name
747     name = "similarities"
748     # messages
749     msgs = MSGS
750     # configuration options
751     # for available dict keys/values see the optik parser 'add_option' method
752     options: Options = (
753         (
754             "min-similarity-lines",
755             {
756                 "default": DEFAULT_MIN_SIMILARITY_LINE,
757                 "type": "int",
758                 "metavar": "<int>",
759                 "help": "Minimum lines number of a similarity.",
760             },
761         ),
762         (
763             "ignore-comments",
764             {
765                 "default": True,
766                 "type": "yn",
767                 "metavar": "<y or n>",
768                 "help": "Comments are removed from the similarity computation",
769             },
770         ),
771         (
772             "ignore-docstrings",
773             {
774                 "default": True,
775                 "type": "yn",
776                 "metavar": "<y or n>",
777                 "help": "Docstrings are removed from the similarity computation",
778             },
779         ),
780         (
781             "ignore-imports",
782             {
783                 "default": True,
784                 "type": "yn",
785                 "metavar": "<y or n>",
786                 "help": "Imports are removed from the similarity computation",
787             },
788         ),
789         (
790             "ignore-signatures",
791             {
792                 "default": True,
793                 "type": "yn",
794                 "metavar": "<y or n>",
795                 "help": "Signatures are removed from the similarity computation",
796             },
797         ),
798     )
799     # reports
800     reports = (("RP0801", "Duplication", report_similarities),)
801 
802     def __init__(self, linter: PyLinter) -> None:
803         BaseRawFileChecker.__init__(self, linter)
804         Similar.__init__(
805             self,
806             min_lines=self.linter.config.min_similarity_lines,
807             ignore_comments=self.linter.config.ignore_comments,
808             ignore_docstrings=self.linter.config.ignore_docstrings,
809             ignore_imports=self.linter.config.ignore_imports,
810             ignore_signatures=self.linter.config.ignore_signatures,
811         )
812 
813     def open(self):
814         """Init the checkers: reset linesets and statistics information."""
815         self.linesets = []
816         self.linter.stats.reset_duplicated_lines()
817 
818     def process_module(self, node: nodes.Module) -> None:
819         """Process a module.
820 
821         the module's content is accessible via the stream object
822 
823         stream must implement the readlines method
824         """
825         if self.linter.current_name is None:
826             warnings.warn(
827                 (
828                     "In pylint 3.0 the current_name attribute of the linter object should be a string. "
829                     "If unknown it should be initialized as an empty string."
830                 ),
831                 DeprecationWarning,
832             )
833         with node.stream() as stream:
834             self.append_stream(self.linter.current_name, stream, node.file_encoding)  # type: ignore[arg-type]
835 
836     def close(self):
837         """Compute and display similarities on closing (i.e. end of parsing)."""
838         total = sum(len(lineset) for lineset in self.linesets)
839         duplicated = 0
840         stats = self.linter.stats
841         for num, couples in self._compute_sims():
842             msg = []
843             lineset = start_line = end_line = None
844             for lineset, start_line, end_line in couples:
845                 msg.append(f"=={lineset.name}:[{start_line}:{end_line}]")
846             msg.sort()
847 
848             if lineset:
849                 for line in lineset.real_lines[start_line:end_line]:
850                     msg.append(line.rstrip())
851 
852             self.add_message("R0801", args=(len(couples), "\n".join(msg)))
853             duplicated += num * (len(couples) - 1)
854         stats.nb_duplicated_lines += int(duplicated)
855         stats.percent_duplicated_lines += float(total and duplicated * 100.0 / total)
856 
857     def get_map_data(self):
858         """Passthru override."""
859         return Similar.get_map_data(self)
860 
861     def reduce_map_data(self, linter, data):
862         """Reduces and recombines data into a format that we can report on.
863 
864         The partner function of get_map_data()
865         """
866         Similar.combine_mapreduce_data(self, linesets_collection=data)
867 
868 
869 def register(linter: PyLinter) -> None:
870     linter.register_checker(SimilarChecker(linter))
871 
872 
873 def usage(status=0):
874     """Display command line usage information."""
875     print("finds copy pasted blocks in a set of files")
876     print()
877     print(
878         "Usage: symilar [-d|--duplicates min_duplicated_lines] \
879 [-i|--ignore-comments] [--ignore-docstrings] [--ignore-imports] [--ignore-signatures] file1..."
880     )
881     sys.exit(status)
882 
883 
884 def Run(argv=None) -> NoReturn:
885     """Standalone command line access point."""
886     if argv is None:
887         argv = sys.argv[1:]
888 
889     s_opts = "hdi"
890     l_opts = [
891         "help",
892         "duplicates=",
893         "ignore-comments",
894         "ignore-imports",
895         "ignore-docstrings",
896         "ignore-signatures",
897     ]
898     min_lines = DEFAULT_MIN_SIMILARITY_LINE
899     ignore_comments = False
900     ignore_docstrings = False
901     ignore_imports = False
902     ignore_signatures = False
903     opts, args = getopt(argv, s_opts, l_opts)
904     for opt, val in opts:
905         if opt in {"-d", "--duplicates"}:
906             min_lines = int(val)
907         elif opt in {"-h", "--help"}:
908             usage()
909         elif opt in {"-i", "--ignore-comments"}:
910             ignore_comments = True
911         elif opt in {"--ignore-docstrings"}:
912             ignore_docstrings = True
913         elif opt in {"--ignore-imports"}:
914             ignore_imports = True
915         elif opt in {"--ignore-signatures"}:
916             ignore_signatures = True
917     if not args:
918         usage(1)
919     sim = Similar(
920         min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures
921     )
922     for filename in args:
923         with open(filename, encoding="utf-8") as stream:
924             sim.append_stream(filename, stream)
925     sim.run()
926     sys.exit(0)
927 
928 
929 if __name__ == "__main__":
930     Run()

</code><intended_behavior>The code within this file should properly heed the `--ignore`, `--ignore-paths`, and `--ignore-patterns` flags when performing similarity checks, particularly in recursive mode. This would involve filtering out files and directories that match the patterns specified by these flags before any analysis or similarity checks are conducted. The intended behavior ensures that any file or directory meant to be ignored as per the user's configuration is indeed skipped during linting, preserving the existing functionality of Pylint while fixing the issue of ignored entities still being analyzed.</intended_behavior>]
2025-10-18 16:05:37.010 | INFO     | Search completed. Bug locations: [<file>pylint/checkers/similar.py</file>
<class>None</class>
<method>None</method>
<code>
1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html
2 # For details: https://github.com/PyCQA/pylint/blob/main/LICENSE
3 # Copyright (c) https://github.com/PyCQA/pylint/blob/main/CONTRIBUTORS.txt
4 
5 """A similarities / code duplication command line tool and pylint checker.
6 
7 The algorithm is based on comparing the hash value of n successive lines of a file.
8 First the files are read and any line that doesn't fulfill requirement are removed (comments, docstrings...)
9 Those stripped lines are stored in the LineSet class which gives access to them.
10 Then each index of the stripped lines collection is associated with the hash of n successive entries of the stripped lines starting at the current index
11 (n is the minimum common lines option).
12 The common hashes between both linesets are then looked for. If there are matches, then the match indices in both linesets are stored and associated
13 with the corresponding couples (start line number/end line number) in both files.
14 This association is then post-processed to handle the case of successive matches. For example if the minimum common lines setting is set to four, then
15 the hashes are computed with four lines. If one of match indices couple (12, 34) is the successor of another one (11, 33) then it means that there are
16 in fact five lines which are common.
17 Once post-processed the values of association table are the result looked for, i.e start and end lines numbers of common lines in both files.
18 """
19 
20 from __future__ import annotations
21 
22 import argparse
23 import copy
24 import functools
25 import itertools
26 import operator
27 import re
28 import sys
29 import warnings
30 from collections import defaultdict
31 from collections.abc import Callable, Generator, Iterable
32 from getopt import getopt
33 from io import BufferedIOBase, BufferedReader, BytesIO
34 from itertools import chain, groupby
35 from typing import (
36     TYPE_CHECKING,
37     Any,
38     Dict,
39     List,
40     NamedTuple,
41     NewType,
42     NoReturn,
43     TextIO,
44     Tuple,
45     Union,
46 )
47 
48 import astroid
49 from astroid import nodes
50 
51 from pylint.checkers import BaseChecker, BaseRawFileChecker, table_lines_from_stats
52 from pylint.reporters.ureports.nodes import Table
53 from pylint.typing import MessageDefinitionTuple, Options
54 from pylint.utils import LinterStats, decoding_stream
55 
56 if TYPE_CHECKING:
57     from pylint.lint import PyLinter
58 
59 DEFAULT_MIN_SIMILARITY_LINE = 4
60 
61 REGEX_FOR_LINES_WITH_CONTENT = re.compile(r".*\w+")
62 
63 # Index defines a location in a LineSet stripped lines collection
64 Index = NewType("Index", int)
65 
66 # LineNumber defines a location in a LinesSet real lines collection (the whole file lines)
67 LineNumber = NewType("LineNumber", int)
68 
69 
70 # LineSpecifs holds characteristics of a line in a file
71 class LineSpecifs(NamedTuple):
72     line_number: LineNumber
73     text: str
74 
75 
76 # Links LinesChunk object to the starting indices (in lineset's stripped lines)
77 # of the different chunk of lines that are used to compute the hash
78 HashToIndex_T = Dict["LinesChunk", List[Index]]
79 
80 # Links index in the lineset's stripped lines to the real lines in the file
81 IndexToLines_T = Dict[Index, "SuccessiveLinesLimits"]
82 
83 # The types the streams read by pylint can take. Originating from astroid.nodes.Module.stream() and open()
84 STREAM_TYPES = Union[TextIO, BufferedReader, BytesIO]
85 
86 
87 class CplSuccessiveLinesLimits:
88     """Holds a SuccessiveLinesLimits object for each file compared and a
89     counter on the number of common lines between both stripped lines collections extracted from both files.
90     """
91 
92     __slots__ = ("first_file", "second_file", "effective_cmn_lines_nb")
93 
94     def __init__(
95         self,
96         first_file: SuccessiveLinesLimits,
97         second_file: SuccessiveLinesLimits,
98         effective_cmn_lines_nb: int,
99     ) -> None:
100         self.first_file = first_file
101         self.second_file = second_file
102         self.effective_cmn_lines_nb = effective_cmn_lines_nb
103 
104 
105 # Links the indices to the starting line in both lineset's stripped lines to
106 # the start and end lines in both files
107 CplIndexToCplLines_T = Dict["LineSetStartCouple", CplSuccessiveLinesLimits]
108 
109 
110 class LinesChunk:
111     """The LinesChunk object computes and stores the hash of some consecutive stripped lines of a lineset."""
112 
113     __slots__ = ("_fileid", "_index", "_hash")
114 
115     def __init__(self, fileid: str, num_line: int, *lines: Iterable[str]) -> None:
116         self._fileid: str = fileid
117         """The name of the file from which the LinesChunk object is generated."""
118 
119         self._index: Index = Index(num_line)
120         """The index in the stripped lines that is the starting of consecutive lines."""
121 
122         self._hash: int = sum(hash(lin) for lin in lines)
123         """The hash of some consecutive lines."""
124 
125     def __eq__(self, o: Any) -> bool:
126         if not isinstance(o, LinesChunk):
127             return NotImplemented
128         return self._hash == o._hash
129 
130     def __hash__(self) -> int:
131         return self._hash
132 
133     def __repr__(self) -> str:
134         return (
135             f"<LinesChunk object for file {self._fileid} ({self._index}, {self._hash})>"
136         )
137 
138     def __str__(self) -> str:
139         return (
140             f"LinesChunk object for file {self._fileid}, starting at line {self._index} \n"
141             f"Hash is {self._hash}"
142         )
143 
144 
145 class SuccessiveLinesLimits:
146     """A class to handle the numbering of begin and end of successive lines.
147 
148     :note: Only the end line number can be updated.
149     """
150 
151     __slots__ = ("_start", "_end")
152 
153     def __init__(self, start: LineNumber, end: LineNumber) -> None:
154         self._start: LineNumber = start
155         self._end: LineNumber = end
156 
157     @property
158     def start(self) -> LineNumber:
159         return self._start
160 
161     @property
162     def end(self) -> LineNumber:
163         return self._end
164 
165     @end.setter
166     def end(self, value: LineNumber) -> None:
167         self._end = value
168 
169     def __repr__(self) -> str:
170         return f"<SuccessiveLinesLimits <{self._start};{self._end}>>"
171 
172 
173 class LineSetStartCouple(NamedTuple):
174     """Indices in both linesets that mark the beginning of successive lines."""
175 
176     fst_lineset_index: Index
177     snd_lineset_index: Index
178 
179     def __repr__(self) -> str:
180         return (
181             f"<LineSetStartCouple <{self.fst_lineset_index};{self.snd_lineset_index}>>"
182         )
183 
184     def __eq__(self, other) -> bool:
185         if not isinstance(other, LineSetStartCouple):
186             return NotImplemented
187         return (
188             self.fst_lineset_index == other.fst_lineset_index
189             and self.snd_lineset_index == other.snd_lineset_index
190         )
191 
192     def __hash__(self) -> int:
193         return hash(self.fst_lineset_index) + hash(self.snd_lineset_index)
194 
195     def increment(self, value: Index) -> LineSetStartCouple:
196         return LineSetStartCouple(
197             Index(self.fst_lineset_index + value),
198             Index(self.snd_lineset_index + value),
199         )
200 
201 
202 LinesChunkLimits_T = Tuple["LineSet", LineNumber, LineNumber]
203 
204 
205 def hash_lineset(
206     lineset: LineSet, min_common_lines: int = DEFAULT_MIN_SIMILARITY_LINE
207 ) -> tuple[HashToIndex_T, IndexToLines_T]:
208     """Return two dicts.
209 
210     The first associates the hash of successive stripped lines of a lineset
211     to the indices of the starting lines.
212     The second dict, associates the index of the starting line in the lineset's stripped lines to the
213     couple [start, end] lines number in the corresponding file.
214 
215     :param lineset: lineset object (i.e the lines in a file)
216     :param min_common_lines: number of successive lines that are used to compute the hash
217     :return: a dict linking hashes to corresponding start index and a dict that links this
218              index to the start and end lines in the file
219     """
220     hash2index = defaultdict(list)
221     index2lines = {}
222     # Comments, docstring and other specific patterns maybe excluded -> call to stripped_lines
223     # to get only what is desired
224     lines = tuple(x.text for x in lineset.stripped_lines)
225     # Need different iterators on same lines but each one is shifted 1 from the precedent
226     shifted_lines = [iter(lines[i:]) for i in range(min_common_lines)]
227 
228     for index_i, *succ_lines in enumerate(zip(*shifted_lines)):
229         start_linenumber = lineset.stripped_lines[index_i].line_number
230         try:
231             end_linenumber = lineset.stripped_lines[
232                 index_i + min_common_lines
233             ].line_number
234         except IndexError:
235             end_linenumber = lineset.stripped_lines[-1].line_number + 1
236 
237         index = Index(index_i)
238         index2lines[index] = SuccessiveLinesLimits(
239             start=LineNumber(start_linenumber), end=LineNumber(end_linenumber)
240         )
241 
242         l_c = LinesChunk(lineset.name, index, *succ_lines)
243         hash2index[l_c].append(index)
244 
245     return hash2index, index2lines
246 
247 
248 def remove_successive(all_couples: CplIndexToCplLines_T) -> None:
249     """Removes all successive entries in the dictionary in argument.
250 
251     :param all_couples: collection that has to be cleaned up from successive entries.
252                         The keys are couples of indices that mark the beginning of common entries
253                         in both linesets. The values have two parts. The first one is the couple
254                         of starting and ending line numbers of common successive lines in the first file.
255                         The second part is the same for the second file.
256 
257     For example consider the following dict:
258 
259     >>> all_couples
260     {(11, 34): ([5, 9], [27, 31]),
261      (23, 79): ([15, 19], [45, 49]),
262      (12, 35): ([6, 10], [28, 32])}
263 
264     There are two successive keys (11, 34) and (12, 35).
265     It means there are two consecutive similar chunks of lines in both files.
266     Thus remove last entry and update the last line numbers in the first entry
267 
268     >>> remove_successive(all_couples)
269     >>> all_couples
270     {(11, 34): ([5, 10], [27, 32]),
271      (23, 79): ([15, 19], [45, 49])}
272     """
273     couple: LineSetStartCouple
274     for couple in tuple(all_couples.keys()):
275         to_remove = []
276         test = couple.increment(Index(1))
277         while test in all_couples:
278             all_couples[couple].first_file.end = all_couples[test].first_file.end
279             all_couples[couple].second_file.end = all_couples[test].second_file.end
280             all_couples[couple].effective_cmn_lines_nb += 1
281             to_remove.append(test)
282             test = test.increment(Index(1))
283 
284         for target in to_remove:
285             try:
286                 all_couples.pop(target)
287             except KeyError:
288                 pass
289 
290 
291 def filter_noncode_lines(
292     ls_1: LineSet,
293     stindex_1: Index,
294     ls_2: LineSet,
295     stindex_2: Index,
296     common_lines_nb: int,
297 ) -> int:
298     """Return the effective number of common lines between lineset1
299     and lineset2 filtered from non code lines.
300 
301     That is to say the number of common successive stripped
302     lines except those that do not contain code (for example
303     a line with only an ending parenthesis)
304 
305     :param ls_1: first lineset
306     :param stindex_1: first lineset starting index
307     :param ls_2: second lineset
308     :param stindex_2: second lineset starting index
309     :param common_lines_nb: number of common successive stripped lines before being filtered from non code lines
310     :return: the number of common successive stripped lines that contain code
311     """
312     stripped_l1 = [
313         lspecif.text
314         for lspecif in ls_1.stripped_lines[stindex_1 : stindex_1 + common_lines_nb]
315         if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)
316     ]
317     stripped_l2 = [
318         lspecif.text
319         for lspecif in ls_2.stripped_lines[stindex_2 : stindex_2 + common_lines_nb]
320         if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)
321     ]
322     return sum(sline_1 == sline_2 for sline_1, sline_2 in zip(stripped_l1, stripped_l2))
323 
324 
325 class Commonality(NamedTuple):
326     cmn_lines_nb: int
327     fst_lset: LineSet
328     fst_file_start: LineNumber
329     fst_file_end: LineNumber
330     snd_lset: LineSet
331     snd_file_start: LineNumber
332     snd_file_end: LineNumber
333 
334 
335 class Similar:
336     """Finds copy-pasted lines of code in a project."""
337 
338     def __init__(
339         self,
340         min_lines: int = DEFAULT_MIN_SIMILARITY_LINE,
341         ignore_comments: bool = False,
342         ignore_docstrings: bool = False,
343         ignore_imports: bool = False,
344         ignore_signatures: bool = False,
345     ) -> None:
346         # If we run in pylint mode we link the namespace objects
347         if isinstance(self, BaseChecker):
348             self.namespace = self.linter.config
349         else:
350             self.namespace = argparse.Namespace()
351 
352         self.namespace.min_similarity_lines = min_lines
353         self.namespace.ignore_comments = ignore_comments
354         self.namespace.ignore_docstrings = ignore_docstrings
355         self.namespace.ignore_imports = ignore_imports
356         self.namespace.ignore_signatures = ignore_signatures
357         self.linesets: list[LineSet] = []
358 
359     def append_stream(
360         self, streamid: str, stream: STREAM_TYPES, encoding: str | None = None
361     ) -> None:
362         """Append a file to search for similarities."""
363         if isinstance(stream, BufferedIOBase):
364             if encoding is None:
365                 raise ValueError
366             readlines = decoding_stream(stream, encoding).readlines
367         else:
368             readlines = stream.readlines  # type: ignore[assignment] # hint parameter is incorrectly typed as non-optional
369 
370         try:
371             lines = readlines()
372         except UnicodeDecodeError:
373             lines = []
374 
375         self.linesets.append(
376             LineSet(
377                 streamid,
378                 lines,
379                 self.namespace.ignore_comments,
380                 self.namespace.ignore_docstrings,
381                 self.namespace.ignore_imports,
382                 self.namespace.ignore_signatures,
383                 line_enabled_callback=self.linter._is_one_message_enabled  # type: ignore[attr-defined]
384                 if hasattr(self, "linter")
385                 else None,
386             )
387         )
388 
389     def run(self) -> None:
390         """Start looking for similarities and display results on stdout."""
391         if self.namespace.min_similarity_lines == 0:
392             return
393         self._display_sims(self._compute_sims())
394 
395     def _compute_sims(self) -> list[tuple[int, set[LinesChunkLimits_T]]]:
396         """Compute similarities in appended files."""
397         no_duplicates: dict[int, list[set[LinesChunkLimits_T]]] = defaultdict(list)
398 
399         for commonality in self._iter_sims():
400             num = commonality.cmn_lines_nb
401             lineset1 = commonality.fst_lset
402             start_line_1 = commonality.fst_file_start
403             end_line_1 = commonality.fst_file_end
404             lineset2 = commonality.snd_lset
405             start_line_2 = commonality.snd_file_start
406             end_line_2 = commonality.snd_file_end
407 
408             duplicate = no_duplicates[num]
409             couples: set[LinesChunkLimits_T]
410             for couples in duplicate:
411                 if (lineset1, start_line_1, end_line_1) in couples or (
412                     lineset2,
413                     start_line_2,
414                     end_line_2,
415                 ) in couples:
416                     break
417             else:
418                 duplicate.append(
419                     {
420                         (lineset1, start_line_1, end_line_1),
421                         (lineset2, start_line_2, end_line_2),
422                     }
423                 )
424         sims: list[tuple[int, set[LinesChunkLimits_T]]] = []
425         ensembles: list[set[LinesChunkLimits_T]]
426         for num, ensembles in no_duplicates.items():
427             cpls: set[LinesChunkLimits_T]
428             for cpls in ensembles:
429                 sims.append((num, cpls))
430         sims.sort()
431         sims.reverse()
432         return sims
433 
434     def _display_sims(
435         self, similarities: list[tuple[int, set[LinesChunkLimits_T]]]
436     ) -> None:
437         """Display computed similarities on stdout."""
438         report = self._get_similarity_report(similarities)
439         print(report)
440 
441     def _get_similarity_report(
442         self, similarities: list[tuple[int, set[LinesChunkLimits_T]]]
443     ) -> str:
444         """Create a report from similarities."""
445         report: str = ""
446         duplicated_line_number: int = 0
447         for number, couples in similarities:
448             report += f"\n{number} similar lines in {len(couples)} files\n"
449             couples_l = sorted(couples)
450             line_set = start_line = end_line = None
451             for line_set, start_line, end_line in couples_l:
452                 report += f"=={line_set.name}:[{start_line}:{end_line}]\n"
453             if line_set:
454                 for line in line_set._real_lines[start_line:end_line]:
455                     report += f"   {line.rstrip()}\n" if line.rstrip() else "\n"
456             duplicated_line_number += number * (len(couples_l) - 1)
457         total_line_number: int = sum(len(lineset) for lineset in self.linesets)
458         report += f"TOTAL lines={total_line_number} duplicates={duplicated_line_number} percent={duplicated_line_number * 100.0 / total_line_number:.2f}\n"
459         return report
460 
461     def _find_common(
462         self, lineset1: LineSet, lineset2: LineSet
463     ) -> Generator[Commonality, None, None]:
464         """Find similarities in the two given linesets.
465 
466         This the core of the algorithm.
467         The idea is to compute the hashes of a minimal number of successive lines of each lineset and then compare the hashes.
468         Every match of such comparison is stored in a dict that links the couple of starting indices in both linesets to
469         the couple of corresponding starting and ending lines in both files.
470         Last regroups all successive couples in a bigger one. It allows to take into account common chunk of lines that have more
471         than the minimal number of successive lines required.
472         """
473         hash_to_index_1: HashToIndex_T
474         hash_to_index_2: HashToIndex_T
475         index_to_lines_1: IndexToLines_T
476         index_to_lines_2: IndexToLines_T
477         hash_to_index_1, index_to_lines_1 = hash_lineset(
478             lineset1, self.namespace.min_similarity_lines
479         )
480         hash_to_index_2, index_to_lines_2 = hash_lineset(
481             lineset2, self.namespace.min_similarity_lines
482         )
483 
484         hash_1: frozenset[LinesChunk] = frozenset(hash_to_index_1.keys())
485         hash_2: frozenset[LinesChunk] = frozenset(hash_to_index_2.keys())
486 
487         common_hashes: Iterable[LinesChunk] = sorted(
488             hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0]
489         )
490 
491         # all_couples is a dict that links the couple of indices in both linesets that mark the beginning of
492         # successive common lines, to the corresponding starting and ending number lines in both files
493         all_couples: CplIndexToCplLines_T = {}
494 
495         for c_hash in sorted(common_hashes, key=operator.attrgetter("_index")):
496             for indices_in_linesets in itertools.product(
497                 hash_to_index_1[c_hash], hash_to_index_2[c_hash]
498             ):
499                 index_1 = indices_in_linesets[0]
500                 index_2 = indices_in_linesets[1]
501                 all_couples[
502                     LineSetStartCouple(index_1, index_2)
503                 ] = CplSuccessiveLinesLimits(
504                     copy.copy(index_to_lines_1[index_1]),
505                     copy.copy(index_to_lines_2[index_2]),
506                     effective_cmn_lines_nb=self.namespace.min_similarity_lines,
507                 )
508 
509         remove_successive(all_couples)
510 
511         for cml_stripped_l, cmn_l in all_couples.items():
512             start_index_1 = cml_stripped_l.fst_lineset_index
513             start_index_2 = cml_stripped_l.snd_lineset_index
514             nb_common_lines = cmn_l.effective_cmn_lines_nb
515 
516             com = Commonality(
517                 cmn_lines_nb=nb_common_lines,
518                 fst_lset=lineset1,
519                 fst_file_start=cmn_l.first_file.start,
520                 fst_file_end=cmn_l.first_file.end,
521                 snd_lset=lineset2,
522                 snd_file_start=cmn_l.second_file.start,
523                 snd_file_end=cmn_l.second_file.end,
524             )
525 
526             eff_cmn_nb = filter_noncode_lines(
527                 lineset1, start_index_1, lineset2, start_index_2, nb_common_lines
528             )
529 
530             if eff_cmn_nb > self.namespace.min_similarity_lines:
531                 yield com
532 
533     def _iter_sims(self) -> Generator[Commonality, None, None]:
534         """Iterate on similarities among all files, by making a Cartesian
535         product.
536         """
537         for idx, lineset in enumerate(self.linesets[:-1]):
538             for lineset2 in self.linesets[idx + 1 :]:
539                 yield from self._find_common(lineset, lineset2)
540 
541     def get_map_data(self):
542         """Returns the data we can use for a map/reduce process.
543 
544         In this case we are returning this instance's Linesets, that is all file
545         information that will later be used for vectorisation.
546         """
547         return self.linesets
548 
549     def combine_mapreduce_data(self, linesets_collection):
550         """Reduces and recombines data into a format that we can report on.
551 
552         The partner function of get_map_data()
553         """
554         self.linesets = [line for lineset in linesets_collection for line in lineset]
555 
556 
557 def stripped_lines(
558     lines: Iterable[str],
559     ignore_comments: bool,
560     ignore_docstrings: bool,
561     ignore_imports: bool,
562     ignore_signatures: bool,
563     line_enabled_callback: Callable[[str, int], bool] | None = None,
564 ) -> list[LineSpecifs]:
565     """Return tuples of line/line number/line type with leading/trailing white-space and any ignored code features removed.
566 
567     :param lines: a collection of lines
568     :param ignore_comments: if true, any comment in the lines collection is removed from the result
569     :param ignore_docstrings: if true, any line that is a docstring is removed from the result
570     :param ignore_imports: if true, any line that is an import is removed from the result
571     :param ignore_signatures: if true, any line that is part of a function signature is removed from the result
572     :param line_enabled_callback: If called with "R0801" and a line number, a return value of False will disregard the line
573     :return: the collection of line/line number/line type tuples
574     """
575     if ignore_imports or ignore_signatures:
576         tree = astroid.parse("".join(lines))
577     if ignore_imports:
578         node_is_import_by_lineno = (
579             (node.lineno, isinstance(node, (nodes.Import, nodes.ImportFrom)))
580             for node in tree.body
581         )
582         line_begins_import = {
583             lineno: all(is_import for _, is_import in node_is_import_group)
584             for lineno, node_is_import_group in groupby(
585                 node_is_import_by_lineno, key=lambda x: x[0]
586             )
587         }
588         current_line_is_import = False
589     if ignore_signatures:
590 
591         def _get_functions(
592             functions: list[nodes.NodeNG], tree: nodes.NodeNG
593         ) -> list[nodes.NodeNG]:
594             """Recursively get all functions including nested in the classes from the tree."""
595 
596             for node in tree.body:
597                 if isinstance(node, (nodes.FunctionDef, nodes.AsyncFunctionDef)):
598                     functions.append(node)
599 
600                 if isinstance(
601                     node,
602                     (nodes.ClassDef, nodes.FunctionDef, nodes.AsyncFunctionDef),
603                 ):
604                     _get_functions(functions, node)
605 
606             return functions
607 
608         functions = _get_functions([], tree)
609         signature_lines = set(
610             chain(
611                 *(
612                     range(
613                         func.lineno,
614                         func.body[0].lineno if func.body else func.tolineno + 1,
615                     )
616                     for func in functions
617                 )
618             )
619         )
620 
621     strippedlines = []
622     docstring = None
623     for lineno, line in enumerate(lines, start=1):
624         if line_enabled_callback is not None and not line_enabled_callback(
625             "R0801", lineno
626         ):
627             continue
628         line = line.strip()
629         if ignore_docstrings:
630             if not docstring:
631                 if line.startswith('"""') or line.startswith("'''"):
632                     docstring = line[:3]
633                     line = line[3:]
634                 elif line.startswith('r"""') or line.startswith("r'''"):
635                     docstring = line[1:4]
636                     line = line[4:]
637             if docstring:
638                 if line.endswith(docstring):
639                     docstring = None
640                 line = ""
641         if ignore_imports:
642             current_line_is_import = line_begins_import.get(
643                 lineno, current_line_is_import
644             )
645             if current_line_is_import:
646                 line = ""
647         if ignore_comments:
648             line = line.split("#", 1)[0].strip()
649         if ignore_signatures and lineno in signature_lines:
650             line = ""
651         if line:
652             strippedlines.append(
653                 LineSpecifs(text=line, line_number=LineNumber(lineno - 1))
654             )
655     return strippedlines
656 
657 
658 @functools.total_ordering
659 class LineSet:
660     """Holds and indexes all the lines of a single source file.
661 
662     Allows for correspondence between real lines of the source file and stripped ones, which
663     are the real ones from which undesired patterns have been removed.
664     """
665 
666     def __init__(
667         self,
668         name: str,
669         lines: list[str],
670         ignore_comments: bool = False,
671         ignore_docstrings: bool = False,
672         ignore_imports: bool = False,
673         ignore_signatures: bool = False,
674         line_enabled_callback: Callable[[str, int], bool] | None = None,
675     ) -> None:
676         self.name = name
677         self._real_lines = lines
678         self._stripped_lines = stripped_lines(
679             lines,
680             ignore_comments,
681             ignore_docstrings,
682             ignore_imports,
683             ignore_signatures,
684             line_enabled_callback=line_enabled_callback,
685         )
686 
687     def __str__(self):
688         return f"<Lineset for {self.name}>"
689 
690     def __len__(self):
691         return len(self._real_lines)
692 
693     def __getitem__(self, index):
694         return self._stripped_lines[index]
695 
696     def __lt__(self, other):
697         return self.name < other.name
698 
699     def __hash__(self):
700         return id(self)
701 
702     def __eq__(self, other):
703         if not isinstance(other, LineSet):
704             return False
705         return self.__dict__ == other.__dict__
706 
707     @property
708     def stripped_lines(self):
709         return self._stripped_lines
710 
711     @property
712     def real_lines(self):
713         return self._real_lines
714 
715 
716 MSGS: dict[str, MessageDefinitionTuple] = {
717     "R0801": (
718         "Similar lines in %s files\n%s",
719         "duplicate-code",
720         "Indicates that a set of similar lines has been detected "
721         "among multiple file. This usually means that the code should "
722         "be refactored to avoid this duplication.",
723     )
724 }
725 
726 
727 def report_similarities(
728     sect,
729     stats: LinterStats,
730     old_stats: LinterStats | None,
731 ) -> None:
732     """Make a layout with some stats about duplication."""
733     lines = ["", "now", "previous", "difference"]
734     lines += table_lines_from_stats(stats, old_stats, "duplicated_lines")
735     sect.append(Table(children=lines, cols=4, rheaders=1, cheaders=1))
736 
737 
738 # wrapper to get a pylint checker from the similar class
739 class SimilarChecker(BaseRawFileChecker, Similar):
740     """Checks for similarities and duplicated code.
741 
742     This computation may be memory / CPU intensive, so you
743     should disable it if you experience some problems.
744     """
745 
746     # configuration section name
747     name = "similarities"
748     # messages
749     msgs = MSGS
750     # configuration options
751     # for available dict keys/values see the optik parser 'add_option' method
752     options: Options = (
753         (
754             "min-similarity-lines",
755             {
756                 "default": DEFAULT_MIN_SIMILARITY_LINE,
757                 "type": "int",
758                 "metavar": "<int>",
759                 "help": "Minimum lines number of a similarity.",
760             },
761         ),
762         (
763             "ignore-comments",
764             {
765                 "default": True,
766                 "type": "yn",
767                 "metavar": "<y or n>",
768                 "help": "Comments are removed from the similarity computation",
769             },
770         ),
771         (
772             "ignore-docstrings",
773             {
774                 "default": True,
775                 "type": "yn",
776                 "metavar": "<y or n>",
777                 "help": "Docstrings are removed from the similarity computation",
778             },
779         ),
780         (
781             "ignore-imports",
782             {
783                 "default": True,
784                 "type": "yn",
785                 "metavar": "<y or n>",
786                 "help": "Imports are removed from the similarity computation",
787             },
788         ),
789         (
790             "ignore-signatures",
791             {
792                 "default": True,
793                 "type": "yn",
794                 "metavar": "<y or n>",
795                 "help": "Signatures are removed from the similarity computation",
796             },
797         ),
798     )
799     # reports
800     reports = (("RP0801", "Duplication", report_similarities),)
801 
802     def __init__(self, linter: PyLinter) -> None:
803         BaseRawFileChecker.__init__(self, linter)
804         Similar.__init__(
805             self,
806             min_lines=self.linter.config.min_similarity_lines,
807             ignore_comments=self.linter.config.ignore_comments,
808             ignore_docstrings=self.linter.config.ignore_docstrings,
809             ignore_imports=self.linter.config.ignore_imports,
810             ignore_signatures=self.linter.config.ignore_signatures,
811         )
812 
813     def open(self):
814         """Init the checkers: reset linesets and statistics information."""
815         self.linesets = []
816         self.linter.stats.reset_duplicated_lines()
817 
818     def process_module(self, node: nodes.Module) -> None:
819         """Process a module.
820 
821         the module's content is accessible via the stream object
822 
823         stream must implement the readlines method
824         """
825         if self.linter.current_name is None:
826             warnings.warn(
827                 (
828                     "In pylint 3.0 the current_name attribute of the linter object should be a string. "
829                     "If unknown it should be initialized as an empty string."
830                 ),
831                 DeprecationWarning,
832             )
833         with node.stream() as stream:
834             self.append_stream(self.linter.current_name, stream, node.file_encoding)  # type: ignore[arg-type]
835 
836     def close(self):
837         """Compute and display similarities on closing (i.e. end of parsing)."""
838         total = sum(len(lineset) for lineset in self.linesets)
839         duplicated = 0
840         stats = self.linter.stats
841         for num, couples in self._compute_sims():
842             msg = []
843             lineset = start_line = end_line = None
844             for lineset, start_line, end_line in couples:
845                 msg.append(f"=={lineset.name}:[{start_line}:{end_line}]")
846             msg.sort()
847 
848             if lineset:
849                 for line in lineset.real_lines[start_line:end_line]:
850                     msg.append(line.rstrip())
851 
852             self.add_message("R0801", args=(len(couples), "\n".join(msg)))
853             duplicated += num * (len(couples) - 1)
854         stats.nb_duplicated_lines += int(duplicated)
855         stats.percent_duplicated_lines += float(total and duplicated * 100.0 / total)
856 
857     def get_map_data(self):
858         """Passthru override."""
859         return Similar.get_map_data(self)
860 
861     def reduce_map_data(self, linter, data):
862         """Reduces and recombines data into a format that we can report on.
863 
864         The partner function of get_map_data()
865         """
866         Similar.combine_mapreduce_data(self, linesets_collection=data)
867 
868 
869 def register(linter: PyLinter) -> None:
870     linter.register_checker(SimilarChecker(linter))
871 
872 
873 def usage(status=0):
874     """Display command line usage information."""
875     print("finds copy pasted blocks in a set of files")
876     print()
877     print(
878         "Usage: symilar [-d|--duplicates min_duplicated_lines] \
879 [-i|--ignore-comments] [--ignore-docstrings] [--ignore-imports] [--ignore-signatures] file1..."
880     )
881     sys.exit(status)
882 
883 
884 def Run(argv=None) -> NoReturn:
885     """Standalone command line access point."""
886     if argv is None:
887         argv = sys.argv[1:]
888 
889     s_opts = "hdi"
890     l_opts = [
891         "help",
892         "duplicates=",
893         "ignore-comments",
894         "ignore-imports",
895         "ignore-docstrings",
896         "ignore-signatures",
897     ]
898     min_lines = DEFAULT_MIN_SIMILARITY_LINE
899     ignore_comments = False
900     ignore_docstrings = False
901     ignore_imports = False
902     ignore_signatures = False
903     opts, args = getopt(argv, s_opts, l_opts)
904     for opt, val in opts:
905         if opt in {"-d", "--duplicates"}:
906             min_lines = int(val)
907         elif opt in {"-h", "--help"}:
908             usage()
909         elif opt in {"-i", "--ignore-comments"}:
910             ignore_comments = True
911         elif opt in {"--ignore-docstrings"}:
912             ignore_docstrings = True
913         elif opt in {"--ignore-imports"}:
914             ignore_imports = True
915         elif opt in {"--ignore-signatures"}:
916             ignore_signatures = True
917     if not args:
918         usage(1)
919     sim = Similar(
920         min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures
921     )
922     for filename in args:
923         with open(filename, encoding="utf-8") as stream:
924             sim.append_stream(filename, stream)
925     sim.run()
926     sys.exit(0)
927 
928 
929 if __name__ == "__main__":
930     Run()

</code><intended_behavior>The code within this file should properly heed the `--ignore`, `--ignore-paths`, and `--ignore-patterns` flags when performing similarity checks, particularly in recursive mode. This would involve filtering out files and directories that match the patterns specified by these flags before any analysis or similarity checks are conducted. The intended behavior ensures that any file or directory meant to be ignored as per the user's configuration is indeed skipped during linting, preserving the existing functionality of Pylint while fixing the issue of ignored entities still being analyzed.</intended_behavior>]
2025-10-18 16:05:37.011 | INFO     | Overall retry 0 succeeded; ending workflow
2025-10-18 16:05:37.011 | INFO     | Search-only: skipping patch selection
2025-10-18 16:05:37.095 | INFO     | Task pylint-dev__pylint-6528 completed successfully.
2025-10-18 16:05:37.095 | INFO     | No patch generated. You can try running ACR again.
