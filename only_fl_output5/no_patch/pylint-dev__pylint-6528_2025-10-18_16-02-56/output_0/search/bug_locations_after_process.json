[
    {
        "rel_file_path": "pylint/checkers/similar.py",
        "abs_file_path": "/home/kimnal0/auto-code-rover/SWE-bench/testbed/pylint-dev__pylint/setup_pylint-dev__pylint__2.14/pylint/checkers/similar.py",
        "start": 1,
        "end": 930,
        "class_name": null,
        "method_name": null,
        "code": "1 # Licensed under the GPL: https://www.gnu.org/licenses/old-licenses/gpl-2.0.html\n2 # For details: https://github.com/PyCQA/pylint/blob/main/LICENSE\n3 # Copyright (c) https://github.com/PyCQA/pylint/blob/main/CONTRIBUTORS.txt\n4 \n5 \"\"\"A similarities / code duplication command line tool and pylint checker.\n6 \n7 The algorithm is based on comparing the hash value of n successive lines of a file.\n8 First the files are read and any line that doesn't fulfill requirement are removed (comments, docstrings...)\n9 Those stripped lines are stored in the LineSet class which gives access to them.\n10 Then each index of the stripped lines collection is associated with the hash of n successive entries of the stripped lines starting at the current index\n11 (n is the minimum common lines option).\n12 The common hashes between both linesets are then looked for. If there are matches, then the match indices in both linesets are stored and associated\n13 with the corresponding couples (start line number/end line number) in both files.\n14 This association is then post-processed to handle the case of successive matches. For example if the minimum common lines setting is set to four, then\n15 the hashes are computed with four lines. If one of match indices couple (12, 34) is the successor of another one (11, 33) then it means that there are\n16 in fact five lines which are common.\n17 Once post-processed the values of association table are the result looked for, i.e start and end lines numbers of common lines in both files.\n18 \"\"\"\n19 \n20 from __future__ import annotations\n21 \n22 import argparse\n23 import copy\n24 import functools\n25 import itertools\n26 import operator\n27 import re\n28 import sys\n29 import warnings\n30 from collections import defaultdict\n31 from collections.abc import Callable, Generator, Iterable\n32 from getopt import getopt\n33 from io import BufferedIOBase, BufferedReader, BytesIO\n34 from itertools import chain, groupby\n35 from typing import (\n36     TYPE_CHECKING,\n37     Any,\n38     Dict,\n39     List,\n40     NamedTuple,\n41     NewType,\n42     NoReturn,\n43     TextIO,\n44     Tuple,\n45     Union,\n46 )\n47 \n48 import astroid\n49 from astroid import nodes\n50 \n51 from pylint.checkers import BaseChecker, BaseRawFileChecker, table_lines_from_stats\n52 from pylint.reporters.ureports.nodes import Table\n53 from pylint.typing import MessageDefinitionTuple, Options\n54 from pylint.utils import LinterStats, decoding_stream\n55 \n56 if TYPE_CHECKING:\n57     from pylint.lint import PyLinter\n58 \n59 DEFAULT_MIN_SIMILARITY_LINE = 4\n60 \n61 REGEX_FOR_LINES_WITH_CONTENT = re.compile(r\".*\\w+\")\n62 \n63 # Index defines a location in a LineSet stripped lines collection\n64 Index = NewType(\"Index\", int)\n65 \n66 # LineNumber defines a location in a LinesSet real lines collection (the whole file lines)\n67 LineNumber = NewType(\"LineNumber\", int)\n68 \n69 \n70 # LineSpecifs holds characteristics of a line in a file\n71 class LineSpecifs(NamedTuple):\n72     line_number: LineNumber\n73     text: str\n74 \n75 \n76 # Links LinesChunk object to the starting indices (in lineset's stripped lines)\n77 # of the different chunk of lines that are used to compute the hash\n78 HashToIndex_T = Dict[\"LinesChunk\", List[Index]]\n79 \n80 # Links index in the lineset's stripped lines to the real lines in the file\n81 IndexToLines_T = Dict[Index, \"SuccessiveLinesLimits\"]\n82 \n83 # The types the streams read by pylint can take. Originating from astroid.nodes.Module.stream() and open()\n84 STREAM_TYPES = Union[TextIO, BufferedReader, BytesIO]\n85 \n86 \n87 class CplSuccessiveLinesLimits:\n88     \"\"\"Holds a SuccessiveLinesLimits object for each file compared and a\n89     counter on the number of common lines between both stripped lines collections extracted from both files.\n90     \"\"\"\n91 \n92     __slots__ = (\"first_file\", \"second_file\", \"effective_cmn_lines_nb\")\n93 \n94     def __init__(\n95         self,\n96         first_file: SuccessiveLinesLimits,\n97         second_file: SuccessiveLinesLimits,\n98         effective_cmn_lines_nb: int,\n99     ) -> None:\n100         self.first_file = first_file\n101         self.second_file = second_file\n102         self.effective_cmn_lines_nb = effective_cmn_lines_nb\n103 \n104 \n105 # Links the indices to the starting line in both lineset's stripped lines to\n106 # the start and end lines in both files\n107 CplIndexToCplLines_T = Dict[\"LineSetStartCouple\", CplSuccessiveLinesLimits]\n108 \n109 \n110 class LinesChunk:\n111     \"\"\"The LinesChunk object computes and stores the hash of some consecutive stripped lines of a lineset.\"\"\"\n112 \n113     __slots__ = (\"_fileid\", \"_index\", \"_hash\")\n114 \n115     def __init__(self, fileid: str, num_line: int, *lines: Iterable[str]) -> None:\n116         self._fileid: str = fileid\n117         \"\"\"The name of the file from which the LinesChunk object is generated.\"\"\"\n118 \n119         self._index: Index = Index(num_line)\n120         \"\"\"The index in the stripped lines that is the starting of consecutive lines.\"\"\"\n121 \n122         self._hash: int = sum(hash(lin) for lin in lines)\n123         \"\"\"The hash of some consecutive lines.\"\"\"\n124 \n125     def __eq__(self, o: Any) -> bool:\n126         if not isinstance(o, LinesChunk):\n127             return NotImplemented\n128         return self._hash == o._hash\n129 \n130     def __hash__(self) -> int:\n131         return self._hash\n132 \n133     def __repr__(self) -> str:\n134         return (\n135             f\"<LinesChunk object for file {self._fileid} ({self._index}, {self._hash})>\"\n136         )\n137 \n138     def __str__(self) -> str:\n139         return (\n140             f\"LinesChunk object for file {self._fileid}, starting at line {self._index} \\n\"\n141             f\"Hash is {self._hash}\"\n142         )\n143 \n144 \n145 class SuccessiveLinesLimits:\n146     \"\"\"A class to handle the numbering of begin and end of successive lines.\n147 \n148     :note: Only the end line number can be updated.\n149     \"\"\"\n150 \n151     __slots__ = (\"_start\", \"_end\")\n152 \n153     def __init__(self, start: LineNumber, end: LineNumber) -> None:\n154         self._start: LineNumber = start\n155         self._end: LineNumber = end\n156 \n157     @property\n158     def start(self) -> LineNumber:\n159         return self._start\n160 \n161     @property\n162     def end(self) -> LineNumber:\n163         return self._end\n164 \n165     @end.setter\n166     def end(self, value: LineNumber) -> None:\n167         self._end = value\n168 \n169     def __repr__(self) -> str:\n170         return f\"<SuccessiveLinesLimits <{self._start};{self._end}>>\"\n171 \n172 \n173 class LineSetStartCouple(NamedTuple):\n174     \"\"\"Indices in both linesets that mark the beginning of successive lines.\"\"\"\n175 \n176     fst_lineset_index: Index\n177     snd_lineset_index: Index\n178 \n179     def __repr__(self) -> str:\n180         return (\n181             f\"<LineSetStartCouple <{self.fst_lineset_index};{self.snd_lineset_index}>>\"\n182         )\n183 \n184     def __eq__(self, other) -> bool:\n185         if not isinstance(other, LineSetStartCouple):\n186             return NotImplemented\n187         return (\n188             self.fst_lineset_index == other.fst_lineset_index\n189             and self.snd_lineset_index == other.snd_lineset_index\n190         )\n191 \n192     def __hash__(self) -> int:\n193         return hash(self.fst_lineset_index) + hash(self.snd_lineset_index)\n194 \n195     def increment(self, value: Index) -> LineSetStartCouple:\n196         return LineSetStartCouple(\n197             Index(self.fst_lineset_index + value),\n198             Index(self.snd_lineset_index + value),\n199         )\n200 \n201 \n202 LinesChunkLimits_T = Tuple[\"LineSet\", LineNumber, LineNumber]\n203 \n204 \n205 def hash_lineset(\n206     lineset: LineSet, min_common_lines: int = DEFAULT_MIN_SIMILARITY_LINE\n207 ) -> tuple[HashToIndex_T, IndexToLines_T]:\n208     \"\"\"Return two dicts.\n209 \n210     The first associates the hash of successive stripped lines of a lineset\n211     to the indices of the starting lines.\n212     The second dict, associates the index of the starting line in the lineset's stripped lines to the\n213     couple [start, end] lines number in the corresponding file.\n214 \n215     :param lineset: lineset object (i.e the lines in a file)\n216     :param min_common_lines: number of successive lines that are used to compute the hash\n217     :return: a dict linking hashes to corresponding start index and a dict that links this\n218              index to the start and end lines in the file\n219     \"\"\"\n220     hash2index = defaultdict(list)\n221     index2lines = {}\n222     # Comments, docstring and other specific patterns maybe excluded -> call to stripped_lines\n223     # to get only what is desired\n224     lines = tuple(x.text for x in lineset.stripped_lines)\n225     # Need different iterators on same lines but each one is shifted 1 from the precedent\n226     shifted_lines = [iter(lines[i:]) for i in range(min_common_lines)]\n227 \n228     for index_i, *succ_lines in enumerate(zip(*shifted_lines)):\n229         start_linenumber = lineset.stripped_lines[index_i].line_number\n230         try:\n231             end_linenumber = lineset.stripped_lines[\n232                 index_i + min_common_lines\n233             ].line_number\n234         except IndexError:\n235             end_linenumber = lineset.stripped_lines[-1].line_number + 1\n236 \n237         index = Index(index_i)\n238         index2lines[index] = SuccessiveLinesLimits(\n239             start=LineNumber(start_linenumber), end=LineNumber(end_linenumber)\n240         )\n241 \n242         l_c = LinesChunk(lineset.name, index, *succ_lines)\n243         hash2index[l_c].append(index)\n244 \n245     return hash2index, index2lines\n246 \n247 \n248 def remove_successive(all_couples: CplIndexToCplLines_T) -> None:\n249     \"\"\"Removes all successive entries in the dictionary in argument.\n250 \n251     :param all_couples: collection that has to be cleaned up from successive entries.\n252                         The keys are couples of indices that mark the beginning of common entries\n253                         in both linesets. The values have two parts. The first one is the couple\n254                         of starting and ending line numbers of common successive lines in the first file.\n255                         The second part is the same for the second file.\n256 \n257     For example consider the following dict:\n258 \n259     >>> all_couples\n260     {(11, 34): ([5, 9], [27, 31]),\n261      (23, 79): ([15, 19], [45, 49]),\n262      (12, 35): ([6, 10], [28, 32])}\n263 \n264     There are two successive keys (11, 34) and (12, 35).\n265     It means there are two consecutive similar chunks of lines in both files.\n266     Thus remove last entry and update the last line numbers in the first entry\n267 \n268     >>> remove_successive(all_couples)\n269     >>> all_couples\n270     {(11, 34): ([5, 10], [27, 32]),\n271      (23, 79): ([15, 19], [45, 49])}\n272     \"\"\"\n273     couple: LineSetStartCouple\n274     for couple in tuple(all_couples.keys()):\n275         to_remove = []\n276         test = couple.increment(Index(1))\n277         while test in all_couples:\n278             all_couples[couple].first_file.end = all_couples[test].first_file.end\n279             all_couples[couple].second_file.end = all_couples[test].second_file.end\n280             all_couples[couple].effective_cmn_lines_nb += 1\n281             to_remove.append(test)\n282             test = test.increment(Index(1))\n283 \n284         for target in to_remove:\n285             try:\n286                 all_couples.pop(target)\n287             except KeyError:\n288                 pass\n289 \n290 \n291 def filter_noncode_lines(\n292     ls_1: LineSet,\n293     stindex_1: Index,\n294     ls_2: LineSet,\n295     stindex_2: Index,\n296     common_lines_nb: int,\n297 ) -> int:\n298     \"\"\"Return the effective number of common lines between lineset1\n299     and lineset2 filtered from non code lines.\n300 \n301     That is to say the number of common successive stripped\n302     lines except those that do not contain code (for example\n303     a line with only an ending parenthesis)\n304 \n305     :param ls_1: first lineset\n306     :param stindex_1: first lineset starting index\n307     :param ls_2: second lineset\n308     :param stindex_2: second lineset starting index\n309     :param common_lines_nb: number of common successive stripped lines before being filtered from non code lines\n310     :return: the number of common successive stripped lines that contain code\n311     \"\"\"\n312     stripped_l1 = [\n313         lspecif.text\n314         for lspecif in ls_1.stripped_lines[stindex_1 : stindex_1 + common_lines_nb]\n315         if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)\n316     ]\n317     stripped_l2 = [\n318         lspecif.text\n319         for lspecif in ls_2.stripped_lines[stindex_2 : stindex_2 + common_lines_nb]\n320         if REGEX_FOR_LINES_WITH_CONTENT.match(lspecif.text)\n321     ]\n322     return sum(sline_1 == sline_2 for sline_1, sline_2 in zip(stripped_l1, stripped_l2))\n323 \n324 \n325 class Commonality(NamedTuple):\n326     cmn_lines_nb: int\n327     fst_lset: LineSet\n328     fst_file_start: LineNumber\n329     fst_file_end: LineNumber\n330     snd_lset: LineSet\n331     snd_file_start: LineNumber\n332     snd_file_end: LineNumber\n333 \n334 \n335 class Similar:\n336     \"\"\"Finds copy-pasted lines of code in a project.\"\"\"\n337 \n338     def __init__(\n339         self,\n340         min_lines: int = DEFAULT_MIN_SIMILARITY_LINE,\n341         ignore_comments: bool = False,\n342         ignore_docstrings: bool = False,\n343         ignore_imports: bool = False,\n344         ignore_signatures: bool = False,\n345     ) -> None:\n346         # If we run in pylint mode we link the namespace objects\n347         if isinstance(self, BaseChecker):\n348             self.namespace = self.linter.config\n349         else:\n350             self.namespace = argparse.Namespace()\n351 \n352         self.namespace.min_similarity_lines = min_lines\n353         self.namespace.ignore_comments = ignore_comments\n354         self.namespace.ignore_docstrings = ignore_docstrings\n355         self.namespace.ignore_imports = ignore_imports\n356         self.namespace.ignore_signatures = ignore_signatures\n357         self.linesets: list[LineSet] = []\n358 \n359     def append_stream(\n360         self, streamid: str, stream: STREAM_TYPES, encoding: str | None = None\n361     ) -> None:\n362         \"\"\"Append a file to search for similarities.\"\"\"\n363         if isinstance(stream, BufferedIOBase):\n364             if encoding is None:\n365                 raise ValueError\n366             readlines = decoding_stream(stream, encoding).readlines\n367         else:\n368             readlines = stream.readlines  # type: ignore[assignment] # hint parameter is incorrectly typed as non-optional\n369 \n370         try:\n371             lines = readlines()\n372         except UnicodeDecodeError:\n373             lines = []\n374 \n375         self.linesets.append(\n376             LineSet(\n377                 streamid,\n378                 lines,\n379                 self.namespace.ignore_comments,\n380                 self.namespace.ignore_docstrings,\n381                 self.namespace.ignore_imports,\n382                 self.namespace.ignore_signatures,\n383                 line_enabled_callback=self.linter._is_one_message_enabled  # type: ignore[attr-defined]\n384                 if hasattr(self, \"linter\")\n385                 else None,\n386             )\n387         )\n388 \n389     def run(self) -> None:\n390         \"\"\"Start looking for similarities and display results on stdout.\"\"\"\n391         if self.namespace.min_similarity_lines == 0:\n392             return\n393         self._display_sims(self._compute_sims())\n394 \n395     def _compute_sims(self) -> list[tuple[int, set[LinesChunkLimits_T]]]:\n396         \"\"\"Compute similarities in appended files.\"\"\"\n397         no_duplicates: dict[int, list[set[LinesChunkLimits_T]]] = defaultdict(list)\n398 \n399         for commonality in self._iter_sims():\n400             num = commonality.cmn_lines_nb\n401             lineset1 = commonality.fst_lset\n402             start_line_1 = commonality.fst_file_start\n403             end_line_1 = commonality.fst_file_end\n404             lineset2 = commonality.snd_lset\n405             start_line_2 = commonality.snd_file_start\n406             end_line_2 = commonality.snd_file_end\n407 \n408             duplicate = no_duplicates[num]\n409             couples: set[LinesChunkLimits_T]\n410             for couples in duplicate:\n411                 if (lineset1, start_line_1, end_line_1) in couples or (\n412                     lineset2,\n413                     start_line_2,\n414                     end_line_2,\n415                 ) in couples:\n416                     break\n417             else:\n418                 duplicate.append(\n419                     {\n420                         (lineset1, start_line_1, end_line_1),\n421                         (lineset2, start_line_2, end_line_2),\n422                     }\n423                 )\n424         sims: list[tuple[int, set[LinesChunkLimits_T]]] = []\n425         ensembles: list[set[LinesChunkLimits_T]]\n426         for num, ensembles in no_duplicates.items():\n427             cpls: set[LinesChunkLimits_T]\n428             for cpls in ensembles:\n429                 sims.append((num, cpls))\n430         sims.sort()\n431         sims.reverse()\n432         return sims\n433 \n434     def _display_sims(\n435         self, similarities: list[tuple[int, set[LinesChunkLimits_T]]]\n436     ) -> None:\n437         \"\"\"Display computed similarities on stdout.\"\"\"\n438         report = self._get_similarity_report(similarities)\n439         print(report)\n440 \n441     def _get_similarity_report(\n442         self, similarities: list[tuple[int, set[LinesChunkLimits_T]]]\n443     ) -> str:\n444         \"\"\"Create a report from similarities.\"\"\"\n445         report: str = \"\"\n446         duplicated_line_number: int = 0\n447         for number, couples in similarities:\n448             report += f\"\\n{number} similar lines in {len(couples)} files\\n\"\n449             couples_l = sorted(couples)\n450             line_set = start_line = end_line = None\n451             for line_set, start_line, end_line in couples_l:\n452                 report += f\"=={line_set.name}:[{start_line}:{end_line}]\\n\"\n453             if line_set:\n454                 for line in line_set._real_lines[start_line:end_line]:\n455                     report += f\"   {line.rstrip()}\\n\" if line.rstrip() else \"\\n\"\n456             duplicated_line_number += number * (len(couples_l) - 1)\n457         total_line_number: int = sum(len(lineset) for lineset in self.linesets)\n458         report += f\"TOTAL lines={total_line_number} duplicates={duplicated_line_number} percent={duplicated_line_number * 100.0 / total_line_number:.2f}\\n\"\n459         return report\n460 \n461     def _find_common(\n462         self, lineset1: LineSet, lineset2: LineSet\n463     ) -> Generator[Commonality, None, None]:\n464         \"\"\"Find similarities in the two given linesets.\n465 \n466         This the core of the algorithm.\n467         The idea is to compute the hashes of a minimal number of successive lines of each lineset and then compare the hashes.\n468         Every match of such comparison is stored in a dict that links the couple of starting indices in both linesets to\n469         the couple of corresponding starting and ending lines in both files.\n470         Last regroups all successive couples in a bigger one. It allows to take into account common chunk of lines that have more\n471         than the minimal number of successive lines required.\n472         \"\"\"\n473         hash_to_index_1: HashToIndex_T\n474         hash_to_index_2: HashToIndex_T\n475         index_to_lines_1: IndexToLines_T\n476         index_to_lines_2: IndexToLines_T\n477         hash_to_index_1, index_to_lines_1 = hash_lineset(\n478             lineset1, self.namespace.min_similarity_lines\n479         )\n480         hash_to_index_2, index_to_lines_2 = hash_lineset(\n481             lineset2, self.namespace.min_similarity_lines\n482         )\n483 \n484         hash_1: frozenset[LinesChunk] = frozenset(hash_to_index_1.keys())\n485         hash_2: frozenset[LinesChunk] = frozenset(hash_to_index_2.keys())\n486 \n487         common_hashes: Iterable[LinesChunk] = sorted(\n488             hash_1 & hash_2, key=lambda m: hash_to_index_1[m][0]\n489         )\n490 \n491         # all_couples is a dict that links the couple of indices in both linesets that mark the beginning of\n492         # successive common lines, to the corresponding starting and ending number lines in both files\n493         all_couples: CplIndexToCplLines_T = {}\n494 \n495         for c_hash in sorted(common_hashes, key=operator.attrgetter(\"_index\")):\n496             for indices_in_linesets in itertools.product(\n497                 hash_to_index_1[c_hash], hash_to_index_2[c_hash]\n498             ):\n499                 index_1 = indices_in_linesets[0]\n500                 index_2 = indices_in_linesets[1]\n501                 all_couples[\n502                     LineSetStartCouple(index_1, index_2)\n503                 ] = CplSuccessiveLinesLimits(\n504                     copy.copy(index_to_lines_1[index_1]),\n505                     copy.copy(index_to_lines_2[index_2]),\n506                     effective_cmn_lines_nb=self.namespace.min_similarity_lines,\n507                 )\n508 \n509         remove_successive(all_couples)\n510 \n511         for cml_stripped_l, cmn_l in all_couples.items():\n512             start_index_1 = cml_stripped_l.fst_lineset_index\n513             start_index_2 = cml_stripped_l.snd_lineset_index\n514             nb_common_lines = cmn_l.effective_cmn_lines_nb\n515 \n516             com = Commonality(\n517                 cmn_lines_nb=nb_common_lines,\n518                 fst_lset=lineset1,\n519                 fst_file_start=cmn_l.first_file.start,\n520                 fst_file_end=cmn_l.first_file.end,\n521                 snd_lset=lineset2,\n522                 snd_file_start=cmn_l.second_file.start,\n523                 snd_file_end=cmn_l.second_file.end,\n524             )\n525 \n526             eff_cmn_nb = filter_noncode_lines(\n527                 lineset1, start_index_1, lineset2, start_index_2, nb_common_lines\n528             )\n529 \n530             if eff_cmn_nb > self.namespace.min_similarity_lines:\n531                 yield com\n532 \n533     def _iter_sims(self) -> Generator[Commonality, None, None]:\n534         \"\"\"Iterate on similarities among all files, by making a Cartesian\n535         product.\n536         \"\"\"\n537         for idx, lineset in enumerate(self.linesets[:-1]):\n538             for lineset2 in self.linesets[idx + 1 :]:\n539                 yield from self._find_common(lineset, lineset2)\n540 \n541     def get_map_data(self):\n542         \"\"\"Returns the data we can use for a map/reduce process.\n543 \n544         In this case we are returning this instance's Linesets, that is all file\n545         information that will later be used for vectorisation.\n546         \"\"\"\n547         return self.linesets\n548 \n549     def combine_mapreduce_data(self, linesets_collection):\n550         \"\"\"Reduces and recombines data into a format that we can report on.\n551 \n552         The partner function of get_map_data()\n553         \"\"\"\n554         self.linesets = [line for lineset in linesets_collection for line in lineset]\n555 \n556 \n557 def stripped_lines(\n558     lines: Iterable[str],\n559     ignore_comments: bool,\n560     ignore_docstrings: bool,\n561     ignore_imports: bool,\n562     ignore_signatures: bool,\n563     line_enabled_callback: Callable[[str, int], bool] | None = None,\n564 ) -> list[LineSpecifs]:\n565     \"\"\"Return tuples of line/line number/line type with leading/trailing white-space and any ignored code features removed.\n566 \n567     :param lines: a collection of lines\n568     :param ignore_comments: if true, any comment in the lines collection is removed from the result\n569     :param ignore_docstrings: if true, any line that is a docstring is removed from the result\n570     :param ignore_imports: if true, any line that is an import is removed from the result\n571     :param ignore_signatures: if true, any line that is part of a function signature is removed from the result\n572     :param line_enabled_callback: If called with \"R0801\" and a line number, a return value of False will disregard the line\n573     :return: the collection of line/line number/line type tuples\n574     \"\"\"\n575     if ignore_imports or ignore_signatures:\n576         tree = astroid.parse(\"\".join(lines))\n577     if ignore_imports:\n578         node_is_import_by_lineno = (\n579             (node.lineno, isinstance(node, (nodes.Import, nodes.ImportFrom)))\n580             for node in tree.body\n581         )\n582         line_begins_import = {\n583             lineno: all(is_import for _, is_import in node_is_import_group)\n584             for lineno, node_is_import_group in groupby(\n585                 node_is_import_by_lineno, key=lambda x: x[0]\n586             )\n587         }\n588         current_line_is_import = False\n589     if ignore_signatures:\n590 \n591         def _get_functions(\n592             functions: list[nodes.NodeNG], tree: nodes.NodeNG\n593         ) -> list[nodes.NodeNG]:\n594             \"\"\"Recursively get all functions including nested in the classes from the tree.\"\"\"\n595 \n596             for node in tree.body:\n597                 if isinstance(node, (nodes.FunctionDef, nodes.AsyncFunctionDef)):\n598                     functions.append(node)\n599 \n600                 if isinstance(\n601                     node,\n602                     (nodes.ClassDef, nodes.FunctionDef, nodes.AsyncFunctionDef),\n603                 ):\n604                     _get_functions(functions, node)\n605 \n606             return functions\n607 \n608         functions = _get_functions([], tree)\n609         signature_lines = set(\n610             chain(\n611                 *(\n612                     range(\n613                         func.lineno,\n614                         func.body[0].lineno if func.body else func.tolineno + 1,\n615                     )\n616                     for func in functions\n617                 )\n618             )\n619         )\n620 \n621     strippedlines = []\n622     docstring = None\n623     for lineno, line in enumerate(lines, start=1):\n624         if line_enabled_callback is not None and not line_enabled_callback(\n625             \"R0801\", lineno\n626         ):\n627             continue\n628         line = line.strip()\n629         if ignore_docstrings:\n630             if not docstring:\n631                 if line.startswith('\"\"\"') or line.startswith(\"'''\"):\n632                     docstring = line[:3]\n633                     line = line[3:]\n634                 elif line.startswith('r\"\"\"') or line.startswith(\"r'''\"):\n635                     docstring = line[1:4]\n636                     line = line[4:]\n637             if docstring:\n638                 if line.endswith(docstring):\n639                     docstring = None\n640                 line = \"\"\n641         if ignore_imports:\n642             current_line_is_import = line_begins_import.get(\n643                 lineno, current_line_is_import\n644             )\n645             if current_line_is_import:\n646                 line = \"\"\n647         if ignore_comments:\n648             line = line.split(\"#\", 1)[0].strip()\n649         if ignore_signatures and lineno in signature_lines:\n650             line = \"\"\n651         if line:\n652             strippedlines.append(\n653                 LineSpecifs(text=line, line_number=LineNumber(lineno - 1))\n654             )\n655     return strippedlines\n656 \n657 \n658 @functools.total_ordering\n659 class LineSet:\n660     \"\"\"Holds and indexes all the lines of a single source file.\n661 \n662     Allows for correspondence between real lines of the source file and stripped ones, which\n663     are the real ones from which undesired patterns have been removed.\n664     \"\"\"\n665 \n666     def __init__(\n667         self,\n668         name: str,\n669         lines: list[str],\n670         ignore_comments: bool = False,\n671         ignore_docstrings: bool = False,\n672         ignore_imports: bool = False,\n673         ignore_signatures: bool = False,\n674         line_enabled_callback: Callable[[str, int], bool] | None = None,\n675     ) -> None:\n676         self.name = name\n677         self._real_lines = lines\n678         self._stripped_lines = stripped_lines(\n679             lines,\n680             ignore_comments,\n681             ignore_docstrings,\n682             ignore_imports,\n683             ignore_signatures,\n684             line_enabled_callback=line_enabled_callback,\n685         )\n686 \n687     def __str__(self):\n688         return f\"<Lineset for {self.name}>\"\n689 \n690     def __len__(self):\n691         return len(self._real_lines)\n692 \n693     def __getitem__(self, index):\n694         return self._stripped_lines[index]\n695 \n696     def __lt__(self, other):\n697         return self.name < other.name\n698 \n699     def __hash__(self):\n700         return id(self)\n701 \n702     def __eq__(self, other):\n703         if not isinstance(other, LineSet):\n704             return False\n705         return self.__dict__ == other.__dict__\n706 \n707     @property\n708     def stripped_lines(self):\n709         return self._stripped_lines\n710 \n711     @property\n712     def real_lines(self):\n713         return self._real_lines\n714 \n715 \n716 MSGS: dict[str, MessageDefinitionTuple] = {\n717     \"R0801\": (\n718         \"Similar lines in %s files\\n%s\",\n719         \"duplicate-code\",\n720         \"Indicates that a set of similar lines has been detected \"\n721         \"among multiple file. This usually means that the code should \"\n722         \"be refactored to avoid this duplication.\",\n723     )\n724 }\n725 \n726 \n727 def report_similarities(\n728     sect,\n729     stats: LinterStats,\n730     old_stats: LinterStats | None,\n731 ) -> None:\n732     \"\"\"Make a layout with some stats about duplication.\"\"\"\n733     lines = [\"\", \"now\", \"previous\", \"difference\"]\n734     lines += table_lines_from_stats(stats, old_stats, \"duplicated_lines\")\n735     sect.append(Table(children=lines, cols=4, rheaders=1, cheaders=1))\n736 \n737 \n738 # wrapper to get a pylint checker from the similar class\n739 class SimilarChecker(BaseRawFileChecker, Similar):\n740     \"\"\"Checks for similarities and duplicated code.\n741 \n742     This computation may be memory / CPU intensive, so you\n743     should disable it if you experience some problems.\n744     \"\"\"\n745 \n746     # configuration section name\n747     name = \"similarities\"\n748     # messages\n749     msgs = MSGS\n750     # configuration options\n751     # for available dict keys/values see the optik parser 'add_option' method\n752     options: Options = (\n753         (\n754             \"min-similarity-lines\",\n755             {\n756                 \"default\": DEFAULT_MIN_SIMILARITY_LINE,\n757                 \"type\": \"int\",\n758                 \"metavar\": \"<int>\",\n759                 \"help\": \"Minimum lines number of a similarity.\",\n760             },\n761         ),\n762         (\n763             \"ignore-comments\",\n764             {\n765                 \"default\": True,\n766                 \"type\": \"yn\",\n767                 \"metavar\": \"<y or n>\",\n768                 \"help\": \"Comments are removed from the similarity computation\",\n769             },\n770         ),\n771         (\n772             \"ignore-docstrings\",\n773             {\n774                 \"default\": True,\n775                 \"type\": \"yn\",\n776                 \"metavar\": \"<y or n>\",\n777                 \"help\": \"Docstrings are removed from the similarity computation\",\n778             },\n779         ),\n780         (\n781             \"ignore-imports\",\n782             {\n783                 \"default\": True,\n784                 \"type\": \"yn\",\n785                 \"metavar\": \"<y or n>\",\n786                 \"help\": \"Imports are removed from the similarity computation\",\n787             },\n788         ),\n789         (\n790             \"ignore-signatures\",\n791             {\n792                 \"default\": True,\n793                 \"type\": \"yn\",\n794                 \"metavar\": \"<y or n>\",\n795                 \"help\": \"Signatures are removed from the similarity computation\",\n796             },\n797         ),\n798     )\n799     # reports\n800     reports = ((\"RP0801\", \"Duplication\", report_similarities),)\n801 \n802     def __init__(self, linter: PyLinter) -> None:\n803         BaseRawFileChecker.__init__(self, linter)\n804         Similar.__init__(\n805             self,\n806             min_lines=self.linter.config.min_similarity_lines,\n807             ignore_comments=self.linter.config.ignore_comments,\n808             ignore_docstrings=self.linter.config.ignore_docstrings,\n809             ignore_imports=self.linter.config.ignore_imports,\n810             ignore_signatures=self.linter.config.ignore_signatures,\n811         )\n812 \n813     def open(self):\n814         \"\"\"Init the checkers: reset linesets and statistics information.\"\"\"\n815         self.linesets = []\n816         self.linter.stats.reset_duplicated_lines()\n817 \n818     def process_module(self, node: nodes.Module) -> None:\n819         \"\"\"Process a module.\n820 \n821         the module's content is accessible via the stream object\n822 \n823         stream must implement the readlines method\n824         \"\"\"\n825         if self.linter.current_name is None:\n826             warnings.warn(\n827                 (\n828                     \"In pylint 3.0 the current_name attribute of the linter object should be a string. \"\n829                     \"If unknown it should be initialized as an empty string.\"\n830                 ),\n831                 DeprecationWarning,\n832             )\n833         with node.stream() as stream:\n834             self.append_stream(self.linter.current_name, stream, node.file_encoding)  # type: ignore[arg-type]\n835 \n836     def close(self):\n837         \"\"\"Compute and display similarities on closing (i.e. end of parsing).\"\"\"\n838         total = sum(len(lineset) for lineset in self.linesets)\n839         duplicated = 0\n840         stats = self.linter.stats\n841         for num, couples in self._compute_sims():\n842             msg = []\n843             lineset = start_line = end_line = None\n844             for lineset, start_line, end_line in couples:\n845                 msg.append(f\"=={lineset.name}:[{start_line}:{end_line}]\")\n846             msg.sort()\n847 \n848             if lineset:\n849                 for line in lineset.real_lines[start_line:end_line]:\n850                     msg.append(line.rstrip())\n851 \n852             self.add_message(\"R0801\", args=(len(couples), \"\\n\".join(msg)))\n853             duplicated += num * (len(couples) - 1)\n854         stats.nb_duplicated_lines += int(duplicated)\n855         stats.percent_duplicated_lines += float(total and duplicated * 100.0 / total)\n856 \n857     def get_map_data(self):\n858         \"\"\"Passthru override.\"\"\"\n859         return Similar.get_map_data(self)\n860 \n861     def reduce_map_data(self, linter, data):\n862         \"\"\"Reduces and recombines data into a format that we can report on.\n863 \n864         The partner function of get_map_data()\n865         \"\"\"\n866         Similar.combine_mapreduce_data(self, linesets_collection=data)\n867 \n868 \n869 def register(linter: PyLinter) -> None:\n870     linter.register_checker(SimilarChecker(linter))\n871 \n872 \n873 def usage(status=0):\n874     \"\"\"Display command line usage information.\"\"\"\n875     print(\"finds copy pasted blocks in a set of files\")\n876     print()\n877     print(\n878         \"Usage: symilar [-d|--duplicates min_duplicated_lines] \\\n879 [-i|--ignore-comments] [--ignore-docstrings] [--ignore-imports] [--ignore-signatures] file1...\"\n880     )\n881     sys.exit(status)\n882 \n883 \n884 def Run(argv=None) -> NoReturn:\n885     \"\"\"Standalone command line access point.\"\"\"\n886     if argv is None:\n887         argv = sys.argv[1:]\n888 \n889     s_opts = \"hdi\"\n890     l_opts = [\n891         \"help\",\n892         \"duplicates=\",\n893         \"ignore-comments\",\n894         \"ignore-imports\",\n895         \"ignore-docstrings\",\n896         \"ignore-signatures\",\n897     ]\n898     min_lines = DEFAULT_MIN_SIMILARITY_LINE\n899     ignore_comments = False\n900     ignore_docstrings = False\n901     ignore_imports = False\n902     ignore_signatures = False\n903     opts, args = getopt(argv, s_opts, l_opts)\n904     for opt, val in opts:\n905         if opt in {\"-d\", \"--duplicates\"}:\n906             min_lines = int(val)\n907         elif opt in {\"-h\", \"--help\"}:\n908             usage()\n909         elif opt in {\"-i\", \"--ignore-comments\"}:\n910             ignore_comments = True\n911         elif opt in {\"--ignore-docstrings\"}:\n912             ignore_docstrings = True\n913         elif opt in {\"--ignore-imports\"}:\n914             ignore_imports = True\n915         elif opt in {\"--ignore-signatures\"}:\n916             ignore_signatures = True\n917     if not args:\n918         usage(1)\n919     sim = Similar(\n920         min_lines, ignore_comments, ignore_docstrings, ignore_imports, ignore_signatures\n921     )\n922     for filename in args:\n923         with open(filename, encoding=\"utf-8\") as stream:\n924             sim.append_stream(filename, stream)\n925     sim.run()\n926     sys.exit(0)\n927 \n928 \n929 if __name__ == \"__main__\":\n930     Run()\n",
        "intended_behavior": "The code within this file should properly heed the `--ignore`, `--ignore-paths`, and `--ignore-patterns` flags when performing similarity checks, particularly in recursive mode. This would involve filtering out files and directories that match the patterns specified by these flags before any analysis or similarity checks are conducted. The intended behavior ensures that any file or directory meant to be ignored as per the user's configuration is indeed skipped during linting, preserving the existing functionality of Pylint while fixing the issue of ignored entities still being analyzed."
    }
]