diff --git a/sklearn/__init__.py b/sklearn/__init__.py
--- a/sklearn/__init__.py
+++ b/sklearn/__init__.py
@@ -45,7 +45,7 @@
 # Dev branch marker is: 'X.Y.dev' or 'X.Y.devN' where N is an integer.
 # 'X.Y.dev0' is the canonical version of 'X.Y.dev'
 #
-__version__ = '0.21.1'
+__version__ = '0.21.2'
 
 
 # On OSX, we can get a runtime error due to multiple OpenMP libraries loaded
diff --git a/sklearn/cross_decomposition/pls_.py b/sklearn/cross_decomposition/pls_.py
--- a/sklearn/cross_decomposition/pls_.py
+++ b/sklearn/cross_decomposition/pls_.py
@@ -285,6 +285,7 @@ def fit(self, X, Y):
         self.n_iter_ = []
 
         # NIPALS algo: outer loop, over components
+        Y_eps = np.finfo(Yk.dtype).eps
         for k in range(self.n_components):
             if np.all(np.dot(Yk.T, Yk) < np.finfo(np.double).eps):
                 # Yk constant
@@ -293,6 +294,10 @@ def fit(self, X, Y):
             # 1) weights estimation (inner loop)
             # -----------------------------------
             if self.algorithm == "nipals":
+                # Replace columns that are all close to zero with zeros
+                Yk_mask = np.all(np.abs(Yk) < 10 * Y_eps, axis=0)
+                Yk[:, Yk_mask] = 0.0
+
                 x_weights, y_weights, n_iter_ = \
                     _nipals_twoblocks_inner_loop(
                         X=Xk, Y=Yk, mode=self.mode, max_iter=self.max_iter,
diff --git a/sklearn/experimental/enable_iterative_imputer.py b/sklearn/experimental/enable_iterative_imputer.py
--- a/sklearn/experimental/enable_iterative_imputer.py
+++ b/sklearn/experimental/enable_iterative_imputer.py
@@ -1,6 +1,6 @@
 """Enables IterativeImputer
 
-The API and results of this estimators might change without any deprecation
+The API and results of this estimator might change without any deprecation
 cycle.
 
 Importing this file dynamically sets :class:`sklearn.impute.IterativeImputer`
diff --git a/sklearn/metrics/classification.py b/sklearn/metrics/classification.py
--- a/sklearn/metrics/classification.py
+++ b/sklearn/metrics/classification.py
@@ -1066,7 +1066,7 @@ def fbeta_score(y_true, y_pred, beta, labels=None, pos_label=1,
     The F-beta score is the weighted harmonic mean of precision and recall,
     reaching its optimal value at 1 and its worst value at 0.
 
-    The `beta` parameter determines the weight of precision in the combined
+    The `beta` parameter determines the weight of recall in the combined
     score. ``beta < 1`` lends more weight to precision, while ``beta > 1``
     favors recall (``beta -> 0`` considers only precision, ``beta -> inf``
     only recall).
diff --git a/sklearn/metrics/pairwise.py b/sklearn/metrics/pairwise.py
--- a/sklearn/metrics/pairwise.py
+++ b/sklearn/metrics/pairwise.py
@@ -283,7 +283,7 @@ def euclidean_distances(X, Y=None, Y_norm_squared=None, squared=False,
     return distances if squared else np.sqrt(distances, out=distances)
 
 
-def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):
+def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None, batch_size=None):
     """Euclidean distances between X and Y
 
     Assumes X and Y have float32 dtype.
@@ -298,28 +298,28 @@ def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):
 
     distances = np.empty((n_samples_X, n_samples_Y), dtype=np.float32)
 
-    x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1
-    y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1
-
-    # Allow 10% more memory than X, Y and the distance matrix take (at least
-    # 10MiB)
-    maxmem = max(
-        ((x_density * n_samples_X + y_density * n_samples_Y) * n_features
-         + (x_density * n_samples_X * y_density * n_samples_Y)) / 10,
-        10 * 2 ** 17)
-
-    # The increase amount of memory in 8-byte blocks is:
-    # - x_density * batch_size * n_features (copy of chunk of X)
-    # - y_density * batch_size * n_features (copy of chunk of Y)
-    # - batch_size * batch_size (chunk of distance matrix)
-    # Hence x² + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem
-    #                                 xd=x_density and yd=y_density
-    tmp = (x_density + y_density) * n_features
-    batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) / 2
-    batch_size = max(int(batch_size), 1)
-
-    x_batches = gen_batches(X.shape[0], batch_size)
-    y_batches = gen_batches(Y.shape[0], batch_size)
+    if batch_size is None:
+        x_density = X.nnz / np.prod(X.shape) if issparse(X) else 1
+        y_density = Y.nnz / np.prod(Y.shape) if issparse(Y) else 1
+
+        # Allow 10% more memory than X, Y and the distance matrix take (at
+        # least 10MiB)
+        maxmem = max(
+            ((x_density * n_samples_X + y_density * n_samples_Y) * n_features
+             + (x_density * n_samples_X * y_density * n_samples_Y)) / 10,
+            10 * 2 ** 17)
+
+        # The increase amount of memory in 8-byte blocks is:
+        # - x_density * batch_size * n_features (copy of chunk of X)
+        # - y_density * batch_size * n_features (copy of chunk of Y)
+        # - batch_size * batch_size (chunk of distance matrix)
+        # Hence x² + (xd+yd)kx = M, where x=batch_size, k=n_features, M=maxmem
+        #                                 xd=x_density and yd=y_density
+        tmp = (x_density + y_density) * n_features
+        batch_size = (-tmp + np.sqrt(tmp ** 2 + 4 * maxmem)) / 2
+        batch_size = max(int(batch_size), 1)
+
+    x_batches = gen_batches(n_samples_X, batch_size)
 
     for i, x_slice in enumerate(x_batches):
         X_chunk = X[x_slice].astype(np.float64)
@@ -328,6 +328,8 @@ def _euclidean_distances_upcast(X, XX=None, Y=None, YY=None):
         else:
             XX_chunk = XX[x_slice]
 
+        y_batches = gen_batches(n_samples_Y, batch_size)
+
         for j, y_slice in enumerate(y_batches):
             if X is Y and j < i:
                 # when X is Y the distance matrix is symmetric so we only need
diff --git a/sklearn/preprocessing/_encoders.py b/sklearn/preprocessing/_encoders.py
--- a/sklearn/preprocessing/_encoders.py
+++ b/sklearn/preprocessing/_encoders.py
@@ -367,7 +367,8 @@ def _handle_deprecations(self, X):
             msg = (
                 "Passing 'n_values' is deprecated in version 0.20 and will be "
                 "removed in 0.22. You can use the 'categories' keyword "
-                "instead. 'n_values=n' corresponds to 'categories=[range(n)]'."
+                "instead. 'n_values=n' corresponds to "
+                "'categories=[range(n)] * n_features'."
             )
             warnings.warn(msg, DeprecationWarning)
             self._legacy_mode = True
@@ -847,6 +848,8 @@ def get_feature_names(self, input_features=None):
         for i in range(len(cats)):
             names = [
                 input_features[i] + '_' + str(t) for t in cats[i]]
+            if self.drop is not None:
+                names.pop(self.drop_idx_[i])
             feature_names.extend(names)
 
         return np.array(feature_names, dtype=object)
diff --git a/sklearn/utils/sparsefuncs.py b/sklearn/utils/sparsefuncs.py
--- a/sklearn/utils/sparsefuncs.py
+++ b/sklearn/utils/sparsefuncs.py
@@ -341,6 +341,11 @@ def inplace_swap_column(X, m, n):
 
 def _minor_reduce(X, ufunc):
     major_index = np.flatnonzero(np.diff(X.indptr))
+
+    # reduceat tries casts X.indptr to intp, which errors
+    # if it is int64 on a 32 bit system.
+    # Reinitializing prevents this where possible, see #13737
+    X = type(X)((X.data, X.indices, X.indptr), shape=X.shape)
     value = ufunc.reduceat(X.data, X.indptr[major_index])
     return major_index, value
 
